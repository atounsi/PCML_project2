{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from helpers import *\n",
    "from plots import *\n",
    "from plots import *\n",
    "from split_data import *\n",
    "from recommender import *\n",
    "from cross_validation import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\u001c",
    "\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "path_dataset = \"../data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncs45l1n/HyRRptq0/AuwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103e6f048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_ratings, train_validation, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "#plot_train_test_data(train_validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Matrix factorisation using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run run.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run run.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods\n",
    "### CCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "number of items: 10000, number of users: 1000\n",
      "Preprocessing data\n",
      "Splitting data into train and test sets\n",
      "Training model\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.093432775120109.\n",
      "iter: 1.0, RMSE on training set: 1.0101849106969572.\n",
      "iter: 2.0, RMSE on training set: 0.9913295912081788.\n",
      "iter: 3.0, RMSE on training set: 0.9869897338042543.\n",
      "iter: 4.0, RMSE on training set: 0.9859527104933422.\n",
      "iter: 5.0, RMSE on training set: 0.9856933115214811.\n",
      "iter: 6.0, RMSE on training set: 0.985624653358203.\n",
      "RMSE on test data: 1.0058753685813262.\n",
      "RMSE on train data: 0.985624653358203.\n",
      "RMSE on test data: 1.0058753685813262.\n"
     ]
    }
   ],
   "source": [
    "%run run.py 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "number of items: 10000, number of users: 1000\n",
      "Preprocessing data\n",
      "Splitting data into train and test sets\n",
      "Training model\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.10159862737426.\n",
      "iter: 1.0, RMSE on training set: 1.0148352668323901.\n",
      "iter: 2.0, RMSE on training set: 0.9929996281233786.\n",
      "iter: 3.0, RMSE on training set: 0.9875365310380664.\n",
      "iter: 4.0, RMSE on training set: 0.9861305714809792.\n",
      "iter: 5.0, RMSE on training set: 0.9857544966429896.\n",
      "iter: 6.0, RMSE on training set: 0.9856490763073388.\n",
      "iter: 7.0, RMSE on training set: 0.9856175604829593.\n",
      "RMSE on test data: 1.005852273519531.\n",
      "RMSE on train data: 0.9856175604829593.\n",
      "RMSE on test data: 1.005852273519531.\n"
     ]
    }
   ],
   "source": [
    "%run run.py 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running num_features=1\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904900809435744.\n",
      "iter: 1.0, RMSE on training set: 0.9904900809435744.\n",
      "RMSE on test data: 1.0006430527407613.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903873201539922.\n",
      "iter: 1.0, RMSE on training set: 0.9903873201539922.\n",
      "RMSE on test data: 1.0015265151970847.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904558225938457.\n",
      "iter: 1.0, RMSE on training set: 0.9904558225938457.\n",
      "RMSE on test data: 1.0009699185394834.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906118679454422.\n",
      "iter: 1.0, RMSE on training set: 0.9906118679454422.\n",
      "RMSE on test data: 0.9997708187658811.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902800428221178.\n",
      "iter: 1.0, RMSE on training set: 0.9902800428221178.\n",
      "RMSE on test data: 1.0026917472323662.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901292704030457.\n",
      "iter: 1.0, RMSE on training set: 0.9901292704030457.\n",
      "RMSE on test data: 1.0038381445272881.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9912379318005341.\n",
      "iter: 1.0, RMSE on training set: 0.9912379318005341.\n",
      "RMSE on test data: 0.9939965569926406.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903381115430898.\n",
      "iter: 1.0, RMSE on training set: 0.9903381115430898.\n",
      "RMSE on test data: 1.0020360383559628.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902613842784617.\n",
      "iter: 1.0, RMSE on training set: 0.9902613842784617.\n",
      "RMSE on test data: 1.0026886060959928.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909437597253823.\n",
      "iter: 1.0, RMSE on training set: 0.9909437597253823.\n",
      "RMSE on test data: 0.9967189551934174.\n",
      "Running num_features=4\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0814501140566293.\n",
      "iter: 1.0, RMSE on training set: 1.0344345005360815.\n",
      "iter: 2.0, RMSE on training set: 1.0105935712175822.\n",
      "iter: 3.0, RMSE on training set: 0.996510196583157.\n",
      "iter: 4.0, RMSE on training set: 0.9882692181030674.\n",
      "iter: 5.0, RMSE on training set: 0.9834888269458077.\n",
      "iter: 6.0, RMSE on training set: 0.9807357737351757.\n",
      "iter: 7.0, RMSE on training set: 0.979161580726277.\n",
      "iter: 8.0, RMSE on training set: 0.978268860098014.\n",
      "iter: 9.0, RMSE on training set: 0.9777679085990005.\n",
      "iter: 10.0, RMSE on training set: 0.9774907926713219.\n",
      "iter: 11.0, RMSE on training set: 0.9773405753058031.\n",
      "iter: 12.0, RMSE on training set: 0.9772615497836432.\n",
      "RMSE on test data: 1.0164540357940843.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0813979050731815.\n",
      "iter: 1.0, RMSE on training set: 1.0343417951812084.\n",
      "iter: 2.0, RMSE on training set: 1.0104957279503448.\n",
      "iter: 3.0, RMSE on training set: 0.9964181610879732.\n",
      "iter: 4.0, RMSE on training set: 0.9881855717121646.\n",
      "iter: 5.0, RMSE on training set: 0.9834127640129435.\n",
      "iter: 6.0, RMSE on training set: 0.9806654929513352.\n",
      "iter: 7.0, RMSE on training set: 0.9790952977430292.\n",
      "iter: 8.0, RMSE on training set: 0.9782051481704795.\n",
      "iter: 9.0, RMSE on training set: 0.977705739300102.\n",
      "iter: 10.0, RMSE on training set: 0.977429472820833.\n",
      "iter: 11.0, RMSE on training set: 0.9772796630271965.\n",
      "iter: 12.0, RMSE on training set: 0.9772007795499793.\n",
      "RMSE on test data: 1.0172159689810225.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0815759252850092.\n",
      "iter: 1.0, RMSE on training set: 1.0344014173728147.\n",
      "iter: 2.0, RMSE on training set: 1.010533168773614.\n",
      "iter: 3.0, RMSE on training set: 0.9964437400582316.\n",
      "iter: 4.0, RMSE on training set: 0.9882062260008204.\n",
      "iter: 5.0, RMSE on training set: 0.9834317423329042.\n",
      "iter: 6.0, RMSE on training set: 0.9806841236014999.\n",
      "iter: 7.0, RMSE on training set: 0.9791140472865273.\n",
      "iter: 8.0, RMSE on training set: 0.9782241388138003.\n",
      "iter: 9.0, RMSE on training set: 0.9777249764042066.\n",
      "iter: 10.0, RMSE on training set: 0.9774489333602291.\n",
      "iter: 11.0, RMSE on training set: 0.9772993238455844.\n",
      "iter: 12.0, RMSE on training set: 0.9772206227760072.\n",
      "RMSE on test data: 1.0173175041653408.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0814489216877976.\n",
      "iter: 1.0, RMSE on training set: 1.0344120409991335.\n",
      "iter: 2.0, RMSE on training set: 1.0105933439039512.\n",
      "iter: 3.0, RMSE on training set: 0.9965302318571084.\n",
      "iter: 4.0, RMSE on training set: 0.9883039861013969.\n",
      "iter: 5.0, RMSE on training set: 0.9835334795978667.\n",
      "iter: 6.0, RMSE on training set: 0.9807866252850457.\n",
      "iter: 7.0, RMSE on training set: 0.979216049511375.\n",
      "iter: 8.0, RMSE on training set: 0.9783252607292932.\n",
      "iter: 9.0, RMSE on training set: 0.9778252035033427.\n",
      "iter: 10.0, RMSE on training set: 0.9775483788159587.\n",
      "iter: 11.0, RMSE on training set: 0.9773981261143455.\n",
      "iter: 12.0, RMSE on training set: 0.9773189070210544.\n",
      "RMSE on test data: 1.0163047114241224.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.081423246810819.\n",
      "iter: 1.0, RMSE on training set: 1.0343257753158046.\n",
      "iter: 2.0, RMSE on training set: 1.0104086697512502.\n",
      "iter: 3.0, RMSE on training set: 0.996268779268365.\n",
      "iter: 4.0, RMSE on training set: 0.9879928234771662.\n",
      "iter: 5.0, RMSE on training set: 0.983191293304531.\n",
      "iter: 6.0, RMSE on training set: 0.9804255041674159.\n",
      "iter: 7.0, RMSE on training set: 0.9788435850037633.\n",
      "iter: 8.0, RMSE on training set: 0.9779461087826551.\n",
      "iter: 9.0, RMSE on training set: 0.9774421596781897.\n",
      "iter: 10.0, RMSE on training set: 0.9771630874763256.\n",
      "iter: 11.0, RMSE on training set: 0.977011533727407.\n",
      "iter: 12.0, RMSE on training set: 0.9769315451578398.\n",
      "RMSE on test data: 1.0203179743486646.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0810489208057974.\n",
      "iter: 1.0, RMSE on training set: 1.0340913733074588.\n",
      "iter: 2.0, RMSE on training set: 1.0102200284898002.\n",
      "iter: 3.0, RMSE on training set: 0.9961250387651902.\n",
      "iter: 4.0, RMSE on training set: 0.987884844388805.\n",
      "iter: 5.0, RMSE on training set: 0.9831097428058712.\n",
      "iter: 6.0, RMSE on training set: 0.9803625470481692.\n",
      "iter: 7.0, RMSE on training set: 0.9787932473063715.\n",
      "iter: 8.0, RMSE on training set: 0.9779041067940337.\n",
      "iter: 9.0, RMSE on training set: 0.9774055487486646.\n",
      "iter: 10.0, RMSE on training set: 0.9771299064982866.\n",
      "iter: 11.0, RMSE on training set: 0.976980509765457.\n",
      "iter: 12.0, RMSE on training set: 0.9769018702177694.\n",
      "RMSE on test data: 1.0199687479421526.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0818589389941002.\n",
      "iter: 1.0, RMSE on training set: 1.0349245100054985.\n",
      "iter: 2.0, RMSE on training set: 1.01115626912648.\n",
      "iter: 3.0, RMSE on training set: 0.9971270535379652.\n",
      "iter: 4.0, RMSE on training set: 0.9889214342500884.\n",
      "iter: 5.0, RMSE on training set: 0.9841633379247664.\n",
      "iter: 6.0, RMSE on training set: 0.9814239841693978.\n",
      "iter: 7.0, RMSE on training set: 0.9798580085363018.\n",
      "iter: 8.0, RMSE on training set: 0.9789701001598828.\n",
      "iter: 9.0, RMSE on training set: 0.978471889454757.\n",
      "iter: 10.0, RMSE on training set: 0.978196276137488.\n",
      "iter: 11.0, RMSE on training set: 0.9780468335306545.\n",
      "iter: 12.0, RMSE on training set: 0.977968162518082.\n",
      "RMSE on test data: 1.010390672701321.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.081110037295384.\n",
      "iter: 1.0, RMSE on training set: 1.0341527170417613.\n",
      "iter: 2.0, RMSE on training set: 1.0103471313981263.\n",
      "iter: 3.0, RMSE on training set: 0.9962924685791491.\n",
      "iter: 4.0, RMSE on training set: 0.9880707659013361.\n",
      "iter: 5.0, RMSE on training set: 0.9833030049238417.\n",
      "iter: 6.0, RMSE on training set: 0.9805582171564046.\n",
      "iter: 7.0, RMSE on training set: 0.9789894010888708.\n",
      "iter: 8.0, RMSE on training set: 0.9781001433302022.\n",
      "iter: 9.0, RMSE on training set: 0.9776013865809109.\n",
      "iter: 10.0, RMSE on training set: 0.9773256279273116.\n",
      "iter: 11.0, RMSE on training set: 0.9771762187766909.\n",
      "iter: 12.0, RMSE on training set: 0.9770976457413824.\n",
      "RMSE on test data: 1.0180005144471906.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.081233483694028.\n",
      "iter: 1.0, RMSE on training set: 1.0342077500322928.\n",
      "iter: 2.0, RMSE on training set: 1.0103293187674125.\n",
      "iter: 3.0, RMSE on training set: 0.9962213361542696.\n",
      "iter: 4.0, RMSE on training set: 0.987965905966434.\n",
      "iter: 5.0, RMSE on training set: 0.9831771726847358.\n",
      "iter: 6.0, RMSE on training set: 0.9804191487442214.\n",
      "iter: 7.0, RMSE on training set: 0.9788418348213841.\n",
      "iter: 8.0, RMSE on training set: 0.9779470307876081.\n",
      "iter: 9.0, RMSE on training set: 0.9774446001601621.\n",
      "iter: 10.0, RMSE on training set: 0.9771663754245096.\n",
      "iter: 11.0, RMSE on training set: 0.9770152906125159.\n",
      "iter: 12.0, RMSE on training set: 0.9769355658246466.\n",
      "RMSE on test data: 1.0193194864639021.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0815520381757302.\n",
      "iter: 1.0, RMSE on training set: 1.0346587920588926.\n",
      "iter: 2.0, RMSE on training set: 1.0108835258938094.\n",
      "iter: 3.0, RMSE on training set: 0.9968443576255559.\n",
      "iter: 4.0, RMSE on training set: 0.9886327436200465.\n",
      "iter: 5.0, RMSE on training set: 0.9838719073609937.\n",
      "iter: 6.0, RMSE on training set: 0.9811317961546185.\n",
      "iter: 7.0, RMSE on training set: 0.9795660416293223.\n",
      "iter: 8.0, RMSE on training set: 0.9786787062860123.\n",
      "iter: 9.0, RMSE on training set: 0.9781810853881299.\n",
      "iter: 10.0, RMSE on training set: 0.9779059301098207.\n",
      "iter: 11.0, RMSE on training set: 0.9777567707775191.\n",
      "iter: 12.0, RMSE on training set: 0.9776782178817663.\n",
      "RMSE on test data: 1.0132167521365243.\n",
      "Running num_features=7\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0791763318783265.\n",
      "iter: 1.0, RMSE on training set: 1.0129861934727962.\n",
      "iter: 2.0, RMSE on training set: 0.9991262210636557.\n",
      "iter: 3.0, RMSE on training set: 0.9895408724235086.\n",
      "iter: 4.0, RMSE on training set: 0.9824688430862772.\n",
      "iter: 5.0, RMSE on training set: 0.9772576664221989.\n",
      "iter: 6.0, RMSE on training set: 0.9734369125618242.\n",
      "iter: 7.0, RMSE on training set: 0.9706510748999836.\n",
      "iter: 8.0, RMSE on training set: 0.9686318735069923.\n",
      "iter: 9.0, RMSE on training set: 0.9671779678943683.\n",
      "iter: 10.0, RMSE on training set: 0.9661390225942886.\n",
      "iter: 11.0, RMSE on training set: 0.9654032821045551.\n",
      "iter: 12.0, RMSE on training set: 0.9648879964522901.\n",
      "iter: 13.0, RMSE on training set: 0.964532116632812.\n",
      "iter: 14.0, RMSE on training set: 0.9642907642859238.\n",
      "iter: 15.0, RMSE on training set: 0.9641310692413348.\n",
      "iter: 16.0, RMSE on training set: 0.9640290510136218.\n",
      "iter: 17.0, RMSE on training set: 0.963967291069516.\n",
      "RMSE on test data: 1.0359264426328314.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.079513656867835.\n",
      "iter: 1.0, RMSE on training set: 1.01306420201027.\n",
      "iter: 2.0, RMSE on training set: 0.9991922155769215.\n",
      "iter: 3.0, RMSE on training set: 0.9895977436959993.\n",
      "iter: 4.0, RMSE on training set: 0.9825172602823552.\n",
      "iter: 5.0, RMSE on training set: 0.9772999723305539.\n",
      "iter: 6.0, RMSE on training set: 0.973474885439146.\n",
      "iter: 7.0, RMSE on training set: 0.9706858099730714.\n",
      "iter: 8.0, RMSE on training set: 0.9686640259985566.\n",
      "iter: 9.0, RMSE on training set: 0.9672079339409809.\n",
      "iter: 10.0, RMSE on training set: 0.966167051179404.\n",
      "iter: 11.0, RMSE on training set: 0.9654295423743479.\n",
      "iter: 12.0, RMSE on training set: 0.964912617162153.\n",
      "iter: 13.0, RMSE on training set: 0.9645552081662953.\n",
      "iter: 14.0, RMSE on training set: 0.9643124300352752.\n",
      "iter: 15.0, RMSE on training set: 0.9641514107338744.\n",
      "iter: 16.0, RMSE on training set: 0.9640481695517965.\n",
      "iter: 17.0, RMSE on training set: 0.9639852875803905.\n",
      "RMSE on test data: 1.0358485273315303.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0793680796742267.\n",
      "iter: 1.0, RMSE on training set: 1.0128658883670683.\n",
      "iter: 2.0, RMSE on training set: 0.9989946784490353.\n",
      "iter: 3.0, RMSE on training set: 0.9894220145767479.\n",
      "iter: 4.0, RMSE on training set: 0.9823630543486356.\n",
      "iter: 5.0, RMSE on training set: 0.9771634260143388.\n",
      "iter: 6.0, RMSE on training set: 0.97335238248865.\n",
      "iter: 7.0, RMSE on training set: 0.970574393073812.\n",
      "iter: 8.0, RMSE on training set: 0.9685613381998297.\n",
      "iter: 9.0, RMSE on training set: 0.9671121235225891.\n",
      "iter: 10.0, RMSE on training set: 0.966076676134755.\n",
      "iter: 11.0, RMSE on training set: 0.9653434852926263.\n",
      "iter: 12.0, RMSE on training set: 0.9648300139797447.\n",
      "iter: 13.0, RMSE on training set: 0.9644753906431826.\n",
      "iter: 14.0, RMSE on training set: 0.9642348802576658.\n",
      "iter: 15.0, RMSE on training set: 0.9640757256079396.\n",
      "iter: 16.0, RMSE on training set: 0.9639740333708761.\n",
      "iter: 17.0, RMSE on training set: 0.9639124510081812.\n",
      "RMSE on test data: 1.0370862270288057.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0791117334107714.\n",
      "iter: 1.0, RMSE on training set: 1.0129787614786903.\n",
      "iter: 2.0, RMSE on training set: 0.9991610388306142.\n",
      "iter: 3.0, RMSE on training set: 0.9895952357810855.\n",
      "iter: 4.0, RMSE on training set: 0.9825333094644917.\n",
      "iter: 5.0, RMSE on training set: 0.9773271402015148.\n",
      "iter: 6.0, RMSE on training set: 0.9735082994408221.\n",
      "iter: 7.0, RMSE on training set: 0.970722467505037.\n",
      "iter: 8.0, RMSE on training set: 0.9687021405258928.\n",
      "iter: 9.0, RMSE on training set: 0.9672464903044576.\n",
      "iter: 10.0, RMSE on training set: 0.9662055146593009.\n",
      "iter: 11.0, RMSE on training set: 0.9654676699584887.\n",
      "iter: 12.0, RMSE on training set: 0.9649503361026947.\n",
      "iter: 13.0, RMSE on training set: 0.9645925390285801.\n",
      "iter: 14.0, RMSE on training set: 0.9643494390268774.\n",
      "iter: 15.0, RMSE on training set: 0.9641881811059024.\n",
      "iter: 16.0, RMSE on training set: 0.9640847852455858.\n",
      "iter: 17.0, RMSE on training set: 0.9640218245928823.\n",
      "RMSE on test data: 1.0376501143010035.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0791127388925068.\n",
      "iter: 1.0, RMSE on training set: 1.0126956604330348.\n",
      "iter: 2.0, RMSE on training set: 0.9988096495150344.\n",
      "iter: 3.0, RMSE on training set: 0.989215773649846.\n",
      "iter: 4.0, RMSE on training set: 0.9821368564503856.\n",
      "iter: 5.0, RMSE on training set: 0.976919045681308.\n",
      "iter: 6.0, RMSE on training set: 0.9730920706869095.\n",
      "iter: 7.0, RMSE on training set: 0.9703005639561582.\n",
      "iter: 8.0, RMSE on training set: 0.9682762874341703.\n",
      "iter: 9.0, RMSE on training set: 0.9668178802763017.\n",
      "iter: 10.0, RMSE on training set: 0.965774958826997.\n",
      "iter: 11.0, RMSE on training set: 0.9650357108655397.\n",
      "iter: 12.0, RMSE on training set: 0.9645173303460686.\n",
      "iter: 13.0, RMSE on training set: 0.9641587166350473.\n",
      "iter: 14.0, RMSE on training set: 0.9639149454303101.\n",
      "iter: 15.0, RMSE on training set: 0.9637531063875409.\n",
      "iter: 16.0, RMSE on training set: 0.9636491842025622.\n",
      "iter: 17.0, RMSE on training set: 0.9635857302899176.\n",
      "RMSE on test data: 1.0409560509119544.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0789770983014988.\n",
      "iter: 1.0, RMSE on training set: 1.0126668323531618.\n",
      "iter: 2.0, RMSE on training set: 0.9988217700157391.\n",
      "iter: 3.0, RMSE on training set: 0.9892496055377626.\n",
      "iter: 4.0, RMSE on training set: 0.9821866494572685.\n",
      "iter: 5.0, RMSE on training set: 0.9769824851497781.\n",
      "iter: 6.0, RMSE on training set: 0.9731674734487484.\n",
      "iter: 7.0, RMSE on training set: 0.9703862173009732.\n",
      "iter: 8.0, RMSE on training set: 0.9683705096080569.\n",
      "iter: 9.0, RMSE on training set: 0.9669191183820665.\n",
      "iter: 10.0, RMSE on training set: 0.9658818448894348.\n",
      "iter: 11.0, RMSE on training set: 0.9651470800226325.\n",
      "iter: 12.0, RMSE on training set: 0.9646322152470211.\n",
      "iter: 13.0, RMSE on training set: 0.9642763292229688.\n",
      "iter: 14.0, RMSE on training set: 0.9640346534640211.\n",
      "iter: 15.0, RMSE on training set: 0.9638744090840587.\n",
      "iter: 16.0, RMSE on training set: 0.9637716893588248.\n",
      "iter: 17.0, RMSE on training set: 0.9637091339704436.\n",
      "RMSE on test data: 1.0390650094255465.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0796230433735705.\n",
      "iter: 1.0, RMSE on training set: 1.0136934395714645.\n",
      "iter: 2.0, RMSE on training set: 0.9998727305500618.\n",
      "iter: 3.0, RMSE on training set: 0.9903199300870468.\n",
      "iter: 4.0, RMSE on training set: 0.9832712147061425.\n",
      "iter: 5.0, RMSE on training set: 0.9780769020569426.\n",
      "iter: 6.0, RMSE on training set: 0.9742682518638934.\n",
      "iter: 7.0, RMSE on training set: 0.9714909143057738.\n",
      "iter: 8.0, RMSE on training set: 0.9694774782469844.\n",
      "iter: 9.0, RMSE on training set: 0.9680272822221728.\n",
      "iter: 10.0, RMSE on training set: 0.9669905228110324.\n",
      "iter: 11.0, RMSE on training set: 0.9662558589419431.\n",
      "iter: 12.0, RMSE on training set: 0.9657408600086645.\n",
      "iter: 13.0, RMSE on training set: 0.9653847199134269.\n",
      "iter: 14.0, RMSE on training set: 0.9651427424350545.\n",
      "iter: 15.0, RMSE on training set: 0.9649821919223411.\n",
      "iter: 16.0, RMSE on training set: 0.9648791856072944.\n",
      "iter: 17.0, RMSE on training set: 0.9648163746011996.\n",
      "RMSE on test data: 1.0279965952936398.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0794720152316113.\n",
      "iter: 1.0, RMSE on training set: 1.0129201898504105.\n",
      "iter: 2.0, RMSE on training set: 0.9990520240280866.\n",
      "iter: 3.0, RMSE on training set: 0.9894788322109435.\n",
      "iter: 4.0, RMSE on training set: 0.9824149329097722.\n",
      "iter: 5.0, RMSE on training set: 0.977208911968289.\n",
      "iter: 6.0, RMSE on training set: 0.9733913966156842.\n",
      "iter: 7.0, RMSE on training set: 0.9706074668398744.\n",
      "iter: 8.0, RMSE on training set: 0.9685892238383914.\n",
      "iter: 9.0, RMSE on training set: 0.9671356059538753.\n",
      "iter: 10.0, RMSE on training set: 0.9660964877236591.\n",
      "iter: 11.0, RMSE on training set: 0.9653602711333867.\n",
      "iter: 12.0, RMSE on training set: 0.9648443226116505.\n",
      "iter: 13.0, RMSE on training set: 0.9644876775130494.\n",
      "iter: 14.0, RMSE on training set: 0.9642455172296721.\n",
      "iter: 15.0, RMSE on training set: 0.9640850127787132.\n",
      "iter: 16.0, RMSE on training set: 0.9639822111001635.\n",
      "iter: 17.0, RMSE on training set: 0.9639197110594381.\n",
      "RMSE on test data: 1.0365228704397071.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0789312383495024.\n",
      "iter: 1.0, RMSE on training set: 1.0126105507877237.\n",
      "iter: 2.0, RMSE on training set: 0.9988119225041217.\n",
      "iter: 3.0, RMSE on training set: 0.9892640840085579.\n",
      "iter: 4.0, RMSE on training set: 0.9822171559991486.\n",
      "iter: 5.0, RMSE on training set: 0.9770235437603276.\n",
      "iter: 6.0, RMSE on training set: 0.9732151467712993.\n",
      "iter: 7.0, RMSE on training set: 0.9704378557757563.\n",
      "iter: 8.0, RMSE on training set: 0.9684243857013226.\n",
      "iter: 9.0, RMSE on training set: 0.9669741426163376.\n"
     ]
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 3     # 0-SGD 1-ALS\n",
    "K = 10         ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features_arr = [1, 4, 7, 10, 13, 16, 20]   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.7\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(num_features_arr))\n",
    "train_rmse_std = np.zeros(len(num_features_arr))\n",
    "validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "for i, num_features in enumerate(num_features_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running num_features={n}'.format(n=num_features))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation_minimalist(ratings, method, K, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    ## Calculate mean and standard deviation    \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(num_features_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(num_features_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Number of features (K)'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lambda_user=0.01\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905120402314358.\n",
      "iter: 1.0, RMSE on training set: 0.9905120402314358.\n",
      "RMSE on test data: 1.0006561439583412.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904092461006758.\n",
      "iter: 1.0, RMSE on training set: 0.9904092461006758.\n",
      "RMSE on test data: 1.001538376604576.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990477750872439.\n",
      "iter: 1.0, RMSE on training set: 0.990477750872439.\n",
      "RMSE on test data: 1.0009871106146258.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990633808868161.\n",
      "iter: 1.0, RMSE on training set: 0.990633808868161.\n",
      "RMSE on test data: 0.9997944327774473.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a9183b17cdf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running lambda_user={n}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_user\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_rmse_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_rmse_arr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_minimalist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_rmse_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rmse_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation_minimalist\u001b[0;34m(ratings, method, K, num_features, lambda_user, lambda_item, gamma)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             [train_rmse, validation_rmse, user_feature, item_features] = CCDplus(train_cross, test_cross, \n\u001b[0;32m--> 195\u001b[0;31m                                                                 num_features, lambda_user, lambda_item)\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Incorrect method, 0-SGD, 1-ALS, 2-CCD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/recommender.py\u001b[0m in \u001b[0;36mCCDplus\u001b[0;34m(train, test, num_features, lambda_user, lambda_item, max_it_inter)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# solving this problem using ccd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCCD_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_user_itemindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_item_userindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnz_items_per_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnz_users_per_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_it_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/recommender.py\u001b[0m in \u001b[0;36mCCD_simple\u001b[0;34m(train, nz_train, nz_user_itemindices, nz_item_userindices, nnz_items_per_user, nnz_users_per_item, lambda_user, lambda_item, max_it)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_item_CCD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnz_users_per_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_item_userindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mtrain_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_error_residual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/recommender.py\u001b[0m in \u001b[0;36mupdate_item_CCD\u001b[0;34m(residual, user_features, item_features, item, lambda_item, nnz_users_per_item, nz_item_userindices)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_item\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnz_userindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnz_userindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mnew_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_residual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnz_userindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnew_item_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnz_userindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mnew_residual\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnz_userindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mnew_item_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from cross_validation import *\n",
    "## !!! Takes long time to run\n",
    "\n",
    "method = 3     # 0-SGD 1-ALS\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 3   # K in the lecture notes\n",
    "lambda_user_arr = [0.01, 0.1, 0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "lambda_item = 0.75\n",
    "min_num_ratings=1\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "for i, lambda_user in enumerate(lambda_user_arr):\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation_minimalist(ratings, method, K, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running num_features=1\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.9964586489194359.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.9964583524323956.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.9964522566776196.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.996441726997925.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.9964463174935322.\n",
      "Running num_features=3\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.9965275894151306.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.9965273966035967.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.9965211996115378.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.9965105969189195.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.996514383877923.\n",
      "Running num_features=5\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.9958706995672243.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.995871082996432.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.9958647252676435.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.9958544761771045.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.9958592801982589.\n",
      "Running num_features=7\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951689138004891.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951682211479115.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951619977785641.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951513392775841.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951561529835574.\n",
      "Running num_features=10\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939761161314021.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939761276075211.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939705430597577.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939586319702657.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939652233061246.\n",
      "Running num_features=13\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9927995702343569.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9928006881240632.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9927934022741228.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9927813610462145.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9927884867072101.\n",
      "Running num_features=15\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922762654420818.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922758515554405.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922704974042525.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922604371501577.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922650837952336.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrlkyMcYYk2uWTIwxxuTa/wM64EJ/c1SAfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b6a2828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5         ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features_arr = [1, 3, 5, 7, 10, 13, 15]   # K in the lecture notes\n",
    "lambda_user = 0.9\n",
    "lambda_item = 0.1\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(num_features_arr))\n",
    "train_rmse_std = np.zeros(len(num_features_arr))\n",
    "validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "for i, num_features in enumerate(num_features_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running num_features={n}'.format(n=num_features))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    ## Calculate mean and standard deviation    \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(num_features_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(num_features_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Number of features (K)'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 4   # K in the lecture notes\n",
    "lambda_user_arr = [0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "lambda_item = 0.7\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "for i, lambda_user in enumerate(lambda_user_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings,  num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 5   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item_arr = [0.01, 0.1, 0.5, 1]\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "\n",
    "for i, lambda_item in enumerate(lambda_item_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_item={n}'.format(n=lambda_item))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings,  num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_item_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_item_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma_arr = [0.01, 0.1, 1]\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.5\n",
    "\n",
    "train_rmse_mean = np.zeros(len(gamma_arr))\n",
    "train_rmse_std = np.zeros(len(gamma_arr))\n",
    "validation_rmse_mean = np.zeros(len(gamma_arr))\n",
    "validation_rmse_std = np.zeros(len(gamma_arr))\n",
    "\n",
    "for i, gamma in enumerate(gamma_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running gamma={n}'.format(n=gamma))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(gamma_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(gamma_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(gamma_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(gamma_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "#### 1. Compare SGD, ALS with the best set of parameters (based on above results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
