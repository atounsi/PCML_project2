{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import calculate_mse\n",
    "%load_ext autoreload\n",
    "%autoreload 2\u001c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "\n",
    "path_dataset = \"../data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plots import plot_raw_data\n",
    "\n",
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(ratings, num_items_per_user, num_users_per_item,\n",
    "               min_num_ratings, p_test=0.1):\n",
    "    \"\"\"split the ratings to training data and test data.\n",
    "    Args:\n",
    "        min_num_ratings: \n",
    "            all users and items we keep must have at least min_num_ratings per user and per item. \n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data and return train and test data. TODO\n",
    "    # NOTE: we only consider users and movies that have more\n",
    "    # than 10 ratings\n",
    "    # generate random indices\n",
    "    train = sp.lil_matrix((len(valid_items), len(valid_users)))\n",
    "    test = sp.lil_matrix((len(valid_items), len(valid_users)))\n",
    "    index_row,index_col = valid_ratings.nonzero()\n",
    "    for i in np.arange(len(index_row)):\n",
    "        # train or test\n",
    "        choice = np.random.choice(a=[1, 2],p=[0.1,0.9])\n",
    "        if(choice == 1):\n",
    "            test[index_row[i],index_col[i]] = valid_ratings[index_row[i],index_col[i]]\n",
    "        else:\n",
    "            train[index_row[i],index_col[i]] = valid_ratings[index_row[i],index_col[i]]\n",
    "    # ***************************************************\n",
    "    #print(\"Total number of nonzero elements in origial data:{v}\".format(v=ratings.nnz))\n",
    "    #print(\"Total number of nonzero elements in train data:{v}\".format(v=train.nnz))\n",
    "    #print(\"Total number of nonzero elements in test data:{v}\".format(v=test.nnz))\n",
    "    return valid_ratings, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plots import plot_train_test_data\n",
    "\n",
    "valid_ratings, train_validation, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "#plot_train_test_data(train_validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Matrix factorisation using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_MF(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    num_items, num_users = train.shape\n",
    "    user_features = np.random.randint(low=0, high=5, size= (num_features,num_users))\n",
    "    item_features = np.random.randint(low=0, high=5, size=(num_items,num_features))\n",
    "    # ***************************************************\n",
    "    return 1.0*user_features,1.0*item_features\n",
    "\n",
    "def compute_error(data, user_features, item_features, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # ***************************************************\n",
    "    real_label = np.array([data[d,n] for (d,n) in nz])\n",
    "    prediction = np.array([(np.dot(item_features[d,:],(user_features[:,n]))) for (d,n) in nz])\n",
    "    rmse = np.sqrt((1/len(nz))*calculate_mse(real_label,prediction))\n",
    "    # ***************************************************\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_SGD(train, test, num_features=10, lambda_user=0.1, lambda_item=0.7, gamma=0.01):\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    #gamma = 0.01\n",
    "    #num_features = 10   # K in the lecture notes\n",
    "    #lambda_user = 0.1\n",
    "    #lambda_item = 0.7\n",
    "    num_epochs = 20     # number of full passes through the train set\n",
    "    #errors = [0]\n",
    "    learning_curve_train = []\n",
    "    learning_curve_test = []\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    #print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        #gamma /= 1.2\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "        # ***************************************************\n",
    "            prediction = item_features[d,:].dot(user_features[:,n])\n",
    "            #gradient\n",
    "            gradient = np.zeros(((num_items + num_users),num_features))\n",
    "            prediction_error = (train[d,n] - prediction)\n",
    "            #print(prediction_error)\n",
    "            #gradient entries for W\n",
    "            gradient[d,:] = -(prediction_error)*(user_features[:,n].T)\n",
    "            #gradient entries for Z\n",
    "            gradient[num_items+n,:] = -(prediction_error)*(item_features[d,:])\n",
    "            \n",
    "            #update\n",
    "            item_features = item_features - gamma*gradient[:num_items,:]\n",
    "            user_features = user_features - gamma*gradient[num_items:,:].T\n",
    "            \n",
    "        train_rmse = compute_error(train, user_features, item_features, nz_train)\n",
    "        # ***************************************************\n",
    "\n",
    "        #print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        learning_curve_train.append(train_rmse)\n",
    "        learning_curve_test.append(compute_error(test, user_features, item_features, nz_test))\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "\n",
    "    test_rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    #print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    \n",
    "    return train_rmse, test_rmse, user_features, item_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    num_items,num_users = train.shape\n",
    "    num_features = item_features.shape[1]\n",
    "    user_feature = np.zeros((num_features,num_users))\n",
    "    for user in np.arange(num_users):\n",
    "        nnz_items = nnz_items_per_user[user]\n",
    "        nz_itemindices = nz_user_itemindices[user]\n",
    "        nz_itemfeatures = item_features[nz_itemindices,:]\n",
    "        A = ((nz_itemfeatures.T).dot(nz_itemfeatures)+lambda_user*nnz_items*np.eye(num_features))\n",
    "        train_user = train[nz_itemindices,user].toarray()#[:,0]\n",
    "        b = ((nz_itemfeatures.T).dot(train_user))[:,0]\n",
    "        user_feature[:,user] = np.linalg.solve(A,b)\n",
    "    # ***************************************************\n",
    "    return user_feature\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    num_items,num_users = train.shape\n",
    "    num_features = user_features.shape[0]\n",
    "    item_feature = np.zeros((num_items,num_features))\n",
    "    for item in np.arange(num_items):\n",
    "        nnz_users = nnz_users_per_item[item]\n",
    "        nz_userindices = nz_item_userindices[item]\n",
    "        nz_userfeatures = user_features[:,nz_userindices]\n",
    "        A = ((nz_userfeatures).dot(nz_userfeatures.T)+lambda_item*nnz_users*np.eye(num_features))\n",
    "        train_item = (train[item,nz_userindices].T).toarray()#[:,0]\n",
    "        b = ((nz_userfeatures).dot(train_item))[:,0]\n",
    "        item_feature[item,:] = np.linalg.solve(A,b)\n",
    "    # ***************************************************\n",
    "    return item_feature\n",
    "\n",
    "def init_MF_ALS(train, num_features):\n",
    "    \"\"\"init the parameter for matrix factorization.\"\"\"\n",
    "    \n",
    "    # ***************************************************\n",
    "    num_items, num_users = train.shape\n",
    "    user_mean = np.array(train.sum(axis=0)/train.getnnz(axis=0))\n",
    "    user_features = np.r_[user_mean,np.random.randn(num_features-1,num_users)]\n",
    "    item_mean = np.array(train.sum(axis=1).T/train.getnnz(axis=1)).T\n",
    "    item_features = np.c_[item_mean,np.random.randn(num_items,num_features-1)]\n",
    "    #item_features = np.zeros((num_items,num_features))\n",
    "    # ***************************************************\n",
    "    return 1.0*user_features,1.0*item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import build_index_groups\n",
    "np.seterr(all='raise') \n",
    "\n",
    "def ALS(train, test, num_features=10, lambda_user=0.1, lambda_item=0.7):\n",
    "    \"\"\"Alternating Least Squares (ALS) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    stop_criterion = 1e-4\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF_ALS(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    nz_train, nz_row_colindices, nz_col_rowindices = build_index_groups(train)\n",
    "    _,nz_user_itemindices = map(list,zip(*nz_col_rowindices))\n",
    "    nnz_items_per_user = [len(i) for i in nz_user_itemindices]\n",
    "    _,nz_item_userindices = map(list,zip(*nz_row_colindices))\n",
    "    nnz_users_per_item = [len(i) for i in nz_item_userindices]\n",
    "    max_it = 20\n",
    "    \n",
    "    print(\"learn the matrix factorization using ALS...\")\n",
    "\n",
    "    for it in np.arange(max_it):\n",
    "        user_features = update_user_feature(train, item_features, lambda_user, nnz_items_per_user, nz_user_itemindices)\n",
    "        item_features = update_item_feature(train, user_features, lambda_item, nnz_users_per_item, nz_item_userindices)\n",
    "        \n",
    "        train_rmse = compute_error(train, user_features, item_features, nz_train)\n",
    "        #print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))        \n",
    "        error_list.append(train_rmse)\n",
    "        if abs(error_list[-1]-error_list[-2])<stop_criterion:\n",
    "            break\n",
    "\n",
    "    test_rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    #print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    \n",
    "    return train_rmse, test_rmse, user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods\n",
    "### CCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_user_CCD(\n",
    "        residual, user_features, item_features, user, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature row.\"\"\"\n",
    "    # ***************************************************\n",
    "    num_items,num_users = residual.shape\n",
    "    num_features = item_features.shape[1]\n",
    "    nnz_items = nnz_items_per_user[user]\n",
    "    nz_itemindices = nz_user_itemindices[user]\n",
    "    for t in np.arange(num_features):\n",
    "        nom = residual[nz_itemindices,user]+ user_features[t,user]*np.c_[item_features[nz_itemindices,t]]\n",
    "        nom = (item_features[nz_itemindices,t].T).dot(nom)[0,0]\n",
    "        denom = lambda_user + (item_features[nz_itemindices,t].T).dot(item_features[nz_itemindices,t])\n",
    "        new = nom/denom\n",
    "        new_res = residual[nz_itemindices,user] - (new-user_features[t,user])*np.c_[item_features[nz_itemindices,t]]\n",
    "        residual[nz_itemindices,user] = np.squeeze(np.asarray(new_res))\n",
    "        user_features[t,user] = new\n",
    "    # ***************************************************\n",
    "\n",
    "def update_item_CCD(\n",
    "        residual, user_features, item_features, item, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature column.\"\"\"\n",
    "    # ***************************************************\n",
    "    num_items,num_users = train.shape\n",
    "    num_features = user_features.shape[0]\n",
    "    nnz_users = nnz_users_per_item[item]\n",
    "    nz_userindices = nz_item_userindices[item]\n",
    "    for t in np.arange(num_features):\n",
    "        nom = residual[item,nz_userindices] + item_features[item,t]*np.r_[user_features[t,nz_userindices]]\n",
    "        nom = nom.dot((user_features[t,nz_userindices]).T)[0,0]\n",
    "        denom = lambda_item + (user_features[t,nz_userindices]).dot(user_features[t,nz_userindices].T)\n",
    "        new = nom/denom\n",
    "        new_res = residual[item,nz_userindices] - (new-item_features[item,t])*user_features[t,nz_userindices]\n",
    "        residual[item,nz_userindices] = np.squeeze(np.asarray(new_res))\n",
    "        item_features[item,t] = new\n",
    "    # ***************************************************\n",
    "\n",
    "def compute_error_residual(residual, nz):\n",
    "    \"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\n",
    "    # ***************************************************\n",
    "    t = np.array([residual[d,n] for (d,n) in nz])\n",
    "    rmse = np.sqrt(t.dot(t.T)/len(nz))\n",
    "    # ***************************************************\n",
    "    return rmse\n",
    "\n",
    "# Cyclic coordinate descent\n",
    "def CCD(train, test, num_features=10, lambda_user=0.1, lambda_item=0.7):\n",
    "    \"\"\"Cyclic coordinate descent (CCD) algorithm.\"\"\"\n",
    "    # define parameters\n",
    "    stop_criterion = 1e-4\n",
    "    change = 1\n",
    "    error_list = [0, 0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init CCD\n",
    "    user_features, item_features = init_MF_ALS(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    nz_train, nz_row_colindices, nz_col_rowindices = build_index_groups(train)\n",
    "    _,nz_user_itemindices = map(list,zip(*nz_col_rowindices))\n",
    "    nnz_items_per_user = [len(i) for i in nz_user_itemindices]\n",
    "    _,nz_item_userindices = map(list,zip(*nz_row_colindices))\n",
    "    nnz_users_per_item = [len(i) for i in nz_item_userindices]\n",
    "    max_it = 20\n",
    "    \n",
    "    print(\"learn the matrix factorization using CCD...\")\n",
    "    \n",
    "    num_items,num_users = train.shape\n",
    "    residual = train - item_features.dot(user_features)\n",
    "\n",
    "    for it in np.arange(max_it):\n",
    "        for user in np.arange(num_users):\n",
    "            update_user_CCD(residual, user_features, item_features, user, lambda_user, nnz_items_per_user, nz_user_itemindices)\n",
    "            \n",
    "        for item in np.arange(num_features):\n",
    "            update_item_CCD(residual, user_features, item_features, item, lambda_item, nnz_users_per_item, nz_item_userindices)\n",
    "        \n",
    "        train_rmse = compute_error_residual(residual, nz_train)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, train_rmse))        \n",
    "        error_list.append(train_rmse)\n",
    "        if abs(error_list[-1]-error_list[-2])<stop_criterion:\n",
    "            break\n",
    "\n",
    "    test_rmse = compute_error(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(test_rmse))\n",
    "    \n",
    "    return train_rmse, test_rmse, user_features, item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "number of items: 10000, number of users: 1000\n",
      "Preprocessing data\n",
      "Splitting data into train and test sets\n",
      "Training model\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9974862731566.\n",
      "iter: 1, RMSE on training set: 0.9945853529189796.\n",
      "iter: 2, RMSE on training set: 0.9945785223529017.\n",
      "RMSE on test data: 1.0076949411065126.\n",
      "RMSE on train data: 0.9945785223529017.\n",
      "RMSE on test data: 1.0076949411065126.\n",
      "Loading test data\n",
      "number of items: 10000, number of users: 1000\n",
      "Generate predictions\n",
      "Creating submission file\n"
     ]
    }
   ],
   "source": [
    "%run run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_generator(X, K, kth_fold, batch_size, data_size, shuffled_index):   \n",
    "    # Select validation and training data in kth fold\n",
    "    start_val_ind = kth_fold*batch_size\n",
    "    end_val_ind   = (kth_fold+1)*batch_size\n",
    "    \n",
    "    ##[TODO: Try to improve runtime of this method]\n",
    "        \n",
    "    ## ==== Dense matrix approach: memory intensive === ##\n",
    "    validation = np.zeros(X.get_shape())   ## Initialise dense matrix\n",
    "    train = np.zeros(X.get_shape())        ## Initialise dense matrix\n",
    "    Xdense = X.todense()                   ## Change ratings to dense matrix\n",
    "    val_ind = shuffled_index[start_val_ind:end_val_ind]   ## Validation data indices\n",
    "    validation[val_ind] = Xdense[val_ind]\n",
    "    train_ind = shuffled_index[np.setxor1d(range(0,data_size),range(start_val_ind,end_val_ind))]\n",
    "    train[train_ind] = Xdense[train_ind]                  ## Training data indices\n",
    "    return sp.lil_matrix(train), sp.lil_matrix(validation)## Return sparse matrices\n",
    "    \n",
    "    \"\"\"\n",
    "    ## ==== Sparse matrix approach: time consuming === ##\n",
    "    validation = sp.lil_matrix(X.get_shape())  ## Initialise sparse matrix\n",
    "    train = sp.lil_matrix(X.get_shape())       ## ## Initialise sparse matrix\n",
    "    val_ind = shuffled_index[start_val_ind:end_val_ind]   ## Validation data indices\n",
    "    #validation[val_ind] = X[val_ind] #[This does not work, will help improve speed]\n",
    "    for ind in val_ind:\n",
    "        validation[ind] = X[ind]       \n",
    "    train_ind = shuffled_index[np.setxor1d(range(0,data_size),range(start_val_ind,end_val_ind))]\n",
    "    #train[train_ind] = X[train_ind] #[This does not work, will help improve speed]\n",
    "    for ind in train_ind:                                 ## Training data indices\n",
    "        train[ind] = X[ind]           \n",
    "    return train, validation\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(ratings, K, method, num_items_per_user, num_users_per_item, min_num_ratings):\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "    \n",
    "    # select user and item based on the condition.\n",
    "    valid_users = np.where(num_items_per_user >= min_num_ratings)[0]\n",
    "    valid_items = np.where(num_users_per_item >= min_num_ratings)[0]\n",
    "    valid_ratings = ratings[valid_items, :][: , valid_users]\n",
    "    \n",
    "    row, col = valid_ratings.nonzero()\n",
    "    ind = list(zip(row, col))\n",
    "    shuffled_index = np.random.permutation(ind)\n",
    "    \n",
    "    batch_size = int(np.floor(len(ind)/K))\n",
    "    data_size = batch_size*K\n",
    "    for k in range(K):\n",
    "        print('Running {}th fold in {} folds'.format(k+1, K))\n",
    "        ### Split data in kth fold\n",
    "        [training, validation] = k_fold_generator(valid_ratings, K, k, batch_size, data_size, shuffled_index)\n",
    "        \n",
    "        ### Matrix factorization using SGD/ALS\n",
    "        if method == 0:  ## SGD\n",
    "            [train_rmse, validation_rmse, user_feature, item_features] = matrix_factorization_SGD(training,\n",
    "                                                 validation, num_features, lambda_user, lambda_item, gamma) \n",
    "        elif method == 1:            ## ALS\n",
    "            [train_rmse, validation_rmse, user_feature, item_features] = ALS(training,\n",
    "                                                    validation, num_features, lambda_user, lambda_item) \n",
    "        elif method == 2:\n",
    "            [train_rmse, validation_rmse, user_feature, item_features] = CCD(training, validation, \n",
    "                                                                num_features, lambda_user, lambda_item)\n",
    "        else:\n",
    "            print(\"Incorrect method, 0-SGD, 1-ALS, 2-CCD\")\n",
    "            \n",
    "        train_rmse_arr.append(train_rmse)\n",
    "        validation_rmse_arr.append(validation_rmse)\n",
    "        \n",
    "    return train_rmse_arr, validation_rmse_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 2         ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features_arr = [10]   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.7\n",
    "\n",
    "train_rmse_mean = np.zeros(len(num_features_arr))\n",
    "train_rmse_std = np.zeros(len(num_features_arr))\n",
    "validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "for i, num_features in enumerate(num_features_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running num_features={n}'.format(n=num_features))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    ## Calculate mean and standard deviation    \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(num_features_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(num_features_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Number of features (K)'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD 1-ALS\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user_arr = [0.01, 0.1, 1, 10]\n",
    "lambda_item = 0.7\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "for i, lambda_user in enumerate(lambda_user_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD 1-ALS\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item_arr = [0.01, 0.1, 0.5, 1]\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "\n",
    "for i, lambda_item in enumerate(lambda_item_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_item={n}'.format(n=lambda_item))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_item_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_item_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma_arr = [0.01, 0.1, 1]\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.5\n",
    "\n",
    "train_rmse_mean = np.zeros(len(gamma_arr))\n",
    "train_rmse_std = np.zeros(len(gamma_arr))\n",
    "validation_rmse_mean = np.zeros(len(gamma_arr))\n",
    "validation_rmse_std = np.zeros(len(gamma_arr))\n",
    "\n",
    "for i, gamma in enumerate(gamma_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running gamma={n}'.format(n=gamma))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(gamma_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(gamma_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(gamma_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(gamma_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "#### 1. Compare SGD, ALS with the best set of parameters (based on above results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
