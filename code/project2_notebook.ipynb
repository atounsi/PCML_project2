{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from helpers import *\n",
    "from plots import *\n",
    "from plots import *\n",
    "from split_data import *\n",
    "from recommender import *\n",
    "from cross_validation import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\u001c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "path_dataset = \"../data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncs45l1n/HyRRptq0/AuwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103f6f7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_ratings, train_validation, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "#plot_train_test_data(train_validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Matrix factorisation using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run run.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run run.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods\n",
    "### CCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "number of items: 10000, number of users: 1000\n",
      "Preprocessing data\n",
      "Splitting data into train and test sets\n",
      "Training model\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9950929124619052.\n",
      "iter: 1.0, RMSE on training set: 2.8375133312177385.\n",
      "iter: 2.0, RMSE on training set: 2.698924358850894.\n",
      "iter: 3.0, RMSE on training set: 2.5769580418708635.\n",
      "iter: 4.0, RMSE on training set: 2.4695058435338724.\n",
      "iter: 5.0, RMSE on training set: 2.3747001823458365.\n",
      "iter: 6.0, RMSE on training set: 2.2908928062854743.\n",
      "iter: 7.0, RMSE on training set: 2.2166350497167655.\n",
      "iter: 8.0, RMSE on training set: 2.1506589227177604.\n",
      "iter: 9.0, RMSE on training set: 2.0918589698662795.\n",
      "iter: 10.0, RMSE on training set: 2.0392749650854327.\n",
      "iter: 11.0, RMSE on training set: 1.9920755622374824.\n",
      "iter: 12.0, RMSE on training set: 1.949543027272273.\n",
      "iter: 13.0, RMSE on training set: 1.9110591555029883.\n",
      "iter: 14.0, RMSE on training set: 1.8760924395755691.\n",
      "iter: 15.0, RMSE on training set: 1.8441865107720288.\n",
      "iter: 16.0, RMSE on training set: 1.8149498360746452.\n",
      "iter: 17.0, RMSE on training set: 1.7880466202723584.\n",
      "iter: 18.0, RMSE on training set: 1.763188838006205.\n",
      "iter: 19.0, RMSE on training set: 1.740129304928859.\n",
      "iter: 20.0, RMSE on training set: 1.7186556889934048.\n",
      "iter: 21.0, RMSE on training set: 1.6985853607671593.\n",
      "iter: 22.0, RMSE on training set: 1.6797609840183791.\n",
      "iter: 23.0, RMSE on training set: 1.6620467532315155.\n",
      "iter: 24.0, RMSE on training set: 1.6453251919953749.\n",
      "iter: 25.0, RMSE on training set: 1.6294944344610105.\n",
      "iter: 26.0, RMSE on training set: 1.6144659206067447.\n",
      "iter: 27.0, RMSE on training set: 1.6001624444076923.\n",
      "iter: 28.0, RMSE on training set: 1.5865165018872058.\n",
      "iter: 29.0, RMSE on training set: 1.5734688932574752.\n",
      "iter: 30.0, RMSE on training set: 1.5609675398599663.\n",
      "iter: 31.0, RMSE on training set: 1.5489664823792688.\n",
      "iter: 32.0, RMSE on training set: 1.5374250318510028.\n",
      "iter: 33.0, RMSE on training set: 1.5263070493615978.\n",
      "iter: 34.0, RMSE on training set: 1.515580334102754.\n",
      "iter: 35.0, RMSE on training set: 1.5052161026589794.\n",
      "iter: 36.0, RMSE on training set: 1.4951885451373188.\n",
      "iter: 37.0, RMSE on training set: 1.4854744460575666.\n",
      "iter: 38.0, RMSE on training set: 1.4760528598679963.\n",
      "iter: 39.0, RMSE on training set: 1.4669048325894816.\n",
      "iter: 40.0, RMSE on training set: 1.4580131624664743.\n",
      "iter: 41.0, RMSE on training set: 1.4493621936568162.\n",
      "iter: 42.0, RMSE on training set: 1.440937637958127.\n",
      "iter: 43.0, RMSE on training set: 1.4327264203760235.\n",
      "iter: 44.0, RMSE on training set: 1.4247165450141406.\n",
      "iter: 45.0, RMSE on training set: 1.4168969783293368.\n",
      "iter: 46.0, RMSE on training set: 1.4092575472660198.\n",
      "iter: 47.0, RMSE on training set: 1.4017888501762632.\n",
      "iter: 48.0, RMSE on training set: 1.3944821787603745.\n",
      "iter: 49.0, RMSE on training set: 1.3873294495360375.\n",
      "iter: 50.0, RMSE on training set: 1.3803231435725267.\n",
      "iter: 51.0, RMSE on training set: 1.3734562534168318.\n",
      "iter: 52.0, RMSE on training set: 1.3667222362974638.\n",
      "iter: 53.0, RMSE on training set: 1.3601149728243156.\n",
      "iter: 54.0, RMSE on training set: 1.3536287305136576.\n",
      "iter: 55.0, RMSE on training set: 1.3472581315596857.\n",
      "iter: 56.0, RMSE on training set: 1.3409981243513214.\n",
      "iter: 57.0, RMSE on training set: 1.334843958297346.\n",
      "iter: 58.0, RMSE on training set: 1.3287911615771115.\n",
      "iter: 59.0, RMSE on training set: 1.3228355214794054.\n",
      "iter: 60.0, RMSE on training set: 1.3169730670305941.\n",
      "iter: 61.0, RMSE on training set: 1.3112000536458304.\n",
      "iter: 62.0, RMSE on training set: 1.3055129495654407.\n",
      "iter: 63.0, RMSE on training set: 1.2999084238630858.\n",
      "iter: 64.0, RMSE on training set: 1.29438333583392.\n",
      "iter: 65.0, RMSE on training set: 1.2889347255900612.\n",
      "iter: 66.0, RMSE on training set: 1.2835598057077426.\n",
      "iter: 67.0, RMSE on training set: 1.278255953785729.\n",
      "iter: 68.0, RMSE on training set: 1.2730207057882998.\n",
      "iter: 69.0, RMSE on training set: 1.267851750058399.\n",
      "iter: 70.0, RMSE on training set: 1.2627469218977694.\n",
      "iter: 71.0, RMSE on training set: 1.2577041986210435.\n",
      "iter: 72.0, RMSE on training set: 1.2527216950001052.\n",
      "iter: 73.0, RMSE on training set: 1.2477976590236646.\n",
      "iter: 74.0, RMSE on training set: 1.242930467904914.\n",
      "iter: 75.0, RMSE on training set: 1.238118624277597.\n",
      "iter: 76.0, RMSE on training set: 1.2333607525276744.\n",
      "iter: 77.0, RMSE on training set: 1.2286555952142264.\n",
      "iter: 78.0, RMSE on training set: 1.2240020095392434.\n",
      "iter: 79.0, RMSE on training set: 1.219398963831547.\n",
      "iter: 80.0, RMSE on training set: 1.214845534015374.\n",
      "iter: 81.0, RMSE on training set: 1.2103409000390697.\n",
      "iter: 82.0, RMSE on training set: 1.2058843422439653.\n",
      "iter: 83.0, RMSE on training set: 1.2014752376578703.\n",
      "iter: 84.0, RMSE on training set: 1.1971130562016397.\n",
      "iter: 85.0, RMSE on training set: 1.1927973568010817.\n",
      "iter: 86.0, RMSE on training set: 1.1885277833999905.\n",
      "iter: 87.0, RMSE on training set: 1.184304060873265.\n",
      "iter: 88.0, RMSE on training set: 1.1801259908420894.\n",
      "iter: 89.0, RMSE on training set: 1.1759934473957403.\n",
      "iter: 90.0, RMSE on training set: 1.1719063727270331.\n",
      "iter: 91.0, RMSE on training set: 1.167864772690451.\n",
      "iter: 92.0, RMSE on training set: 1.1638687122939024.\n",
      "iter: 93.0, RMSE on training set: 1.1599183111364988.\n",
      "iter: 94.0, RMSE on training set: 1.1560137388061325.\n",
      "iter: 95.0, RMSE on training set: 1.152155210251541.\n",
      "iter: 96.0, RMSE on training set: 1.1483429811443606.\n",
      "iter: 97.0, RMSE on training set: 1.144577343247124.\n",
      "iter: 98.0, RMSE on training set: 1.1408586198034265.\n",
      "iter: 99.0, RMSE on training set: 1.137187160966526.\n",
      "iter: 100.0, RMSE on training set: 1.133563339282457.\n",
      "iter: 101.0, RMSE on training set: 1.1299875452434098.\n",
      "iter: 102.0, RMSE on training set: 1.1264601829266339.\n",
      "iter: 103.0, RMSE on training set: 1.1229816657334972.\n",
      "iter: 104.0, RMSE on training set: 1.1195524122426386.\n",
      "iter: 105.0, RMSE on training set: 1.1161728421903083.\n",
      "iter: 106.0, RMSE on training set: 1.1128433725901763.\n",
      "iter: 107.0, RMSE on training set: 1.1095644140039398.\n",
      "iter: 108.0, RMSE on training set: 1.1063363669731185.\n",
      "iter: 109.0, RMSE on training set: 1.1031596186214798.\n",
      "iter: 110.0, RMSE on training set: 1.1000345394364839.\n",
      "iter: 111.0, RMSE on training set: 1.0969614802371752.\n",
      "iter: 112.0, RMSE on training set: 1.0939407693348857.\n",
      "iter: 113.0, RMSE on training set: 1.0909727098921183.\n",
      "iter: 114.0, RMSE on training set: 1.088057577483889.\n",
      "iter: 115.0, RMSE on training set: 1.0851956178648468.\n",
      "iter: 116.0, RMSE on training set: 1.0823870449443806.\n",
      "iter: 117.0, RMSE on training set: 1.079632038970979.\n",
      "iter: 118.0, RMSE on training set: 1.076930744926076.\n",
      "iter: 119.0, RMSE on training set: 1.0742832711266361.\n",
      "iter: 120.0, RMSE on training set: 1.0716896880348525.\n",
      "iter: 121.0, RMSE on training set: 1.0691500272723484.\n",
      "iter: 122.0, RMSE on training set: 1.0666642808354825.\n",
      "iter: 123.0, RMSE on training set: 1.064232400507507.\n",
      "iter: 124.0, RMSE on training set: 1.061854297462582.\n",
      "iter: 125.0, RMSE on training set: 1.0595298420559118.\n",
      "iter: 126.0, RMSE on training set: 1.0572588637936327.\n",
      "iter: 127.0, RMSE on training set: 1.0550411514754612.\n",
      "iter: 128.0, RMSE on training set: 1.0528764535025639.\n",
      "iter: 129.0, RMSE on training set: 1.0507644783426207.\n",
      "iter: 130.0, RMSE on training set: 1.0487048951436038.\n",
      "iter: 131.0, RMSE on training set: 1.046697334487469.\n",
      "iter: 132.0, RMSE on training set: 1.044741389274569.\n",
      "iter: 133.0, RMSE on training set: 1.0428366157294342.\n",
      "iter: 134.0, RMSE on training set: 1.0409825345183268.\n",
      "iter: 135.0, RMSE on training set: 1.0391786319688625.\n",
      "iter: 136.0, RMSE on training set: 1.037424361381951.\n",
      "iter: 137.0, RMSE on training set: 1.0357191444262908.\n",
      "iter: 138.0, RMSE on training set: 1.0340623726057043.\n",
      "iter: 139.0, RMSE on training set: 1.032453408789732.\n",
      "iter: 140.0, RMSE on training set: 1.0308915887980297.\n",
      "iter: 141.0, RMSE on training set: 1.0293762230293635.\n",
      "iter: 142.0, RMSE on training set: 1.0279065981261855.\n",
      "iter: 143.0, RMSE on training set: 1.0264819786661297.\n",
      "iter: 144.0, RMSE on training set: 1.0251016088720142.\n",
      "iter: 145.0, RMSE on training set: 1.0237647143323487.\n",
      "iter: 146.0, RMSE on training set: 1.0224705037246753.\n",
      "iter: 147.0, RMSE on training set: 1.0212181705344958.\n",
      "iter: 148.0, RMSE on training set: 1.020006894762938.\n",
      "iter: 149.0, RMSE on training set: 1.0188358446167638.\n",
      "iter: 150.0, RMSE on training set: 1.0177041781747411.\n",
      "iter: 151.0, RMSE on training set: 1.016611045024874.\n",
      "iter: 152.0, RMSE on training set: 1.0155555878674312.\n",
      "iter: 153.0, RMSE on training set: 1.014536944079172.\n",
      "iter: 154.0, RMSE on training set: 1.0135542472346215.\n",
      "iter: 155.0, RMSE on training set: 1.0126066285807058.\n",
      "iter: 156.0, RMSE on training set: 1.011693218461488.\n",
      "iter: 157.0, RMSE on training set: 1.0108131476901934.\n",
      "iter: 158.0, RMSE on training set: 1.0099655488661057.\n",
      "iter: 159.0, RMSE on training set: 1.0091495576343674.\n",
      "iter: 160.0, RMSE on training set: 1.0083643138870448.\n",
      "iter: 161.0, RMSE on training set: 1.007608962904249.\n",
      "iter: 162.0, RMSE on training set: 1.0068826564344022.\n",
      "iter: 163.0, RMSE on training set: 1.006184553713112.\n",
      "iter: 164.0, RMSE on training set: 1.0055138224204048.\n",
      "iter: 165.0, RMSE on training set: 1.0048696395763415.\n",
      "iter: 166.0, RMSE on training set: 1.0042511923753539.\n",
      "iter: 167.0, RMSE on training set: 1.0036576789598155.\n",
      "iter: 168.0, RMSE on training set: 1.0030883091336416.\n",
      "iter: 169.0, RMSE on training set: 1.0025423050168702.\n",
      "iter: 170.0, RMSE on training set: 1.0020189016423795.\n",
      "iter: 171.0, RMSE on training set: 1.0015173474960257.\n",
      "iter: 172.0, RMSE on training set: 1.0010369050016446.\n",
      "iter: 173.0, RMSE on training set: 1.00057685095247.\n",
      "iter: 174.0, RMSE on training set: 1.0001364768906074.\n",
      "iter: 175.0, RMSE on training set: 0.9997150894362942.\n",
      "iter: 176.0, RMSE on training set: 0.9993120105687514.\n",
      "iter: 177.0, RMSE on training set: 0.9989265778604557.\n",
      "iter: 178.0, RMSE on training set: 0.9985581446667368.\n",
      "iter: 179.0, RMSE on training set: 0.9982060802725955.\n",
      "iter: 180.0, RMSE on training set: 0.9978697699986987.\n",
      "iter: 181.0, RMSE on training set: 0.9975486152684686.\n",
      "iter: 182.0, RMSE on training set: 0.997242033638235.\n",
      "iter: 183.0, RMSE on training set: 0.9969494587923851.\n",
      "iter: 184.0, RMSE on training set: 0.9966703405054541.\n",
      "iter: 185.0, RMSE on training set: 0.9964041445730819.\n",
      "iter: 186.0, RMSE on training set: 0.9961503527137577.\n",
      "iter: 187.0, RMSE on training set: 0.995908462443239.\n",
      "iter: 188.0, RMSE on training set: 0.9956779869235384.\n",
      "iter: 189.0, RMSE on training set: 0.9954584547883318.\n",
      "iter: 190.0, RMSE on training set: 0.9952494099466169.\n",
      "iter: 191.0, RMSE on training set: 0.995050411366437.\n",
      "iter: 192.0, RMSE on training set: 0.9948610328404519.\n",
      "iter: 193.0, RMSE on training set: 0.9946808627350946.\n",
      "iter: 194.0, RMSE on training set: 0.994509503725018.\n",
      "iter: 195.0, RMSE on training set: 0.9943465725145186.\n",
      "iter: 196.0, RMSE on training set: 0.9941916995475322.\n",
      "iter: 197.0, RMSE on training set: 0.9940445287077938.\n",
      "iter: 198.0, RMSE on training set: 0.9939047170106623.\n",
      "iter: 199.0, RMSE on training set: 0.9937719342880723.\n",
      "iter: 200.0, RMSE on training set: 0.9936458628679986.\n",
      "iter: 201.0, RMSE on training set: 0.9935261972497534.\n",
      "iter: 202.0, RMSE on training set: 0.993412643776371.\n",
      "iter: 203.0, RMSE on training set: 0.9933049203052366.\n",
      "iter: 204.0, RMSE on training set: 0.9932027558780667.\n",
      "iter: 205.0, RMSE on training set: 0.993105890391254.\n",
      "RMSE on test data: 1.0128703895487268.\n",
      "RMSE on train data: 0.993105890391254.\n",
      "RMSE on test data: 1.0128703895487268.\n"
     ]
    }
   ],
   "source": [
    "%run run.py 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running num_features=1\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1.0, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964587238475606.\n",
      "Running 2th fold in 5 folds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-286ac0b41682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running num_features={n}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n\u001b[0;32m---> 22\u001b[0;31m                                                              num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m## Calculate mean and standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(ratings, K, method, num_items_per_user, num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item, gamma)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running {}th fold in {} folds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m### Split data in kth fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffled_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m### Matrix factorization using SGD/ALS/CCD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/cross_validation.py\u001b[0m in \u001b[0;36mk_fold_generator\u001b[0;34m(X, K, kth_fold, batch_size, data_size, shuffled_index)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffled_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetxor1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_val_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_val_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXdense\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[0;34m]\u001b[0m                  \u001b[0;31m## Training data indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m## Return sparse matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcsr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mtolil\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtolil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mlil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# lil_matrix needs sorted column indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized lil_matrix constructor usage'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5         ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features_arr = [1, 3, 5, 7, 10, 13, 15]   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.7\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(num_features_arr))\n",
    "train_rmse_std = np.zeros(len(num_features_arr))\n",
    "validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "for i, num_features in enumerate(num_features_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running num_features={n}'.format(n=num_features))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    ## Calculate mean and standard deviation    \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(num_features_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(num_features_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Number of features (K)'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99645032  0.99644168  0.          0.          0.          0.          0.        ]\n",
      "[  1.11022302e-16   1.11022302e-16   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[ 0.99645154  0.99644258  0.          0.          0.          0.          0.        ]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_rmse_mean)\n",
    "print(train_rmse_std)\n",
    "print(validation_rmse_mean)\n",
    "print(validation_rmse_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lambda_user=0.01\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483779205621994.\n",
      "iter: 1.0, RMSE on training set: 1.862727347449762.\n",
      "iter: 2.0, RMSE on training set: 1.7872175860855795.\n",
      "iter: 3.0, RMSE on training set: 1.7205475041220912.\n",
      "iter: 4.0, RMSE on training set: 1.661578195998678.\n",
      "iter: 5.0, RMSE on training set: 1.6092995189186037.\n",
      "iter: 6.0, RMSE on training set: 1.5628209137467963.\n",
      "iter: 7.0, RMSE on training set: 1.5213619368710363.\n",
      "iter: 8.0, RMSE on training set: 1.4842425041715162.\n",
      "iter: 9.0, RMSE on training set: 1.4508730741511724.\n",
      "iter: 10.0, RMSE on training set: 1.4207450030579345.\n",
      "iter: 11.0, RMSE on training set: 1.3934212788117923.\n",
      "iter: 12.0, RMSE on training set: 1.3685277973314718.\n",
      "iter: 13.0, RMSE on training set: 1.3457452954175693.\n",
      "iter: 14.0, RMSE on training set: 1.324802006513885.\n",
      "iter: 15.0, RMSE on training set: 1.3054670641842618.\n",
      "iter: 16.0, RMSE on training set: 1.2875446451985857.\n",
      "iter: 17.0, RMSE on training set: 1.2708688200732612.\n",
      "iter: 18.0, RMSE on training set: 1.255299062979451.\n",
      "iter: 19.0, RMSE on training set: 1.2407163637716345.\n",
      "iter: 20.0, RMSE on training set: 1.2270198809860613.\n",
      "iter: 21.0, RMSE on training set: 1.2141240745759287.\n",
      "iter: 22.0, RMSE on training set: 1.201956259652497.\n",
      "iter: 23.0, RMSE on training set: 1.1904545266014612.\n",
      "iter: 24.0, RMSE on training set: 1.1795659778983676.\n",
      "iter: 25.0, RMSE on training set: 1.1692452372253204.\n",
      "iter: 26.0, RMSE on training set: 1.1594531917359772.\n",
      "iter: 27.0, RMSE on training set: 1.1501559333001476.\n",
      "iter: 28.0, RMSE on training set: 1.1413238691517926.\n",
      "iter: 29.0, RMSE on training set: 1.132930976499839.\n",
      "iter: 30.0, RMSE on training set: 1.1249541793201105.\n",
      "iter: 31.0, RMSE on training set: 1.1173728287387117.\n",
      "iter: 32.0, RMSE on training set: 1.1101682711711987.\n",
      "iter: 33.0, RMSE on training set: 1.1033234907382183.\n",
      "iter: 34.0, RMSE on training set: 1.0968228144829233.\n",
      "iter: 35.0, RMSE on training set: 1.0906516706161031.\n",
      "iter: 36.0, RMSE on training set: 1.0847963914573158.\n",
      "iter: 37.0, RMSE on training set: 1.0792440539656993.\n",
      "iter: 38.0, RMSE on training set: 1.0739823517979104.\n",
      "iter: 39.0, RMSE on training set: 1.0689994937216385.\n",
      "iter: 40.0, RMSE on training set: 1.0642841239743102.\n",
      "iter: 41.0, RMSE on training set: 1.0598252608063692.\n",
      "iter: 42.0, RMSE on training set: 1.0556122500025436.\n",
      "iter: 43.0, RMSE on training set: 1.051634730646674.\n",
      "iter: 44.0, RMSE on training set: 1.0478826107989343.\n",
      "iter: 45.0, RMSE on training set: 1.044346051100054.\n",
      "iter: 46.0, RMSE on training set: 1.0410154546155663.\n",
      "iter: 47.0, RMSE on training set: 1.0378814614921335.\n",
      "iter: 48.0, RMSE on training set: 1.0349349472237062.\n",
      "iter: 49.0, RMSE on training set: 1.0321670235226197.\n",
      "iter: 50.0, RMSE on training set: 1.0295690409629397.\n",
      "iter: 51.0, RMSE on training set: 1.0271325927136383.\n",
      "iter: 52.0, RMSE on training set: 1.0248495188096116.\n",
      "iter: 53.0, RMSE on training set: 1.0227119105213276.\n",
      "iter: 54.0, RMSE on training set: 1.0207121144807316.\n",
      "iter: 55.0, RMSE on training set: 1.0188427363035497.\n",
      "iter: 56.0, RMSE on training set: 1.017096643517871.\n",
      "iter: 57.0, RMSE on training set: 1.0154669676673025.\n",
      "iter: 58.0, RMSE on training set: 1.013947105505472.\n",
      "iter: 59.0, RMSE on training set: 1.0125307192385773.\n",
      "iter: 60.0, RMSE on training set: 1.0112117358051582.\n",
      "iter: 61.0, RMSE on training set: 1.0099843452084645.\n",
      "iter: 62.0, RMSE on training set: 1.0088429979374227.\n",
      "iter: 63.0, RMSE on training set: 1.007782401528168.\n",
      "iter: 64.0, RMSE on training set: 1.0067975163298752.\n",
      "iter: 65.0, RMSE on training set: 1.0058835505468757.\n",
      "iter: 66.0, RMSE on training set: 1.005035954634191.\n",
      "iter: 67.0, RMSE on training set: 1.0042504151262228.\n",
      "iter: 68.0, RMSE on training set: 1.0035228479788028.\n",
      "iter: 69.0, RMSE on training set: 1.0028493915035535.\n",
      "iter: 70.0, RMSE on training set: 1.0022263989709224.\n",
      "iter: 71.0, RMSE on training set: 1.001650430954617.\n",
      "iter: 72.0, RMSE on training set: 1.001118247485842.\n",
      "iter: 73.0, RMSE on training set: 1.0006268000807654.\n",
      "iter: 74.0, RMSE on training set: 1.0001732236995202.\n",
      "iter: 75.0, RMSE on training set: 0.9997548286896443.\n",
      "iter: 76.0, RMSE on training set: 0.9993690927614612.\n",
      "iter: 77.0, RMSE on training set: 0.9990136530376758.\n",
      "iter: 78.0, RMSE on training set: 0.9986862982142485.\n",
      "iter: 79.0, RMSE on training set: 0.9983849608648175.\n",
      "iter: 80.0, RMSE on training set: 0.9981077099162533.\n",
      "iter: 81.0, RMSE on training set: 0.9978527433186322.\n",
      "iter: 82.0, RMSE on training set: 0.997618380928937.\n",
      "iter: 83.0, RMSE on training set: 0.9974030576240738.\n",
      "iter: 84.0, RMSE on training set: 0.9972053166554243.\n",
      "iter: 85.0, RMSE on training set: 0.9970238032541001.\n",
      "iter: 86.0, RMSE on training set: 0.9968572584932456.\n",
      "iter: 87.0, RMSE on training set: 0.9967045134112896.\n",
      "iter: 88.0, RMSE on training set: 0.9965644833977858.\n",
      "iter: 89.0, RMSE on training set: 0.99643616284153.\n",
      "iter: 90.0, RMSE on training set: 0.996318620038935.\n",
      "iter: 91.0, RMSE on training set: 0.996210992359149.\n",
      "iter: 92.0, RMSE on training set: 0.9961124816611646.\n",
      "RMSE on test data: 0.9961220929336961.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483779205621994.\n",
      "iter: 1.0, RMSE on training set: 1.862727347449762.\n",
      "iter: 2.0, RMSE on training set: 1.7872175860855795.\n",
      "iter: 3.0, RMSE on training set: 1.7205475041220912.\n",
      "iter: 4.0, RMSE on training set: 1.661578195998678.\n",
      "iter: 5.0, RMSE on training set: 1.6092995189186037.\n",
      "iter: 6.0, RMSE on training set: 1.5628209137467963.\n",
      "iter: 7.0, RMSE on training set: 1.5213619368710363.\n",
      "iter: 8.0, RMSE on training set: 1.4842425041715162.\n",
      "iter: 9.0, RMSE on training set: 1.4508730741511724.\n",
      "iter: 10.0, RMSE on training set: 1.4207450030579345.\n",
      "iter: 11.0, RMSE on training set: 1.3934212788117923.\n",
      "iter: 12.0, RMSE on training set: 1.3685277973314718.\n",
      "iter: 13.0, RMSE on training set: 1.3457452954175693.\n",
      "iter: 14.0, RMSE on training set: 1.324802006513885.\n",
      "iter: 15.0, RMSE on training set: 1.3054670641842618.\n",
      "iter: 16.0, RMSE on training set: 1.2875446451985857.\n",
      "iter: 17.0, RMSE on training set: 1.2708688200732612.\n",
      "iter: 18.0, RMSE on training set: 1.255299062979451.\n",
      "iter: 19.0, RMSE on training set: 1.2407163637716345.\n",
      "iter: 20.0, RMSE on training set: 1.2270198809860613.\n",
      "iter: 21.0, RMSE on training set: 1.2141240745759287.\n",
      "iter: 22.0, RMSE on training set: 1.201956259652497.\n",
      "iter: 23.0, RMSE on training set: 1.1904545266014612.\n",
      "iter: 24.0, RMSE on training set: 1.1795659778983676.\n",
      "iter: 25.0, RMSE on training set: 1.1692452372253204.\n",
      "iter: 26.0, RMSE on training set: 1.1594531917359772.\n",
      "iter: 27.0, RMSE on training set: 1.1501559333001476.\n",
      "iter: 28.0, RMSE on training set: 1.1413238691517926.\n",
      "iter: 29.0, RMSE on training set: 1.132930976499839.\n",
      "iter: 30.0, RMSE on training set: 1.1249541793201105.\n",
      "iter: 31.0, RMSE on training set: 1.1173728287387117.\n",
      "iter: 32.0, RMSE on training set: 1.1101682711711987.\n",
      "iter: 33.0, RMSE on training set: 1.1033234907382183.\n",
      "iter: 34.0, RMSE on training set: 1.0968228144829233.\n",
      "iter: 35.0, RMSE on training set: 1.0906516706161031.\n",
      "iter: 36.0, RMSE on training set: 1.0847963914573158.\n",
      "iter: 37.0, RMSE on training set: 1.0792440539656993.\n",
      "iter: 38.0, RMSE on training set: 1.0739823517979104.\n",
      "iter: 39.0, RMSE on training set: 1.0689994937216385.\n",
      "iter: 40.0, RMSE on training set: 1.0642841239743102.\n",
      "iter: 41.0, RMSE on training set: 1.0598252608063692.\n",
      "iter: 42.0, RMSE on training set: 1.0556122500025436.\n",
      "iter: 43.0, RMSE on training set: 1.051634730646674.\n",
      "iter: 44.0, RMSE on training set: 1.0478826107989343.\n",
      "iter: 45.0, RMSE on training set: 1.044346051100054.\n",
      "iter: 46.0, RMSE on training set: 1.0410154546155663.\n",
      "iter: 47.0, RMSE on training set: 1.0378814614921335.\n",
      "iter: 48.0, RMSE on training set: 1.0349349472237062.\n",
      "iter: 49.0, RMSE on training set: 1.0321670235226197.\n",
      "iter: 50.0, RMSE on training set: 1.0295690409629397.\n",
      "iter: 51.0, RMSE on training set: 1.0271325927136383.\n",
      "iter: 52.0, RMSE on training set: 1.0248495188096116.\n",
      "iter: 53.0, RMSE on training set: 1.0227119105213276.\n",
      "iter: 54.0, RMSE on training set: 1.0207121144807316.\n",
      "iter: 55.0, RMSE on training set: 1.0188427363035497.\n",
      "iter: 56.0, RMSE on training set: 1.017096643517871.\n",
      "iter: 57.0, RMSE on training set: 1.0154669676673025.\n",
      "iter: 58.0, RMSE on training set: 1.013947105505472.\n",
      "iter: 59.0, RMSE on training set: 1.0125307192385773.\n",
      "iter: 60.0, RMSE on training set: 1.0112117358051582.\n",
      "iter: 61.0, RMSE on training set: 1.0099843452084645.\n",
      "iter: 62.0, RMSE on training set: 1.0088429979374227.\n",
      "iter: 63.0, RMSE on training set: 1.007782401528168.\n",
      "iter: 64.0, RMSE on training set: 1.0067975163298752.\n",
      "iter: 65.0, RMSE on training set: 1.0058835505468757.\n",
      "iter: 66.0, RMSE on training set: 1.005035954634191.\n",
      "iter: 67.0, RMSE on training set: 1.0042504151262228.\n",
      "iter: 68.0, RMSE on training set: 1.0035228479788028.\n",
      "iter: 69.0, RMSE on training set: 1.0028493915035535.\n",
      "iter: 70.0, RMSE on training set: 1.0022263989709224.\n",
      "iter: 71.0, RMSE on training set: 1.001650430954617.\n",
      "iter: 72.0, RMSE on training set: 1.001118247485842.\n",
      "iter: 73.0, RMSE on training set: 1.0006268000807654.\n",
      "iter: 74.0, RMSE on training set: 1.0001732236995202.\n",
      "iter: 75.0, RMSE on training set: 0.9997548286896443.\n",
      "iter: 76.0, RMSE on training set: 0.9993690927614612.\n",
      "iter: 77.0, RMSE on training set: 0.9990136530376758.\n",
      "iter: 78.0, RMSE on training set: 0.9986862982142485.\n",
      "iter: 79.0, RMSE on training set: 0.9983849608648175.\n",
      "iter: 80.0, RMSE on training set: 0.9981077099162533.\n",
      "iter: 81.0, RMSE on training set: 0.9978527433186322.\n",
      "iter: 82.0, RMSE on training set: 0.997618380928937.\n",
      "iter: 83.0, RMSE on training set: 0.9974030576240738.\n",
      "iter: 84.0, RMSE on training set: 0.9972053166554243.\n",
      "iter: 85.0, RMSE on training set: 0.9970238032541001.\n",
      "iter: 86.0, RMSE on training set: 0.9968572584932456.\n",
      "iter: 87.0, RMSE on training set: 0.9967045134112896.\n",
      "iter: 88.0, RMSE on training set: 0.9965644833977858.\n",
      "iter: 89.0, RMSE on training set: 0.99643616284153.\n",
      "iter: 90.0, RMSE on training set: 0.996318620038935.\n",
      "iter: 91.0, RMSE on training set: 0.996210992359149.\n",
      "iter: 92.0, RMSE on training set: 0.9961124816611646.\n",
      "RMSE on test data: 0.9961210239671617.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483779205621994.\n",
      "iter: 1.0, RMSE on training set: 1.862727347449762.\n",
      "iter: 2.0, RMSE on training set: 1.7872175860855795.\n",
      "iter: 3.0, RMSE on training set: 1.7205475041220912.\n",
      "iter: 4.0, RMSE on training set: 1.661578195998678.\n",
      "iter: 5.0, RMSE on training set: 1.6092995189186037.\n",
      "iter: 6.0, RMSE on training set: 1.5628209137467963.\n",
      "iter: 7.0, RMSE on training set: 1.5213619368710363.\n",
      "iter: 8.0, RMSE on training set: 1.4842425041715162.\n",
      "iter: 9.0, RMSE on training set: 1.4508730741511724.\n",
      "iter: 10.0, RMSE on training set: 1.4207450030579345.\n",
      "iter: 11.0, RMSE on training set: 1.3934212788117923.\n",
      "iter: 12.0, RMSE on training set: 1.3685277973314718.\n",
      "iter: 13.0, RMSE on training set: 1.3457452954175693.\n",
      "iter: 14.0, RMSE on training set: 1.324802006513885.\n",
      "iter: 15.0, RMSE on training set: 1.3054670641842618.\n",
      "iter: 16.0, RMSE on training set: 1.2875446451985857.\n",
      "iter: 17.0, RMSE on training set: 1.2708688200732612.\n",
      "iter: 18.0, RMSE on training set: 1.255299062979451.\n",
      "iter: 19.0, RMSE on training set: 1.2407163637716345.\n",
      "iter: 20.0, RMSE on training set: 1.2270198809860613.\n",
      "iter: 21.0, RMSE on training set: 1.2141240745759287.\n",
      "iter: 22.0, RMSE on training set: 1.201956259652497.\n",
      "iter: 23.0, RMSE on training set: 1.1904545266014612.\n",
      "iter: 24.0, RMSE on training set: 1.1795659778983676.\n",
      "iter: 25.0, RMSE on training set: 1.1692452372253204.\n",
      "iter: 26.0, RMSE on training set: 1.1594531917359772.\n",
      "iter: 27.0, RMSE on training set: 1.1501559333001476.\n",
      "iter: 28.0, RMSE on training set: 1.1413238691517926.\n",
      "iter: 29.0, RMSE on training set: 1.132930976499839.\n",
      "iter: 30.0, RMSE on training set: 1.1249541793201105.\n",
      "iter: 31.0, RMSE on training set: 1.1173728287387117.\n",
      "iter: 32.0, RMSE on training set: 1.1101682711711987.\n",
      "iter: 33.0, RMSE on training set: 1.1033234907382183.\n",
      "iter: 34.0, RMSE on training set: 1.0968228144829233.\n",
      "iter: 35.0, RMSE on training set: 1.0906516706161031.\n",
      "iter: 36.0, RMSE on training set: 1.0847963914573158.\n",
      "iter: 37.0, RMSE on training set: 1.0792440539656993.\n",
      "iter: 38.0, RMSE on training set: 1.0739823517979104.\n",
      "iter: 39.0, RMSE on training set: 1.0689994937216385.\n",
      "iter: 40.0, RMSE on training set: 1.0642841239743102.\n",
      "iter: 41.0, RMSE on training set: 1.0598252608063692.\n",
      "iter: 42.0, RMSE on training set: 1.0556122500025436.\n",
      "iter: 43.0, RMSE on training set: 1.051634730646674.\n",
      "iter: 44.0, RMSE on training set: 1.0478826107989343.\n",
      "iter: 45.0, RMSE on training set: 1.044346051100054.\n",
      "iter: 46.0, RMSE on training set: 1.0410154546155663.\n",
      "iter: 47.0, RMSE on training set: 1.0378814614921335.\n",
      "iter: 48.0, RMSE on training set: 1.0349349472237062.\n",
      "iter: 49.0, RMSE on training set: 1.0321670235226197.\n",
      "iter: 50.0, RMSE on training set: 1.0295690409629397.\n",
      "iter: 51.0, RMSE on training set: 1.0271325927136383.\n",
      "iter: 52.0, RMSE on training set: 1.0248495188096116.\n",
      "iter: 53.0, RMSE on training set: 1.0227119105213276.\n",
      "iter: 54.0, RMSE on training set: 1.0207121144807316.\n",
      "iter: 55.0, RMSE on training set: 1.0188427363035497.\n",
      "iter: 56.0, RMSE on training set: 1.017096643517871.\n",
      "iter: 57.0, RMSE on training set: 1.0154669676673025.\n",
      "iter: 58.0, RMSE on training set: 1.013947105505472.\n",
      "iter: 59.0, RMSE on training set: 1.0125307192385773.\n",
      "iter: 60.0, RMSE on training set: 1.0112117358051582.\n",
      "iter: 61.0, RMSE on training set: 1.0099843452084645.\n",
      "iter: 62.0, RMSE on training set: 1.0088429979374227.\n",
      "iter: 63.0, RMSE on training set: 1.007782401528168.\n",
      "iter: 64.0, RMSE on training set: 1.0067975163298752.\n",
      "iter: 65.0, RMSE on training set: 1.0058835505468757.\n",
      "iter: 66.0, RMSE on training set: 1.005035954634191.\n",
      "iter: 67.0, RMSE on training set: 1.0042504151262228.\n",
      "iter: 68.0, RMSE on training set: 1.0035228479788028.\n",
      "iter: 69.0, RMSE on training set: 1.0028493915035535.\n",
      "iter: 70.0, RMSE on training set: 1.0022263989709224.\n",
      "iter: 71.0, RMSE on training set: 1.001650430954617.\n",
      "iter: 72.0, RMSE on training set: 1.001118247485842.\n",
      "iter: 73.0, RMSE on training set: 1.0006268000807654.\n",
      "iter: 74.0, RMSE on training set: 1.0001732236995202.\n",
      "iter: 75.0, RMSE on training set: 0.9997548286896443.\n",
      "iter: 76.0, RMSE on training set: 0.9993690927614612.\n",
      "iter: 77.0, RMSE on training set: 0.9990136530376758.\n",
      "iter: 78.0, RMSE on training set: 0.9986862982142485.\n",
      "iter: 79.0, RMSE on training set: 0.9983849608648175.\n",
      "iter: 80.0, RMSE on training set: 0.9981077099162533.\n",
      "iter: 81.0, RMSE on training set: 0.9978527433186322.\n",
      "iter: 82.0, RMSE on training set: 0.997618380928937.\n",
      "iter: 83.0, RMSE on training set: 0.9974030576240738.\n",
      "iter: 84.0, RMSE on training set: 0.9972053166554243.\n",
      "iter: 85.0, RMSE on training set: 0.9970238032541001.\n",
      "iter: 86.0, RMSE on training set: 0.9968572584932456.\n",
      "iter: 87.0, RMSE on training set: 0.9967045134112896.\n",
      "iter: 88.0, RMSE on training set: 0.9965644833977858.\n",
      "iter: 89.0, RMSE on training set: 0.99643616284153.\n",
      "iter: 90.0, RMSE on training set: 0.996318620038935.\n",
      "iter: 91.0, RMSE on training set: 0.996210992359149.\n",
      "iter: 92.0, RMSE on training set: 0.9961124816611646.\n",
      "RMSE on test data: 0.9961143921689328.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483779205621994.\n",
      "iter: 1.0, RMSE on training set: 1.862727347449762.\n",
      "iter: 2.0, RMSE on training set: 1.7872175860855795.\n",
      "iter: 3.0, RMSE on training set: 1.7205475041220912.\n",
      "iter: 4.0, RMSE on training set: 1.661578195998678.\n",
      "iter: 5.0, RMSE on training set: 1.6092995189186037.\n",
      "iter: 6.0, RMSE on training set: 1.5628209137467963.\n",
      "iter: 7.0, RMSE on training set: 1.5213619368710363.\n",
      "iter: 8.0, RMSE on training set: 1.4842425041715162.\n",
      "iter: 9.0, RMSE on training set: 1.4508730741511724.\n",
      "iter: 10.0, RMSE on training set: 1.4207450030579345.\n",
      "iter: 11.0, RMSE on training set: 1.3934212788117923.\n",
      "iter: 12.0, RMSE on training set: 1.3685277973314718.\n",
      "iter: 13.0, RMSE on training set: 1.3457452954175693.\n",
      "iter: 14.0, RMSE on training set: 1.324802006513885.\n",
      "iter: 15.0, RMSE on training set: 1.3054670641842618.\n",
      "iter: 16.0, RMSE on training set: 1.2875446451985857.\n",
      "iter: 17.0, RMSE on training set: 1.2708688200732612.\n",
      "iter: 18.0, RMSE on training set: 1.255299062979451.\n",
      "iter: 19.0, RMSE on training set: 1.2407163637716345.\n",
      "iter: 20.0, RMSE on training set: 1.2270198809860613.\n",
      "iter: 21.0, RMSE on training set: 1.2141240745759287.\n",
      "iter: 22.0, RMSE on training set: 1.201956259652497.\n",
      "iter: 23.0, RMSE on training set: 1.1904545266014612.\n",
      "iter: 24.0, RMSE on training set: 1.1795659778983676.\n",
      "iter: 25.0, RMSE on training set: 1.1692452372253204.\n",
      "iter: 26.0, RMSE on training set: 1.1594531917359772.\n",
      "iter: 27.0, RMSE on training set: 1.1501559333001476.\n",
      "iter: 28.0, RMSE on training set: 1.1413238691517926.\n",
      "iter: 29.0, RMSE on training set: 1.132930976499839.\n",
      "iter: 30.0, RMSE on training set: 1.1249541793201105.\n",
      "iter: 31.0, RMSE on training set: 1.1173728287387117.\n",
      "iter: 32.0, RMSE on training set: 1.1101682711711987.\n",
      "iter: 33.0, RMSE on training set: 1.1033234907382183.\n",
      "iter: 34.0, RMSE on training set: 1.0968228144829233.\n",
      "iter: 35.0, RMSE on training set: 1.0906516706161031.\n",
      "iter: 36.0, RMSE on training set: 1.0847963914573158.\n",
      "iter: 37.0, RMSE on training set: 1.0792440539656993.\n",
      "iter: 38.0, RMSE on training set: 1.0739823517979104.\n",
      "iter: 39.0, RMSE on training set: 1.0689994937216385.\n",
      "iter: 40.0, RMSE on training set: 1.0642841239743102.\n",
      "iter: 41.0, RMSE on training set: 1.0598252608063692.\n",
      "iter: 42.0, RMSE on training set: 1.0556122500025436.\n",
      "iter: 43.0, RMSE on training set: 1.051634730646674.\n",
      "iter: 44.0, RMSE on training set: 1.0478826107989343.\n",
      "iter: 45.0, RMSE on training set: 1.044346051100054.\n",
      "iter: 46.0, RMSE on training set: 1.0410154546155663.\n",
      "iter: 47.0, RMSE on training set: 1.0378814614921335.\n",
      "iter: 48.0, RMSE on training set: 1.0349349472237062.\n",
      "iter: 49.0, RMSE on training set: 1.0321670235226197.\n",
      "iter: 50.0, RMSE on training set: 1.0295690409629397.\n",
      "iter: 51.0, RMSE on training set: 1.0271325927136383.\n",
      "iter: 52.0, RMSE on training set: 1.0248495188096116.\n",
      "iter: 53.0, RMSE on training set: 1.0227119105213276.\n",
      "iter: 54.0, RMSE on training set: 1.0207121144807316.\n",
      "iter: 55.0, RMSE on training set: 1.0188427363035497.\n",
      "iter: 56.0, RMSE on training set: 1.017096643517871.\n",
      "iter: 57.0, RMSE on training set: 1.0154669676673025.\n",
      "iter: 58.0, RMSE on training set: 1.013947105505472.\n",
      "iter: 59.0, RMSE on training set: 1.0125307192385773.\n",
      "iter: 60.0, RMSE on training set: 1.0112117358051582.\n",
      "iter: 61.0, RMSE on training set: 1.0099843452084645.\n",
      "iter: 62.0, RMSE on training set: 1.0088429979374227.\n",
      "iter: 63.0, RMSE on training set: 1.007782401528168.\n",
      "iter: 64.0, RMSE on training set: 1.0067975163298752.\n",
      "iter: 65.0, RMSE on training set: 1.0058835505468757.\n",
      "iter: 66.0, RMSE on training set: 1.005035954634191.\n",
      "iter: 67.0, RMSE on training set: 1.0042504151262228.\n",
      "iter: 68.0, RMSE on training set: 1.0035228479788028.\n",
      "iter: 69.0, RMSE on training set: 1.0028493915035535.\n",
      "iter: 70.0, RMSE on training set: 1.0022263989709224.\n",
      "iter: 71.0, RMSE on training set: 1.001650430954617.\n",
      "iter: 72.0, RMSE on training set: 1.001118247485842.\n",
      "iter: 73.0, RMSE on training set: 1.0006268000807654.\n",
      "iter: 74.0, RMSE on training set: 1.0001732236995202.\n",
      "iter: 75.0, RMSE on training set: 0.9997548286896443.\n",
      "iter: 76.0, RMSE on training set: 0.9993690927614612.\n",
      "iter: 77.0, RMSE on training set: 0.9990136530376758.\n",
      "iter: 78.0, RMSE on training set: 0.9986862982142485.\n",
      "iter: 79.0, RMSE on training set: 0.9983849608648175.\n",
      "iter: 80.0, RMSE on training set: 0.9981077099162533.\n",
      "iter: 81.0, RMSE on training set: 0.9978527433186322.\n",
      "iter: 82.0, RMSE on training set: 0.997618380928937.\n",
      "iter: 83.0, RMSE on training set: 0.9974030576240738.\n",
      "iter: 84.0, RMSE on training set: 0.9972053166554243.\n",
      "iter: 85.0, RMSE on training set: 0.9970238032541001.\n",
      "iter: 86.0, RMSE on training set: 0.9968572584932456.\n",
      "iter: 87.0, RMSE on training set: 0.9967045134112896.\n",
      "iter: 88.0, RMSE on training set: 0.9965644833977858.\n",
      "iter: 89.0, RMSE on training set: 0.99643616284153.\n",
      "iter: 90.0, RMSE on training set: 0.996318620038935.\n",
      "iter: 91.0, RMSE on training set: 0.996210992359149.\n",
      "iter: 92.0, RMSE on training set: 0.9961124816611646.\n",
      "RMSE on test data: 0.996103192594091.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483779205621994.\n",
      "iter: 1.0, RMSE on training set: 1.862727347449762.\n",
      "iter: 2.0, RMSE on training set: 1.7872175860855795.\n",
      "iter: 3.0, RMSE on training set: 1.7205475041220912.\n",
      "iter: 4.0, RMSE on training set: 1.661578195998678.\n",
      "iter: 5.0, RMSE on training set: 1.6092995189186037.\n",
      "iter: 6.0, RMSE on training set: 1.5628209137467963.\n",
      "iter: 7.0, RMSE on training set: 1.5213619368710363.\n",
      "iter: 8.0, RMSE on training set: 1.4842425041715162.\n",
      "iter: 9.0, RMSE on training set: 1.4508730741511724.\n",
      "iter: 10.0, RMSE on training set: 1.4207450030579345.\n",
      "iter: 11.0, RMSE on training set: 1.3934212788117923.\n",
      "iter: 12.0, RMSE on training set: 1.3685277973314718.\n",
      "iter: 13.0, RMSE on training set: 1.3457452954175693.\n",
      "iter: 14.0, RMSE on training set: 1.324802006513885.\n",
      "iter: 15.0, RMSE on training set: 1.3054670641842618.\n",
      "iter: 16.0, RMSE on training set: 1.2875446451985857.\n",
      "iter: 17.0, RMSE on training set: 1.2708688200732612.\n",
      "iter: 18.0, RMSE on training set: 1.255299062979451.\n",
      "iter: 19.0, RMSE on training set: 1.2407163637716345.\n",
      "iter: 20.0, RMSE on training set: 1.2270198809860613.\n",
      "iter: 21.0, RMSE on training set: 1.2141240745759287.\n",
      "iter: 22.0, RMSE on training set: 1.201956259652497.\n",
      "iter: 23.0, RMSE on training set: 1.1904545266014612.\n",
      "iter: 24.0, RMSE on training set: 1.1795659778983676.\n",
      "iter: 25.0, RMSE on training set: 1.1692452372253204.\n",
      "iter: 26.0, RMSE on training set: 1.1594531917359772.\n",
      "iter: 27.0, RMSE on training set: 1.1501559333001476.\n",
      "iter: 28.0, RMSE on training set: 1.1413238691517926.\n",
      "iter: 29.0, RMSE on training set: 1.132930976499839.\n",
      "iter: 30.0, RMSE on training set: 1.1249541793201105.\n",
      "iter: 31.0, RMSE on training set: 1.1173728287387117.\n",
      "iter: 32.0, RMSE on training set: 1.1101682711711987.\n",
      "iter: 33.0, RMSE on training set: 1.1033234907382183.\n",
      "iter: 34.0, RMSE on training set: 1.0968228144829233.\n",
      "iter: 35.0, RMSE on training set: 1.0906516706161031.\n",
      "iter: 36.0, RMSE on training set: 1.0847963914573158.\n",
      "iter: 37.0, RMSE on training set: 1.0792440539656993.\n",
      "iter: 38.0, RMSE on training set: 1.0739823517979104.\n",
      "iter: 39.0, RMSE on training set: 1.0689994937216385.\n",
      "iter: 40.0, RMSE on training set: 1.0642841239743102.\n",
      "iter: 41.0, RMSE on training set: 1.0598252608063692.\n",
      "iter: 42.0, RMSE on training set: 1.0556122500025436.\n",
      "iter: 43.0, RMSE on training set: 1.051634730646674.\n",
      "iter: 44.0, RMSE on training set: 1.0478826107989343.\n",
      "iter: 45.0, RMSE on training set: 1.044346051100054.\n",
      "iter: 46.0, RMSE on training set: 1.0410154546155663.\n",
      "iter: 47.0, RMSE on training set: 1.0378814614921335.\n",
      "iter: 48.0, RMSE on training set: 1.0349349472237062.\n",
      "iter: 49.0, RMSE on training set: 1.0321670235226197.\n",
      "iter: 50.0, RMSE on training set: 1.0295690409629397.\n",
      "iter: 51.0, RMSE on training set: 1.0271325927136383.\n",
      "iter: 52.0, RMSE on training set: 1.0248495188096116.\n",
      "iter: 53.0, RMSE on training set: 1.0227119105213276.\n",
      "iter: 54.0, RMSE on training set: 1.0207121144807316.\n",
      "iter: 55.0, RMSE on training set: 1.0188427363035497.\n",
      "iter: 56.0, RMSE on training set: 1.017096643517871.\n",
      "iter: 57.0, RMSE on training set: 1.0154669676673025.\n",
      "iter: 58.0, RMSE on training set: 1.013947105505472.\n",
      "iter: 59.0, RMSE on training set: 1.0125307192385773.\n",
      "iter: 60.0, RMSE on training set: 1.0112117358051582.\n",
      "iter: 61.0, RMSE on training set: 1.0099843452084645.\n",
      "iter: 62.0, RMSE on training set: 1.0088429979374227.\n",
      "iter: 63.0, RMSE on training set: 1.007782401528168.\n",
      "iter: 64.0, RMSE on training set: 1.0067975163298752.\n",
      "iter: 65.0, RMSE on training set: 1.0058835505468757.\n",
      "iter: 66.0, RMSE on training set: 1.005035954634191.\n",
      "iter: 67.0, RMSE on training set: 1.0042504151262228.\n",
      "iter: 68.0, RMSE on training set: 1.0035228479788028.\n",
      "iter: 69.0, RMSE on training set: 1.0028493915035535.\n",
      "iter: 70.0, RMSE on training set: 1.0022263989709224.\n",
      "iter: 71.0, RMSE on training set: 1.001650430954617.\n",
      "iter: 72.0, RMSE on training set: 1.001118247485842.\n",
      "iter: 73.0, RMSE on training set: 1.0006268000807654.\n",
      "iter: 74.0, RMSE on training set: 1.0001732236995202.\n",
      "iter: 75.0, RMSE on training set: 0.9997548286896443.\n",
      "iter: 76.0, RMSE on training set: 0.9993690927614612.\n",
      "iter: 77.0, RMSE on training set: 0.9990136530376758.\n",
      "iter: 78.0, RMSE on training set: 0.9986862982142485.\n",
      "iter: 79.0, RMSE on training set: 0.9983849608648175.\n",
      "iter: 80.0, RMSE on training set: 0.9981077099162533.\n",
      "iter: 81.0, RMSE on training set: 0.9978527433186322.\n",
      "iter: 82.0, RMSE on training set: 0.997618380928937.\n",
      "iter: 83.0, RMSE on training set: 0.9974030576240738.\n",
      "iter: 84.0, RMSE on training set: 0.9972053166554243.\n",
      "iter: 85.0, RMSE on training set: 0.9970238032541001.\n",
      "iter: 86.0, RMSE on training set: 0.9968572584932456.\n",
      "iter: 87.0, RMSE on training set: 0.9967045134112896.\n",
      "iter: 88.0, RMSE on training set: 0.9965644833977858.\n",
      "iter: 89.0, RMSE on training set: 0.99643616284153.\n",
      "iter: 90.0, RMSE on training set: 0.996318620038935.\n",
      "iter: 91.0, RMSE on training set: 0.996210992359149.\n",
      "iter: 92.0, RMSE on training set: 0.9961124816611646.\n",
      "RMSE on test data: 0.9961075634258394.\n",
      "Running lambda_user=0.1\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483712574660754.\n",
      "iter: 1.0, RMSE on training set: 1.8627131731984305.\n",
      "iter: 2.0, RMSE on training set: 1.7871972078721687.\n",
      "iter: 3.0, RMSE on training set: 1.7205219935899132.\n",
      "iter: 4.0, RMSE on training set: 1.66154842748036.\n",
      "iter: 5.0, RMSE on training set: 1.6092662021609976.\n",
      "iter: 6.0, RMSE on training set: 1.5627846228797284.\n",
      "iter: 7.0, RMSE on training set: 1.521323135697907.\n",
      "iter: 8.0, RMSE on training set: 1.4842015679286717.\n",
      "iter: 9.0, RMSE on training set: 1.450830307947034.\n",
      "iter: 10.0, RMSE on training set: 1.4207006572603167.\n",
      "iter: 11.0, RMSE on training set: 1.3933755616923975.\n",
      "iter: 12.0, RMSE on training set: 1.368480885296914.\n",
      "iter: 13.0, RMSE on training set: 1.3456973411577544.\n",
      "iter: 14.0, RMSE on training set: 1.324753145384815.\n",
      "iter: 15.0, RMSE on training set: 1.3054174191178949.\n",
      "iter: 16.0, RMSE on training set: 1.2874943304017297.\n",
      "iter: 17.0, RMSE on training set: 1.2708179437442624.\n",
      "iter: 18.0, RMSE on training set: 1.2552477292383772.\n",
      "iter: 19.0, RMSE on training set: 1.2406646739686216.\n",
      "iter: 20.0, RMSE on training set: 1.2269679345244813.\n",
      "iter: 21.0, RMSE on training set: 1.2140719693625548.\n",
      "iter: 22.0, RMSE on training set: 1.201904092265859.\n",
      "iter: 23.0, RMSE on training set: 1.1904023922520486.\n",
      "iter: 24.0, RMSE on training set: 1.1795139702397253.\n",
      "iter: 25.0, RMSE on training set: 1.1691934480631552.\n",
      "iter: 26.0, RMSE on training set: 1.159401710672666.\n",
      "iter: 27.0, RMSE on training set: 1.1501048473441735.\n",
      "iter: 28.0, RMSE on training set: 1.1412732623152901.\n",
      "iter: 29.0, RMSE on training set: 1.132880929402301.\n",
      "iter: 30.0, RMSE on training set: 1.1249047688121803.\n",
      "iter: 31.0, RMSE on training set: 1.1173241275565693.\n",
      "iter: 32.0, RMSE on training set: 1.110120347629312.\n",
      "iter: 33.0, RMSE on training set: 1.1032764084658706.\n",
      "iter: 34.0, RMSE on training set: 1.096776632207971.\n",
      "iter: 35.0, RMSE on training set: 1.0906064419977752.\n",
      "iter: 36.0, RMSE on training set: 1.0847521649684522.\n",
      "iter: 37.0, RMSE on training set: 1.079200872823668.\n",
      "iter: 38.0, RMSE on training set: 1.0739402539425094.\n",
      "iter: 39.0, RMSE on training set: 1.0689585118375773.\n",
      "iter: 40.0, RMSE on training set: 1.0642442855553018.\n",
      "iter: 41.0, RMSE on training set: 1.0597865882574686.\n",
      "iter: 42.0, RMSE on training set: 1.0555747607771049.\n",
      "iter: 43.0, RMSE on training set: 1.0515984374140814.\n",
      "iter: 44.0, RMSE on training set: 1.0478475216391592.\n",
      "iter: 45.0, RMSE on training set: 1.0443121697210325.\n",
      "iter: 46.0, RMSE on training set: 1.0409827805894247.\n",
      "iter: 47.0, RMSE on training set: 1.0378499905063507.\n",
      "iter: 48.0, RMSE on training set: 1.0349046713434908.\n",
      "iter: 49.0, RMSE on training set: 1.032137931460916.\n",
      "iter: 50.0, RMSE on training set: 1.0295411183547958.\n",
      "iter: 51.0, RMSE on training set: 1.027105822391873.\n",
      "iter: 52.0, RMSE on training set: 1.0248238810790469.\n",
      "iter: 53.0, RMSE on training set: 1.022687383429135.\n",
      "iter: 54.0, RMSE on training set: 1.02068867408075.\n",
      "iter: 55.0, RMSE on training set: 1.0188203569127197.\n",
      "iter: 56.0, RMSE on training set: 1.0170752979632154.\n",
      "iter: 57.0, RMSE on training set: 1.0154466275221492.\n",
      "iter: 58.0, RMSE on training set: 1.0139277413138807.\n",
      "iter: 59.0, RMSE on training set: 1.0125123007271515.\n",
      "iter: 60.0, RMSE on training set: 1.0111942320816607.\n",
      "iter: 61.0, RMSE on training set: 1.0099677249468564.\n",
      "iter: 62.0, RMSE on training set: 1.0088272295491338.\n",
      "iter: 63.0, RMSE on training set: 1.0077674533195713.\n",
      "iter: 64.0, RMSE on training set: 1.0067833566460977.\n",
      "iter: 65.0, RMSE on training set: 1.0058701479022025.\n",
      "iter: 66.0, RMSE on training set: 1.0050232778294228.\n",
      "iter: 67.0, RMSE on training set: 1.004238433353447.\n",
      "iter: 68.0, RMSE on training set: 1.0035115309141047.\n",
      "iter: 69.0, RMSE on training set: 1.002838709388244.\n",
      "iter: 70.0, RMSE on training set: 1.0022163226819119.\n",
      "iter: 71.0, RMSE on training set: 1.001640932064592.\n",
      "iter: 72.0, RMSE on training set: 1.001109298313883.\n",
      "iter: 73.0, RMSE on training set: 1.0006183737341128.\n",
      "iter: 74.0, RMSE on training set: 1.0001652941071348.\n",
      "iter: 75.0, RMSE on training set: 0.9997473706282263.\n",
      "iter: 76.0, RMSE on training set: 0.9993620818745823.\n",
      "iter: 77.0, RMSE on training set: 0.9990070658486225.\n",
      "iter: 78.0, RMSE on training set: 0.9986801121332104.\n",
      "iter: 79.0, RMSE on training set: 0.9983791541909603.\n",
      "iter: 80.0, RMSE on training set: 0.9981022618352374.\n",
      "iter: 81.0, RMSE on training set: 0.9978476338960816.\n",
      "iter: 82.0, RMSE on training set: 0.9976135911003439.\n",
      "iter: 83.0, RMSE on training set: 0.9973985691815807.\n",
      "iter: 84.0, RMSE on training set: 0.9972011122319058.\n",
      "iter: 85.0, RMSE on training set: 0.997019866304929.\n",
      "iter: 86.0, RMSE on training set: 0.9968535732760966.\n",
      "iter: 87.0, RMSE on training set: 0.9967010649643184.\n",
      "iter: 88.0, RMSE on training set: 0.9965612575164665.\n",
      "iter: 89.0, RMSE on training set: 0.9964331460544369.\n",
      "iter: 90.0, RMSE on training set: 0.9963157995827171.\n",
      "iter: 91.0, RMSE on training set: 0.996208356152913.\n",
      "iter: 92.0, RMSE on training set: 0.9961100182804874.\n",
      "RMSE on test data: 0.996119628195722.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483712574660754.\n",
      "iter: 1.0, RMSE on training set: 1.8627131731984305.\n",
      "iter: 2.0, RMSE on training set: 1.7871972078721687.\n",
      "iter: 3.0, RMSE on training set: 1.7205219935899132.\n",
      "iter: 4.0, RMSE on training set: 1.66154842748036.\n",
      "iter: 5.0, RMSE on training set: 1.6092662021609976.\n",
      "iter: 6.0, RMSE on training set: 1.5627846228797284.\n",
      "iter: 7.0, RMSE on training set: 1.521323135697907.\n",
      "iter: 8.0, RMSE on training set: 1.4842015679286717.\n",
      "iter: 9.0, RMSE on training set: 1.450830307947034.\n",
      "iter: 10.0, RMSE on training set: 1.4207006572603167.\n",
      "iter: 11.0, RMSE on training set: 1.3933755616923975.\n",
      "iter: 12.0, RMSE on training set: 1.368480885296914.\n",
      "iter: 13.0, RMSE on training set: 1.3456973411577544.\n",
      "iter: 14.0, RMSE on training set: 1.324753145384815.\n",
      "iter: 15.0, RMSE on training set: 1.3054174191178949.\n",
      "iter: 16.0, RMSE on training set: 1.2874943304017297.\n",
      "iter: 17.0, RMSE on training set: 1.2708179437442624.\n",
      "iter: 18.0, RMSE on training set: 1.2552477292383772.\n",
      "iter: 19.0, RMSE on training set: 1.2406646739686216.\n",
      "iter: 20.0, RMSE on training set: 1.2269679345244813.\n",
      "iter: 21.0, RMSE on training set: 1.2140719693625548.\n",
      "iter: 22.0, RMSE on training set: 1.201904092265859.\n",
      "iter: 23.0, RMSE on training set: 1.1904023922520486.\n",
      "iter: 24.0, RMSE on training set: 1.1795139702397253.\n",
      "iter: 25.0, RMSE on training set: 1.1691934480631552.\n",
      "iter: 26.0, RMSE on training set: 1.159401710672666.\n",
      "iter: 27.0, RMSE on training set: 1.1501048473441735.\n",
      "iter: 28.0, RMSE on training set: 1.1412732623152901.\n",
      "iter: 29.0, RMSE on training set: 1.132880929402301.\n",
      "iter: 30.0, RMSE on training set: 1.1249047688121803.\n",
      "iter: 31.0, RMSE on training set: 1.1173241275565693.\n",
      "iter: 32.0, RMSE on training set: 1.110120347629312.\n",
      "iter: 33.0, RMSE on training set: 1.1032764084658706.\n",
      "iter: 34.0, RMSE on training set: 1.096776632207971.\n",
      "iter: 35.0, RMSE on training set: 1.0906064419977752.\n",
      "iter: 36.0, RMSE on training set: 1.0847521649684522.\n",
      "iter: 37.0, RMSE on training set: 1.079200872823668.\n",
      "iter: 38.0, RMSE on training set: 1.0739402539425094.\n",
      "iter: 39.0, RMSE on training set: 1.0689585118375773.\n",
      "iter: 40.0, RMSE on training set: 1.0642442855553018.\n",
      "iter: 41.0, RMSE on training set: 1.0597865882574686.\n",
      "iter: 42.0, RMSE on training set: 1.0555747607771049.\n",
      "iter: 43.0, RMSE on training set: 1.0515984374140814.\n",
      "iter: 44.0, RMSE on training set: 1.0478475216391592.\n",
      "iter: 45.0, RMSE on training set: 1.0443121697210325.\n",
      "iter: 46.0, RMSE on training set: 1.0409827805894247.\n",
      "iter: 47.0, RMSE on training set: 1.0378499905063507.\n",
      "iter: 48.0, RMSE on training set: 1.0349046713434908.\n",
      "iter: 49.0, RMSE on training set: 1.032137931460916.\n",
      "iter: 50.0, RMSE on training set: 1.0295411183547958.\n",
      "iter: 51.0, RMSE on training set: 1.027105822391873.\n",
      "iter: 52.0, RMSE on training set: 1.0248238810790469.\n",
      "iter: 53.0, RMSE on training set: 1.022687383429135.\n",
      "iter: 54.0, RMSE on training set: 1.02068867408075.\n",
      "iter: 55.0, RMSE on training set: 1.0188203569127197.\n",
      "iter: 56.0, RMSE on training set: 1.0170752979632154.\n",
      "iter: 57.0, RMSE on training set: 1.0154466275221492.\n",
      "iter: 58.0, RMSE on training set: 1.0139277413138807.\n",
      "iter: 59.0, RMSE on training set: 1.0125123007271515.\n",
      "iter: 60.0, RMSE on training set: 1.0111942320816607.\n",
      "iter: 61.0, RMSE on training set: 1.0099677249468564.\n",
      "iter: 62.0, RMSE on training set: 1.0088272295491338.\n",
      "iter: 63.0, RMSE on training set: 1.0077674533195713.\n",
      "iter: 64.0, RMSE on training set: 1.0067833566460977.\n",
      "iter: 65.0, RMSE on training set: 1.0058701479022025.\n",
      "iter: 66.0, RMSE on training set: 1.0050232778294228.\n",
      "iter: 67.0, RMSE on training set: 1.004238433353447.\n",
      "iter: 68.0, RMSE on training set: 1.0035115309141047.\n",
      "iter: 69.0, RMSE on training set: 1.002838709388244.\n",
      "iter: 70.0, RMSE on training set: 1.0022163226819119.\n",
      "iter: 71.0, RMSE on training set: 1.001640932064592.\n",
      "iter: 72.0, RMSE on training set: 1.001109298313883.\n",
      "iter: 73.0, RMSE on training set: 1.0006183737341128.\n",
      "iter: 74.0, RMSE on training set: 1.0001652941071348.\n",
      "iter: 75.0, RMSE on training set: 0.9997473706282263.\n",
      "iter: 76.0, RMSE on training set: 0.9993620818745823.\n",
      "iter: 77.0, RMSE on training set: 0.9990070658486225.\n",
      "iter: 78.0, RMSE on training set: 0.9986801121332104.\n",
      "iter: 79.0, RMSE on training set: 0.9983791541909603.\n",
      "iter: 80.0, RMSE on training set: 0.9981022618352374.\n",
      "iter: 81.0, RMSE on training set: 0.9978476338960816.\n",
      "iter: 82.0, RMSE on training set: 0.9976135911003439.\n",
      "iter: 83.0, RMSE on training set: 0.9973985691815807.\n",
      "iter: 84.0, RMSE on training set: 0.9972011122319058.\n",
      "iter: 85.0, RMSE on training set: 0.997019866304929.\n",
      "iter: 86.0, RMSE on training set: 0.9968535732760966.\n",
      "iter: 87.0, RMSE on training set: 0.9967010649643184.\n",
      "iter: 88.0, RMSE on training set: 0.9965612575164665.\n",
      "iter: 89.0, RMSE on training set: 0.9964331460544369.\n",
      "iter: 90.0, RMSE on training set: 0.9963157995827171.\n",
      "iter: 91.0, RMSE on training set: 0.996208356152913.\n",
      "iter: 92.0, RMSE on training set: 0.9961100182804874.\n",
      "RMSE on test data: 0.9961185602579425.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483712574660754.\n",
      "iter: 1.0, RMSE on training set: 1.8627131731984305.\n",
      "iter: 2.0, RMSE on training set: 1.7871972078721687.\n",
      "iter: 3.0, RMSE on training set: 1.7205219935899132.\n",
      "iter: 4.0, RMSE on training set: 1.66154842748036.\n",
      "iter: 5.0, RMSE on training set: 1.6092662021609976.\n",
      "iter: 6.0, RMSE on training set: 1.5627846228797284.\n",
      "iter: 7.0, RMSE on training set: 1.521323135697907.\n",
      "iter: 8.0, RMSE on training set: 1.4842015679286717.\n",
      "iter: 9.0, RMSE on training set: 1.450830307947034.\n",
      "iter: 10.0, RMSE on training set: 1.4207006572603167.\n",
      "iter: 11.0, RMSE on training set: 1.3933755616923975.\n",
      "iter: 12.0, RMSE on training set: 1.368480885296914.\n",
      "iter: 13.0, RMSE on training set: 1.3456973411577544.\n",
      "iter: 14.0, RMSE on training set: 1.324753145384815.\n",
      "iter: 15.0, RMSE on training set: 1.3054174191178949.\n",
      "iter: 16.0, RMSE on training set: 1.2874943304017297.\n",
      "iter: 17.0, RMSE on training set: 1.2708179437442624.\n",
      "iter: 18.0, RMSE on training set: 1.2552477292383772.\n",
      "iter: 19.0, RMSE on training set: 1.2406646739686216.\n",
      "iter: 20.0, RMSE on training set: 1.2269679345244813.\n",
      "iter: 21.0, RMSE on training set: 1.2140719693625548.\n",
      "iter: 22.0, RMSE on training set: 1.201904092265859.\n",
      "iter: 23.0, RMSE on training set: 1.1904023922520486.\n",
      "iter: 24.0, RMSE on training set: 1.1795139702397253.\n",
      "iter: 25.0, RMSE on training set: 1.1691934480631552.\n",
      "iter: 26.0, RMSE on training set: 1.159401710672666.\n",
      "iter: 27.0, RMSE on training set: 1.1501048473441735.\n",
      "iter: 28.0, RMSE on training set: 1.1412732623152901.\n",
      "iter: 29.0, RMSE on training set: 1.132880929402301.\n",
      "iter: 30.0, RMSE on training set: 1.1249047688121803.\n",
      "iter: 31.0, RMSE on training set: 1.1173241275565693.\n",
      "iter: 32.0, RMSE on training set: 1.110120347629312.\n",
      "iter: 33.0, RMSE on training set: 1.1032764084658706.\n",
      "iter: 34.0, RMSE on training set: 1.096776632207971.\n",
      "iter: 35.0, RMSE on training set: 1.0906064419977752.\n",
      "iter: 36.0, RMSE on training set: 1.0847521649684522.\n",
      "iter: 37.0, RMSE on training set: 1.079200872823668.\n",
      "iter: 38.0, RMSE on training set: 1.0739402539425094.\n",
      "iter: 39.0, RMSE on training set: 1.0689585118375773.\n",
      "iter: 40.0, RMSE on training set: 1.0642442855553018.\n",
      "iter: 41.0, RMSE on training set: 1.0597865882574686.\n",
      "iter: 42.0, RMSE on training set: 1.0555747607771049.\n",
      "iter: 43.0, RMSE on training set: 1.0515984374140814.\n",
      "iter: 44.0, RMSE on training set: 1.0478475216391592.\n",
      "iter: 45.0, RMSE on training set: 1.0443121697210325.\n",
      "iter: 46.0, RMSE on training set: 1.0409827805894247.\n",
      "iter: 47.0, RMSE on training set: 1.0378499905063507.\n",
      "iter: 48.0, RMSE on training set: 1.0349046713434908.\n",
      "iter: 49.0, RMSE on training set: 1.032137931460916.\n",
      "iter: 50.0, RMSE on training set: 1.0295411183547958.\n",
      "iter: 51.0, RMSE on training set: 1.027105822391873.\n",
      "iter: 52.0, RMSE on training set: 1.0248238810790469.\n",
      "iter: 53.0, RMSE on training set: 1.022687383429135.\n",
      "iter: 54.0, RMSE on training set: 1.02068867408075.\n",
      "iter: 55.0, RMSE on training set: 1.0188203569127197.\n",
      "iter: 56.0, RMSE on training set: 1.0170752979632154.\n",
      "iter: 57.0, RMSE on training set: 1.0154466275221492.\n",
      "iter: 58.0, RMSE on training set: 1.0139277413138807.\n",
      "iter: 59.0, RMSE on training set: 1.0125123007271515.\n",
      "iter: 60.0, RMSE on training set: 1.0111942320816607.\n",
      "iter: 61.0, RMSE on training set: 1.0099677249468564.\n",
      "iter: 62.0, RMSE on training set: 1.0088272295491338.\n",
      "iter: 63.0, RMSE on training set: 1.0077674533195713.\n",
      "iter: 64.0, RMSE on training set: 1.0067833566460977.\n",
      "iter: 65.0, RMSE on training set: 1.0058701479022025.\n",
      "iter: 66.0, RMSE on training set: 1.0050232778294228.\n",
      "iter: 67.0, RMSE on training set: 1.004238433353447.\n",
      "iter: 68.0, RMSE on training set: 1.0035115309141047.\n",
      "iter: 69.0, RMSE on training set: 1.002838709388244.\n",
      "iter: 70.0, RMSE on training set: 1.0022163226819119.\n",
      "iter: 71.0, RMSE on training set: 1.001640932064592.\n",
      "iter: 72.0, RMSE on training set: 1.001109298313883.\n",
      "iter: 73.0, RMSE on training set: 1.0006183737341128.\n",
      "iter: 74.0, RMSE on training set: 1.0001652941071348.\n",
      "iter: 75.0, RMSE on training set: 0.9997473706282263.\n",
      "iter: 76.0, RMSE on training set: 0.9993620818745823.\n",
      "iter: 77.0, RMSE on training set: 0.9990070658486225.\n",
      "iter: 78.0, RMSE on training set: 0.9986801121332104.\n",
      "iter: 79.0, RMSE on training set: 0.9983791541909603.\n",
      "iter: 80.0, RMSE on training set: 0.9981022618352374.\n",
      "iter: 81.0, RMSE on training set: 0.9978476338960816.\n",
      "iter: 82.0, RMSE on training set: 0.9976135911003439.\n",
      "iter: 83.0, RMSE on training set: 0.9973985691815807.\n",
      "iter: 84.0, RMSE on training set: 0.9972011122319058.\n",
      "iter: 85.0, RMSE on training set: 0.997019866304929.\n",
      "iter: 86.0, RMSE on training set: 0.9968535732760966.\n",
      "iter: 87.0, RMSE on training set: 0.9967010649643184.\n",
      "iter: 88.0, RMSE on training set: 0.9965612575164665.\n",
      "iter: 89.0, RMSE on training set: 0.9964331460544369.\n",
      "iter: 90.0, RMSE on training set: 0.9963157995827171.\n",
      "iter: 91.0, RMSE on training set: 0.996208356152913.\n",
      "iter: 92.0, RMSE on training set: 0.9961100182804874.\n",
      "RMSE on test data: 0.9961119285982559.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483712574660754.\n",
      "iter: 1.0, RMSE on training set: 1.8627131731984305.\n",
      "iter: 2.0, RMSE on training set: 1.7871972078721687.\n",
      "iter: 3.0, RMSE on training set: 1.7205219935899132.\n",
      "iter: 4.0, RMSE on training set: 1.66154842748036.\n",
      "iter: 5.0, RMSE on training set: 1.6092662021609976.\n",
      "iter: 6.0, RMSE on training set: 1.5627846228797284.\n",
      "iter: 7.0, RMSE on training set: 1.521323135697907.\n",
      "iter: 8.0, RMSE on training set: 1.4842015679286717.\n",
      "iter: 9.0, RMSE on training set: 1.450830307947034.\n",
      "iter: 10.0, RMSE on training set: 1.4207006572603167.\n",
      "iter: 11.0, RMSE on training set: 1.3933755616923975.\n",
      "iter: 12.0, RMSE on training set: 1.368480885296914.\n",
      "iter: 13.0, RMSE on training set: 1.3456973411577544.\n",
      "iter: 14.0, RMSE on training set: 1.324753145384815.\n",
      "iter: 15.0, RMSE on training set: 1.3054174191178949.\n",
      "iter: 16.0, RMSE on training set: 1.2874943304017297.\n",
      "iter: 17.0, RMSE on training set: 1.2708179437442624.\n",
      "iter: 18.0, RMSE on training set: 1.2552477292383772.\n",
      "iter: 19.0, RMSE on training set: 1.2406646739686216.\n",
      "iter: 20.0, RMSE on training set: 1.2269679345244813.\n",
      "iter: 21.0, RMSE on training set: 1.2140719693625548.\n",
      "iter: 22.0, RMSE on training set: 1.201904092265859.\n",
      "iter: 23.0, RMSE on training set: 1.1904023922520486.\n",
      "iter: 24.0, RMSE on training set: 1.1795139702397253.\n",
      "iter: 25.0, RMSE on training set: 1.1691934480631552.\n",
      "iter: 26.0, RMSE on training set: 1.159401710672666.\n",
      "iter: 27.0, RMSE on training set: 1.1501048473441735.\n",
      "iter: 28.0, RMSE on training set: 1.1412732623152901.\n",
      "iter: 29.0, RMSE on training set: 1.132880929402301.\n",
      "iter: 30.0, RMSE on training set: 1.1249047688121803.\n",
      "iter: 31.0, RMSE on training set: 1.1173241275565693.\n",
      "iter: 32.0, RMSE on training set: 1.110120347629312.\n",
      "iter: 33.0, RMSE on training set: 1.1032764084658706.\n",
      "iter: 34.0, RMSE on training set: 1.096776632207971.\n",
      "iter: 35.0, RMSE on training set: 1.0906064419977752.\n",
      "iter: 36.0, RMSE on training set: 1.0847521649684522.\n",
      "iter: 37.0, RMSE on training set: 1.079200872823668.\n",
      "iter: 38.0, RMSE on training set: 1.0739402539425094.\n",
      "iter: 39.0, RMSE on training set: 1.0689585118375773.\n",
      "iter: 40.0, RMSE on training set: 1.0642442855553018.\n",
      "iter: 41.0, RMSE on training set: 1.0597865882574686.\n",
      "iter: 42.0, RMSE on training set: 1.0555747607771049.\n",
      "iter: 43.0, RMSE on training set: 1.0515984374140814.\n",
      "iter: 44.0, RMSE on training set: 1.0478475216391592.\n",
      "iter: 45.0, RMSE on training set: 1.0443121697210325.\n",
      "iter: 46.0, RMSE on training set: 1.0409827805894247.\n",
      "iter: 47.0, RMSE on training set: 1.0378499905063507.\n",
      "iter: 48.0, RMSE on training set: 1.0349046713434908.\n",
      "iter: 49.0, RMSE on training set: 1.032137931460916.\n",
      "iter: 50.0, RMSE on training set: 1.0295411183547958.\n",
      "iter: 51.0, RMSE on training set: 1.027105822391873.\n",
      "iter: 52.0, RMSE on training set: 1.0248238810790469.\n",
      "iter: 53.0, RMSE on training set: 1.022687383429135.\n",
      "iter: 54.0, RMSE on training set: 1.02068867408075.\n",
      "iter: 55.0, RMSE on training set: 1.0188203569127197.\n",
      "iter: 56.0, RMSE on training set: 1.0170752979632154.\n",
      "iter: 57.0, RMSE on training set: 1.0154466275221492.\n",
      "iter: 58.0, RMSE on training set: 1.0139277413138807.\n",
      "iter: 59.0, RMSE on training set: 1.0125123007271515.\n",
      "iter: 60.0, RMSE on training set: 1.0111942320816607.\n",
      "iter: 61.0, RMSE on training set: 1.0099677249468564.\n",
      "iter: 62.0, RMSE on training set: 1.0088272295491338.\n",
      "iter: 63.0, RMSE on training set: 1.0077674533195713.\n",
      "iter: 64.0, RMSE on training set: 1.0067833566460977.\n",
      "iter: 65.0, RMSE on training set: 1.0058701479022025.\n",
      "iter: 66.0, RMSE on training set: 1.0050232778294228.\n",
      "iter: 67.0, RMSE on training set: 1.004238433353447.\n",
      "iter: 68.0, RMSE on training set: 1.0035115309141047.\n",
      "iter: 69.0, RMSE on training set: 1.002838709388244.\n",
      "iter: 70.0, RMSE on training set: 1.0022163226819119.\n",
      "iter: 71.0, RMSE on training set: 1.001640932064592.\n",
      "iter: 72.0, RMSE on training set: 1.001109298313883.\n",
      "iter: 73.0, RMSE on training set: 1.0006183737341128.\n",
      "iter: 74.0, RMSE on training set: 1.0001652941071348.\n",
      "iter: 75.0, RMSE on training set: 0.9997473706282263.\n",
      "iter: 76.0, RMSE on training set: 0.9993620818745823.\n",
      "iter: 77.0, RMSE on training set: 0.9990070658486225.\n",
      "iter: 78.0, RMSE on training set: 0.9986801121332104.\n",
      "iter: 79.0, RMSE on training set: 0.9983791541909603.\n",
      "iter: 80.0, RMSE on training set: 0.9981022618352374.\n",
      "iter: 81.0, RMSE on training set: 0.9978476338960816.\n",
      "iter: 82.0, RMSE on training set: 0.9976135911003439.\n",
      "iter: 83.0, RMSE on training set: 0.9973985691815807.\n",
      "iter: 84.0, RMSE on training set: 0.9972011122319058.\n",
      "iter: 85.0, RMSE on training set: 0.997019866304929.\n",
      "iter: 86.0, RMSE on training set: 0.9968535732760966.\n",
      "iter: 87.0, RMSE on training set: 0.9967010649643184.\n",
      "iter: 88.0, RMSE on training set: 0.9965612575164665.\n",
      "iter: 89.0, RMSE on training set: 0.9964331460544369.\n",
      "iter: 90.0, RMSE on training set: 0.9963157995827171.\n",
      "iter: 91.0, RMSE on training set: 0.996208356152913.\n",
      "iter: 92.0, RMSE on training set: 0.9961100182804874.\n",
      "RMSE on test data: 0.9961007288622592.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483712574660754.\n",
      "iter: 1.0, RMSE on training set: 1.8627131731984305.\n",
      "iter: 2.0, RMSE on training set: 1.7871972078721687.\n",
      "iter: 3.0, RMSE on training set: 1.7205219935899132.\n",
      "iter: 4.0, RMSE on training set: 1.66154842748036.\n",
      "iter: 5.0, RMSE on training set: 1.6092662021609976.\n",
      "iter: 6.0, RMSE on training set: 1.5627846228797284.\n",
      "iter: 7.0, RMSE on training set: 1.521323135697907.\n",
      "iter: 8.0, RMSE on training set: 1.4842015679286717.\n",
      "iter: 9.0, RMSE on training set: 1.450830307947034.\n",
      "iter: 10.0, RMSE on training set: 1.4207006572603167.\n",
      "iter: 11.0, RMSE on training set: 1.3933755616923975.\n",
      "iter: 12.0, RMSE on training set: 1.368480885296914.\n",
      "iter: 13.0, RMSE on training set: 1.3456973411577544.\n",
      "iter: 14.0, RMSE on training set: 1.324753145384815.\n",
      "iter: 15.0, RMSE on training set: 1.3054174191178949.\n",
      "iter: 16.0, RMSE on training set: 1.2874943304017297.\n",
      "iter: 17.0, RMSE on training set: 1.2708179437442624.\n",
      "iter: 18.0, RMSE on training set: 1.2552477292383772.\n",
      "iter: 19.0, RMSE on training set: 1.2406646739686216.\n",
      "iter: 20.0, RMSE on training set: 1.2269679345244813.\n",
      "iter: 21.0, RMSE on training set: 1.2140719693625548.\n",
      "iter: 22.0, RMSE on training set: 1.201904092265859.\n",
      "iter: 23.0, RMSE on training set: 1.1904023922520486.\n",
      "iter: 24.0, RMSE on training set: 1.1795139702397253.\n",
      "iter: 25.0, RMSE on training set: 1.1691934480631552.\n",
      "iter: 26.0, RMSE on training set: 1.159401710672666.\n",
      "iter: 27.0, RMSE on training set: 1.1501048473441735.\n",
      "iter: 28.0, RMSE on training set: 1.1412732623152901.\n",
      "iter: 29.0, RMSE on training set: 1.132880929402301.\n",
      "iter: 30.0, RMSE on training set: 1.1249047688121803.\n",
      "iter: 31.0, RMSE on training set: 1.1173241275565693.\n",
      "iter: 32.0, RMSE on training set: 1.110120347629312.\n",
      "iter: 33.0, RMSE on training set: 1.1032764084658706.\n",
      "iter: 34.0, RMSE on training set: 1.096776632207971.\n",
      "iter: 35.0, RMSE on training set: 1.0906064419977752.\n",
      "iter: 36.0, RMSE on training set: 1.0847521649684522.\n",
      "iter: 37.0, RMSE on training set: 1.079200872823668.\n",
      "iter: 38.0, RMSE on training set: 1.0739402539425094.\n",
      "iter: 39.0, RMSE on training set: 1.0689585118375773.\n",
      "iter: 40.0, RMSE on training set: 1.0642442855553018.\n",
      "iter: 41.0, RMSE on training set: 1.0597865882574686.\n",
      "iter: 42.0, RMSE on training set: 1.0555747607771049.\n",
      "iter: 43.0, RMSE on training set: 1.0515984374140814.\n",
      "iter: 44.0, RMSE on training set: 1.0478475216391592.\n",
      "iter: 45.0, RMSE on training set: 1.0443121697210325.\n",
      "iter: 46.0, RMSE on training set: 1.0409827805894247.\n",
      "iter: 47.0, RMSE on training set: 1.0378499905063507.\n",
      "iter: 48.0, RMSE on training set: 1.0349046713434908.\n",
      "iter: 49.0, RMSE on training set: 1.032137931460916.\n",
      "iter: 50.0, RMSE on training set: 1.0295411183547958.\n",
      "iter: 51.0, RMSE on training set: 1.027105822391873.\n",
      "iter: 52.0, RMSE on training set: 1.0248238810790469.\n",
      "iter: 53.0, RMSE on training set: 1.022687383429135.\n",
      "iter: 54.0, RMSE on training set: 1.02068867408075.\n",
      "iter: 55.0, RMSE on training set: 1.0188203569127197.\n",
      "iter: 56.0, RMSE on training set: 1.0170752979632154.\n",
      "iter: 57.0, RMSE on training set: 1.0154466275221492.\n",
      "iter: 58.0, RMSE on training set: 1.0139277413138807.\n",
      "iter: 59.0, RMSE on training set: 1.0125123007271515.\n",
      "iter: 60.0, RMSE on training set: 1.0111942320816607.\n",
      "iter: 61.0, RMSE on training set: 1.0099677249468564.\n",
      "iter: 62.0, RMSE on training set: 1.0088272295491338.\n",
      "iter: 63.0, RMSE on training set: 1.0077674533195713.\n",
      "iter: 64.0, RMSE on training set: 1.0067833566460977.\n",
      "iter: 65.0, RMSE on training set: 1.0058701479022025.\n",
      "iter: 66.0, RMSE on training set: 1.0050232778294228.\n",
      "iter: 67.0, RMSE on training set: 1.004238433353447.\n",
      "iter: 68.0, RMSE on training set: 1.0035115309141047.\n",
      "iter: 69.0, RMSE on training set: 1.002838709388244.\n",
      "iter: 70.0, RMSE on training set: 1.0022163226819119.\n",
      "iter: 71.0, RMSE on training set: 1.001640932064592.\n",
      "iter: 72.0, RMSE on training set: 1.001109298313883.\n",
      "iter: 73.0, RMSE on training set: 1.0006183737341128.\n",
      "iter: 74.0, RMSE on training set: 1.0001652941071348.\n",
      "iter: 75.0, RMSE on training set: 0.9997473706282263.\n",
      "iter: 76.0, RMSE on training set: 0.9993620818745823.\n",
      "iter: 77.0, RMSE on training set: 0.9990070658486225.\n",
      "iter: 78.0, RMSE on training set: 0.9986801121332104.\n",
      "iter: 79.0, RMSE on training set: 0.9983791541909603.\n",
      "iter: 80.0, RMSE on training set: 0.9981022618352374.\n",
      "iter: 81.0, RMSE on training set: 0.9978476338960816.\n",
      "iter: 82.0, RMSE on training set: 0.9976135911003439.\n",
      "iter: 83.0, RMSE on training set: 0.9973985691815807.\n",
      "iter: 84.0, RMSE on training set: 0.9972011122319058.\n",
      "iter: 85.0, RMSE on training set: 0.997019866304929.\n",
      "iter: 86.0, RMSE on training set: 0.9968535732760966.\n",
      "iter: 87.0, RMSE on training set: 0.9967010649643184.\n",
      "iter: 88.0, RMSE on training set: 0.9965612575164665.\n",
      "iter: 89.0, RMSE on training set: 0.9964331460544369.\n",
      "iter: 90.0, RMSE on training set: 0.9963157995827171.\n",
      "iter: 91.0, RMSE on training set: 0.996208356152913.\n",
      "iter: 92.0, RMSE on training set: 0.9961100182804874.\n",
      "RMSE on test data: 0.9961050992535811.\n",
      "Running lambda_user=1\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483046767177221.\n",
      "iter: 1.0, RMSE on training set: 1.8625715384126393.\n",
      "iter: 2.0, RMSE on training set: 1.7869936182974169.\n",
      "iter: 3.0, RMSE on training set: 1.7202671859369674.\n",
      "iter: 4.0, RMSE on training set: 1.6612511598506239.\n",
      "iter: 5.0, RMSE on training set: 1.608933582577258.\n",
      "iter: 6.0, RMSE on training set: 1.5624224000154436.\n",
      "iter: 7.0, RMSE on training set: 1.52093595261701.\n",
      "iter: 8.0, RMSE on training set: 1.4837931803335602.\n",
      "iter: 9.0, RMSE on training set: 1.450403769071238.\n",
      "iter: 10.0, RMSE on training set: 1.4202584721154106.\n",
      "iter: 11.0, RMSE on training set: 1.3929198137820646.\n",
      "iter: 12.0, RMSE on training set: 1.3680133391125726.\n",
      "iter: 13.0, RMSE on training set: 1.3452195237822298.\n",
      "iter: 14.0, RMSE on training set: 1.3242664103901878.\n",
      "iter: 15.0, RMSE on training set: 1.3049229956967918.\n",
      "iter: 16.0, RMSE on training set: 1.2869933603622798.\n",
      "iter: 17.0, RMSE on training set: 1.270311508664587.\n",
      "iter: 18.0, RMSE on training set: 1.2547368697442485.\n",
      "iter: 19.0, RMSE on training set: 1.240150402785962.\n",
      "iter: 20.0, RMSE on training set: 1.226451244677157.\n",
      "iter: 21.0, RMSE on training set: 1.2135538386399758.\n",
      "iter: 22.0, RMSE on training set: 1.201385484874704.\n",
      "iter: 23.0, RMSE on training set: 1.1898842583899665.\n",
      "iter: 24.0, RMSE on training set: 1.1789972441826866.\n",
      "iter: 25.0, RMSE on training set: 1.1686790452383233.\n",
      "iter: 26.0, RMSE on training set: 1.1588905240911322.\n",
      "iter: 27.0, RMSE on training set: 1.1495977436890625.\n",
      "iter: 28.0, RMSE on training set: 1.1407710779170348.\n",
      "iter: 29.0, RMSE on training set: 1.1323844662815459.\n",
      "iter: 30.0, RMSE on training set: 1.124414790929159.\n",
      "iter: 31.0, RMSE on training set: 1.1168413573719127.\n",
      "iter: 32.0, RMSE on training set: 1.1096454630532588.\n",
      "iter: 33.0, RMSE on training set: 1.1028100402495413.\n",
      "iter: 34.0, RMSE on training set: 1.0963193618106977.\n",
      "iter: 35.0, RMSE on training set: 1.0901587999477995.\n",
      "iter: 36.0, RMSE on training set: 1.084314629720338.\n",
      "iter: 37.0, RMSE on training set: 1.0787738701041727.\n",
      "iter: 38.0, RMSE on training set: 1.0735241565673879.\n",
      "iter: 39.0, RMSE on training set: 1.0685536399745832.\n",
      "iter: 40.0, RMSE on training set: 1.063850907403266.\n",
      "iter: 41.0, RMSE on training set: 1.0594049211074015.\n",
      "iter: 42.0, RMSE on training set: 1.0552049724184565.\n",
      "iter: 43.0, RMSE on training set: 1.051240647847437.\n",
      "iter: 44.0, RMSE on training set: 1.0475018050554525.\n",
      "iter: 45.0, RMSE on training set: 1.043978556706951.\n",
      "iter: 46.0, RMSE on training set: 1.0406612605189192.\n",
      "iter: 47.0, RMSE on training set: 1.0375405140790812.\n",
      "iter: 48.0, RMSE on training set: 1.0346071532325711.\n",
      "iter: 49.0, RMSE on training set: 1.031852253034408.\n",
      "iter: 50.0, RMSE on training set: 1.0292671304378158.\n",
      "iter: 51.0, RMSE on training set: 1.026843348038971.\n",
      "iter: 52.0, RMSE on training set: 1.024572718329438.\n",
      "iter: 53.0, RMSE on training set: 1.0224473080203829.\n",
      "iter: 54.0, RMSE on training set: 1.0204594420995243.\n",
      "iter: 55.0, RMSE on training set: 1.018601707364197.\n",
      "iter: 56.0, RMSE on training set: 1.0168669552435683.\n",
      "iter: 57.0, RMSE on training set: 1.0152483037812419.\n",
      "iter: 58.0, RMSE on training set: 1.0137391386978698.\n",
      "iter: 59.0, RMSE on training set: 1.012333113493087.\n",
      "iter: 60.0, RMSE on training set: 1.0110241485784244.\n",
      "iter: 61.0, RMSE on training set: 1.009806429458823.\n",
      "iter: 62.0, RMSE on training set: 1.0086744040008462.\n",
      "iter: 63.0, RMSE on training set: 1.0076227788414163.\n",
      "iter: 64.0, RMSE on training set: 1.006646515002436.\n",
      "iter: 65.0, RMSE on training set: 1.0057408227847073.\n",
      "iter: 66.0, RMSE on training set: 1.0049011560194496.\n",
      "iter: 67.0, RMSE on training set: 1.004123205758147.\n",
      "iter: 68.0, RMSE on training set: 1.0034028934816799.\n",
      "iter: 69.0, RMSE on training set: 1.0027363639083013.\n",
      "iter: 70.0, RMSE on training set: 1.0021199774772367.\n",
      "iter: 71.0, RMSE on training set: 1.001550302580945.\n",
      "iter: 72.0, RMSE on training set: 1.0010241076145558.\n",
      "iter: 73.0, RMSE on training set: 1.0005383529060263.\n",
      "iter: 74.0, RMSE on training set: 1.0000901825852306.\n",
      "iter: 75.0, RMSE on training set: 0.9996769164447931.\n",
      "iter: 76.0, RMSE on training set: 0.9992960418399818.\n",
      "iter: 77.0, RMSE on training set: 0.9989452056696847.\n",
      "iter: 78.0, RMSE on training set: 0.9986222064752963.\n",
      "iter: 79.0, RMSE on training set: 0.9983249866894345.\n",
      "iter: 80.0, RMSE on training set: 0.9980516250617673.\n",
      "iter: 81.0, RMSE on training set: 0.9978003292849077.\n",
      "iter: 82.0, RMSE on training set: 0.9975694288393028.\n",
      "iter: 83.0, RMSE on training set: 0.9973573680723897.\n",
      "iter: 84.0, RMSE on training set: 0.9971626995238677.\n",
      "iter: 85.0, RMSE on training set: 0.9969840775059188.\n",
      "iter: 86.0, RMSE on training set: 0.9968202519444024.\n",
      "iter: 87.0, RMSE on training set: 0.9966700624845902.\n",
      "iter: 88.0, RMSE on training set: 0.9965324328627883.\n",
      "iter: 89.0, RMSE on training set: 0.9964063655432445.\n",
      "iter: 90.0, RMSE on training set: 0.9962909366180539.\n",
      "iter: 91.0, RMSE on training set: 0.996185290966291.\n",
      "iter: 92.0, RMSE on training set: 0.9960886376673755.\n",
      "RMSE on test data: 0.996098234074182.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483046767177221.\n",
      "iter: 1.0, RMSE on training set: 1.8625715384126393.\n",
      "iter: 2.0, RMSE on training set: 1.7869936182974169.\n",
      "iter: 3.0, RMSE on training set: 1.7202671859369674.\n",
      "iter: 4.0, RMSE on training set: 1.6612511598506239.\n",
      "iter: 5.0, RMSE on training set: 1.608933582577258.\n",
      "iter: 6.0, RMSE on training set: 1.5624224000154436.\n",
      "iter: 7.0, RMSE on training set: 1.52093595261701.\n",
      "iter: 8.0, RMSE on training set: 1.4837931803335602.\n",
      "iter: 9.0, RMSE on training set: 1.450403769071238.\n",
      "iter: 10.0, RMSE on training set: 1.4202584721154106.\n",
      "iter: 11.0, RMSE on training set: 1.3929198137820646.\n",
      "iter: 12.0, RMSE on training set: 1.3680133391125726.\n",
      "iter: 13.0, RMSE on training set: 1.3452195237822298.\n",
      "iter: 14.0, RMSE on training set: 1.3242664103901878.\n",
      "iter: 15.0, RMSE on training set: 1.3049229956967918.\n",
      "iter: 16.0, RMSE on training set: 1.2869933603622798.\n",
      "iter: 17.0, RMSE on training set: 1.270311508664587.\n",
      "iter: 18.0, RMSE on training set: 1.2547368697442485.\n",
      "iter: 19.0, RMSE on training set: 1.240150402785962.\n",
      "iter: 20.0, RMSE on training set: 1.226451244677157.\n",
      "iter: 21.0, RMSE on training set: 1.2135538386399758.\n",
      "iter: 22.0, RMSE on training set: 1.201385484874704.\n",
      "iter: 23.0, RMSE on training set: 1.1898842583899665.\n",
      "iter: 24.0, RMSE on training set: 1.1789972441826866.\n",
      "iter: 25.0, RMSE on training set: 1.1686790452383233.\n",
      "iter: 26.0, RMSE on training set: 1.1588905240911322.\n",
      "iter: 27.0, RMSE on training set: 1.1495977436890625.\n",
      "iter: 28.0, RMSE on training set: 1.1407710779170348.\n",
      "iter: 29.0, RMSE on training set: 1.1323844662815459.\n",
      "iter: 30.0, RMSE on training set: 1.124414790929159.\n",
      "iter: 31.0, RMSE on training set: 1.1168413573719127.\n",
      "iter: 32.0, RMSE on training set: 1.1096454630532588.\n",
      "iter: 33.0, RMSE on training set: 1.1028100402495413.\n",
      "iter: 34.0, RMSE on training set: 1.0963193618106977.\n",
      "iter: 35.0, RMSE on training set: 1.0901587999477995.\n",
      "iter: 36.0, RMSE on training set: 1.084314629720338.\n",
      "iter: 37.0, RMSE on training set: 1.0787738701041727.\n",
      "iter: 38.0, RMSE on training set: 1.0735241565673879.\n",
      "iter: 39.0, RMSE on training set: 1.0685536399745832.\n",
      "iter: 40.0, RMSE on training set: 1.063850907403266.\n",
      "iter: 41.0, RMSE on training set: 1.0594049211074015.\n",
      "iter: 42.0, RMSE on training set: 1.0552049724184565.\n",
      "iter: 43.0, RMSE on training set: 1.051240647847437.\n",
      "iter: 44.0, RMSE on training set: 1.0475018050554525.\n",
      "iter: 45.0, RMSE on training set: 1.043978556706951.\n",
      "iter: 46.0, RMSE on training set: 1.0406612605189192.\n",
      "iter: 47.0, RMSE on training set: 1.0375405140790812.\n",
      "iter: 48.0, RMSE on training set: 1.0346071532325711.\n",
      "iter: 49.0, RMSE on training set: 1.031852253034408.\n",
      "iter: 50.0, RMSE on training set: 1.0292671304378158.\n",
      "iter: 51.0, RMSE on training set: 1.026843348038971.\n",
      "iter: 52.0, RMSE on training set: 1.024572718329438.\n",
      "iter: 53.0, RMSE on training set: 1.0224473080203829.\n",
      "iter: 54.0, RMSE on training set: 1.0204594420995243.\n",
      "iter: 55.0, RMSE on training set: 1.018601707364197.\n",
      "iter: 56.0, RMSE on training set: 1.0168669552435683.\n",
      "iter: 57.0, RMSE on training set: 1.0152483037812419.\n",
      "iter: 58.0, RMSE on training set: 1.0137391386978698.\n",
      "iter: 59.0, RMSE on training set: 1.012333113493087.\n",
      "iter: 60.0, RMSE on training set: 1.0110241485784244.\n",
      "iter: 61.0, RMSE on training set: 1.009806429458823.\n",
      "iter: 62.0, RMSE on training set: 1.0086744040008462.\n",
      "iter: 63.0, RMSE on training set: 1.0076227788414163.\n",
      "iter: 64.0, RMSE on training set: 1.006646515002436.\n",
      "iter: 65.0, RMSE on training set: 1.0057408227847073.\n",
      "iter: 66.0, RMSE on training set: 1.0049011560194496.\n",
      "iter: 67.0, RMSE on training set: 1.004123205758147.\n",
      "iter: 68.0, RMSE on training set: 1.0034028934816799.\n",
      "iter: 69.0, RMSE on training set: 1.0027363639083013.\n",
      "iter: 70.0, RMSE on training set: 1.0021199774772367.\n",
      "iter: 71.0, RMSE on training set: 1.001550302580945.\n",
      "iter: 72.0, RMSE on training set: 1.0010241076145558.\n",
      "iter: 73.0, RMSE on training set: 1.0005383529060263.\n",
      "iter: 74.0, RMSE on training set: 1.0000901825852306.\n",
      "iter: 75.0, RMSE on training set: 0.9996769164447931.\n",
      "iter: 76.0, RMSE on training set: 0.9992960418399818.\n",
      "iter: 77.0, RMSE on training set: 0.9989452056696847.\n",
      "iter: 78.0, RMSE on training set: 0.9986222064752963.\n",
      "iter: 79.0, RMSE on training set: 0.9983249866894345.\n",
      "iter: 80.0, RMSE on training set: 0.9980516250617673.\n",
      "iter: 81.0, RMSE on training set: 0.9978003292849077.\n",
      "iter: 82.0, RMSE on training set: 0.9975694288393028.\n",
      "iter: 83.0, RMSE on training set: 0.9973573680723897.\n",
      "iter: 84.0, RMSE on training set: 0.9971626995238677.\n",
      "iter: 85.0, RMSE on training set: 0.9969840775059188.\n",
      "iter: 86.0, RMSE on training set: 0.9968202519444024.\n",
      "iter: 87.0, RMSE on training set: 0.9966700624845902.\n",
      "iter: 88.0, RMSE on training set: 0.9965324328627883.\n",
      "iter: 89.0, RMSE on training set: 0.9964063655432445.\n",
      "iter: 90.0, RMSE on training set: 0.9962909366180539.\n",
      "iter: 91.0, RMSE on training set: 0.996185290966291.\n",
      "iter: 92.0, RMSE on training set: 0.9960886376673755.\n",
      "RMSE on test data: 0.9960971765166938.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483046767177221.\n",
      "iter: 1.0, RMSE on training set: 1.8625715384126393.\n",
      "iter: 2.0, RMSE on training set: 1.7869936182974169.\n",
      "iter: 3.0, RMSE on training set: 1.7202671859369674.\n",
      "iter: 4.0, RMSE on training set: 1.6612511598506239.\n",
      "iter: 5.0, RMSE on training set: 1.608933582577258.\n",
      "iter: 6.0, RMSE on training set: 1.5624224000154436.\n",
      "iter: 7.0, RMSE on training set: 1.52093595261701.\n",
      "iter: 8.0, RMSE on training set: 1.4837931803335602.\n",
      "iter: 9.0, RMSE on training set: 1.450403769071238.\n",
      "iter: 10.0, RMSE on training set: 1.4202584721154106.\n",
      "iter: 11.0, RMSE on training set: 1.3929198137820646.\n",
      "iter: 12.0, RMSE on training set: 1.3680133391125726.\n",
      "iter: 13.0, RMSE on training set: 1.3452195237822298.\n",
      "iter: 14.0, RMSE on training set: 1.3242664103901878.\n",
      "iter: 15.0, RMSE on training set: 1.3049229956967918.\n",
      "iter: 16.0, RMSE on training set: 1.2869933603622798.\n",
      "iter: 17.0, RMSE on training set: 1.270311508664587.\n",
      "iter: 18.0, RMSE on training set: 1.2547368697442485.\n",
      "iter: 19.0, RMSE on training set: 1.240150402785962.\n",
      "iter: 20.0, RMSE on training set: 1.226451244677157.\n",
      "iter: 21.0, RMSE on training set: 1.2135538386399758.\n",
      "iter: 22.0, RMSE on training set: 1.201385484874704.\n",
      "iter: 23.0, RMSE on training set: 1.1898842583899665.\n",
      "iter: 24.0, RMSE on training set: 1.1789972441826866.\n",
      "iter: 25.0, RMSE on training set: 1.1686790452383233.\n",
      "iter: 26.0, RMSE on training set: 1.1588905240911322.\n",
      "iter: 27.0, RMSE on training set: 1.1495977436890625.\n",
      "iter: 28.0, RMSE on training set: 1.1407710779170348.\n",
      "iter: 29.0, RMSE on training set: 1.1323844662815459.\n",
      "iter: 30.0, RMSE on training set: 1.124414790929159.\n",
      "iter: 31.0, RMSE on training set: 1.1168413573719127.\n",
      "iter: 32.0, RMSE on training set: 1.1096454630532588.\n",
      "iter: 33.0, RMSE on training set: 1.1028100402495413.\n",
      "iter: 34.0, RMSE on training set: 1.0963193618106977.\n",
      "iter: 35.0, RMSE on training set: 1.0901587999477995.\n",
      "iter: 36.0, RMSE on training set: 1.084314629720338.\n",
      "iter: 37.0, RMSE on training set: 1.0787738701041727.\n",
      "iter: 38.0, RMSE on training set: 1.0735241565673879.\n",
      "iter: 39.0, RMSE on training set: 1.0685536399745832.\n",
      "iter: 40.0, RMSE on training set: 1.063850907403266.\n",
      "iter: 41.0, RMSE on training set: 1.0594049211074015.\n",
      "iter: 42.0, RMSE on training set: 1.0552049724184565.\n",
      "iter: 43.0, RMSE on training set: 1.051240647847437.\n",
      "iter: 44.0, RMSE on training set: 1.0475018050554525.\n",
      "iter: 45.0, RMSE on training set: 1.043978556706951.\n",
      "iter: 46.0, RMSE on training set: 1.0406612605189192.\n",
      "iter: 47.0, RMSE on training set: 1.0375405140790812.\n",
      "iter: 48.0, RMSE on training set: 1.0346071532325711.\n",
      "iter: 49.0, RMSE on training set: 1.031852253034408.\n",
      "iter: 50.0, RMSE on training set: 1.0292671304378158.\n",
      "iter: 51.0, RMSE on training set: 1.026843348038971.\n",
      "iter: 52.0, RMSE on training set: 1.024572718329438.\n",
      "iter: 53.0, RMSE on training set: 1.0224473080203829.\n",
      "iter: 54.0, RMSE on training set: 1.0204594420995243.\n",
      "iter: 55.0, RMSE on training set: 1.018601707364197.\n",
      "iter: 56.0, RMSE on training set: 1.0168669552435683.\n",
      "iter: 57.0, RMSE on training set: 1.0152483037812419.\n",
      "iter: 58.0, RMSE on training set: 1.0137391386978698.\n",
      "iter: 59.0, RMSE on training set: 1.012333113493087.\n",
      "iter: 60.0, RMSE on training set: 1.0110241485784244.\n",
      "iter: 61.0, RMSE on training set: 1.009806429458823.\n",
      "iter: 62.0, RMSE on training set: 1.0086744040008462.\n",
      "iter: 63.0, RMSE on training set: 1.0076227788414163.\n",
      "iter: 64.0, RMSE on training set: 1.006646515002436.\n",
      "iter: 65.0, RMSE on training set: 1.0057408227847073.\n",
      "iter: 66.0, RMSE on training set: 1.0049011560194496.\n",
      "iter: 67.0, RMSE on training set: 1.004123205758147.\n",
      "iter: 68.0, RMSE on training set: 1.0034028934816799.\n",
      "iter: 69.0, RMSE on training set: 1.0027363639083013.\n",
      "iter: 70.0, RMSE on training set: 1.0021199774772367.\n",
      "iter: 71.0, RMSE on training set: 1.001550302580945.\n",
      "iter: 72.0, RMSE on training set: 1.0010241076145558.\n",
      "iter: 73.0, RMSE on training set: 1.0005383529060263.\n",
      "iter: 74.0, RMSE on training set: 1.0000901825852306.\n",
      "iter: 75.0, RMSE on training set: 0.9996769164447931.\n",
      "iter: 76.0, RMSE on training set: 0.9992960418399818.\n",
      "iter: 77.0, RMSE on training set: 0.9989452056696847.\n",
      "iter: 78.0, RMSE on training set: 0.9986222064752963.\n",
      "iter: 79.0, RMSE on training set: 0.9983249866894345.\n",
      "iter: 80.0, RMSE on training set: 0.9980516250617673.\n",
      "iter: 81.0, RMSE on training set: 0.9978003292849077.\n",
      "iter: 82.0, RMSE on training set: 0.9975694288393028.\n",
      "iter: 83.0, RMSE on training set: 0.9973573680723897.\n",
      "iter: 84.0, RMSE on training set: 0.9971626995238677.\n",
      "iter: 85.0, RMSE on training set: 0.9969840775059188.\n",
      "iter: 86.0, RMSE on training set: 0.9968202519444024.\n",
      "iter: 87.0, RMSE on training set: 0.9966700624845902.\n",
      "iter: 88.0, RMSE on training set: 0.9965324328627883.\n",
      "iter: 89.0, RMSE on training set: 0.9964063655432445.\n",
      "iter: 90.0, RMSE on training set: 0.9962909366180539.\n",
      "iter: 91.0, RMSE on training set: 0.996185290966291.\n",
      "iter: 92.0, RMSE on training set: 0.9960886376673755.\n",
      "RMSE on test data: 0.9960905461675595.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483046767177221.\n",
      "iter: 1.0, RMSE on training set: 1.8625715384126393.\n",
      "iter: 2.0, RMSE on training set: 1.7869936182974169.\n",
      "iter: 3.0, RMSE on training set: 1.7202671859369674.\n",
      "iter: 4.0, RMSE on training set: 1.6612511598506239.\n",
      "iter: 5.0, RMSE on training set: 1.608933582577258.\n",
      "iter: 6.0, RMSE on training set: 1.5624224000154436.\n",
      "iter: 7.0, RMSE on training set: 1.52093595261701.\n",
      "iter: 8.0, RMSE on training set: 1.4837931803335602.\n",
      "iter: 9.0, RMSE on training set: 1.450403769071238.\n",
      "iter: 10.0, RMSE on training set: 1.4202584721154106.\n",
      "iter: 11.0, RMSE on training set: 1.3929198137820646.\n",
      "iter: 12.0, RMSE on training set: 1.3680133391125726.\n",
      "iter: 13.0, RMSE on training set: 1.3452195237822298.\n",
      "iter: 14.0, RMSE on training set: 1.3242664103901878.\n",
      "iter: 15.0, RMSE on training set: 1.3049229956967918.\n",
      "iter: 16.0, RMSE on training set: 1.2869933603622798.\n",
      "iter: 17.0, RMSE on training set: 1.270311508664587.\n",
      "iter: 18.0, RMSE on training set: 1.2547368697442485.\n",
      "iter: 19.0, RMSE on training set: 1.240150402785962.\n",
      "iter: 20.0, RMSE on training set: 1.226451244677157.\n",
      "iter: 21.0, RMSE on training set: 1.2135538386399758.\n",
      "iter: 22.0, RMSE on training set: 1.201385484874704.\n",
      "iter: 23.0, RMSE on training set: 1.1898842583899665.\n",
      "iter: 24.0, RMSE on training set: 1.1789972441826866.\n",
      "iter: 25.0, RMSE on training set: 1.1686790452383233.\n",
      "iter: 26.0, RMSE on training set: 1.1588905240911322.\n",
      "iter: 27.0, RMSE on training set: 1.1495977436890625.\n",
      "iter: 28.0, RMSE on training set: 1.1407710779170348.\n",
      "iter: 29.0, RMSE on training set: 1.1323844662815459.\n",
      "iter: 30.0, RMSE on training set: 1.124414790929159.\n",
      "iter: 31.0, RMSE on training set: 1.1168413573719127.\n",
      "iter: 32.0, RMSE on training set: 1.1096454630532588.\n",
      "iter: 33.0, RMSE on training set: 1.1028100402495413.\n",
      "iter: 34.0, RMSE on training set: 1.0963193618106977.\n",
      "iter: 35.0, RMSE on training set: 1.0901587999477995.\n",
      "iter: 36.0, RMSE on training set: 1.084314629720338.\n",
      "iter: 37.0, RMSE on training set: 1.0787738701041727.\n",
      "iter: 38.0, RMSE on training set: 1.0735241565673879.\n",
      "iter: 39.0, RMSE on training set: 1.0685536399745832.\n",
      "iter: 40.0, RMSE on training set: 1.063850907403266.\n",
      "iter: 41.0, RMSE on training set: 1.0594049211074015.\n",
      "iter: 42.0, RMSE on training set: 1.0552049724184565.\n",
      "iter: 43.0, RMSE on training set: 1.051240647847437.\n",
      "iter: 44.0, RMSE on training set: 1.0475018050554525.\n",
      "iter: 45.0, RMSE on training set: 1.043978556706951.\n",
      "iter: 46.0, RMSE on training set: 1.0406612605189192.\n",
      "iter: 47.0, RMSE on training set: 1.0375405140790812.\n",
      "iter: 48.0, RMSE on training set: 1.0346071532325711.\n",
      "iter: 49.0, RMSE on training set: 1.031852253034408.\n",
      "iter: 50.0, RMSE on training set: 1.0292671304378158.\n",
      "iter: 51.0, RMSE on training set: 1.026843348038971.\n",
      "iter: 52.0, RMSE on training set: 1.024572718329438.\n",
      "iter: 53.0, RMSE on training set: 1.0224473080203829.\n",
      "iter: 54.0, RMSE on training set: 1.0204594420995243.\n",
      "iter: 55.0, RMSE on training set: 1.018601707364197.\n",
      "iter: 56.0, RMSE on training set: 1.0168669552435683.\n",
      "iter: 57.0, RMSE on training set: 1.0152483037812419.\n",
      "iter: 58.0, RMSE on training set: 1.0137391386978698.\n",
      "iter: 59.0, RMSE on training set: 1.012333113493087.\n",
      "iter: 60.0, RMSE on training set: 1.0110241485784244.\n",
      "iter: 61.0, RMSE on training set: 1.009806429458823.\n",
      "iter: 62.0, RMSE on training set: 1.0086744040008462.\n",
      "iter: 63.0, RMSE on training set: 1.0076227788414163.\n",
      "iter: 64.0, RMSE on training set: 1.006646515002436.\n",
      "iter: 65.0, RMSE on training set: 1.0057408227847073.\n",
      "iter: 66.0, RMSE on training set: 1.0049011560194496.\n",
      "iter: 67.0, RMSE on training set: 1.004123205758147.\n",
      "iter: 68.0, RMSE on training set: 1.0034028934816799.\n",
      "iter: 69.0, RMSE on training set: 1.0027363639083013.\n",
      "iter: 70.0, RMSE on training set: 1.0021199774772367.\n",
      "iter: 71.0, RMSE on training set: 1.001550302580945.\n",
      "iter: 72.0, RMSE on training set: 1.0010241076145558.\n",
      "iter: 73.0, RMSE on training set: 1.0005383529060263.\n",
      "iter: 74.0, RMSE on training set: 1.0000901825852306.\n",
      "iter: 75.0, RMSE on training set: 0.9996769164447931.\n",
      "iter: 76.0, RMSE on training set: 0.9992960418399818.\n",
      "iter: 77.0, RMSE on training set: 0.9989452056696847.\n",
      "iter: 78.0, RMSE on training set: 0.9986222064752963.\n",
      "iter: 79.0, RMSE on training set: 0.9983249866894345.\n",
      "iter: 80.0, RMSE on training set: 0.9980516250617673.\n",
      "iter: 81.0, RMSE on training set: 0.9978003292849077.\n",
      "iter: 82.0, RMSE on training set: 0.9975694288393028.\n",
      "iter: 83.0, RMSE on training set: 0.9973573680723897.\n",
      "iter: 84.0, RMSE on training set: 0.9971626995238677.\n",
      "iter: 85.0, RMSE on training set: 0.9969840775059188.\n",
      "iter: 86.0, RMSE on training set: 0.9968202519444024.\n",
      "iter: 87.0, RMSE on training set: 0.9966700624845902.\n",
      "iter: 88.0, RMSE on training set: 0.9965324328627883.\n",
      "iter: 89.0, RMSE on training set: 0.9964063655432445.\n",
      "iter: 90.0, RMSE on training set: 0.9962909366180539.\n",
      "iter: 91.0, RMSE on training set: 0.996185290966291.\n",
      "iter: 92.0, RMSE on training set: 0.9960886376673755.\n",
      "RMSE on test data: 0.9960793449317298.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9483046767177221.\n",
      "iter: 1.0, RMSE on training set: 1.8625715384126393.\n",
      "iter: 2.0, RMSE on training set: 1.7869936182974169.\n",
      "iter: 3.0, RMSE on training set: 1.7202671859369674.\n",
      "iter: 4.0, RMSE on training set: 1.6612511598506239.\n",
      "iter: 5.0, RMSE on training set: 1.608933582577258.\n",
      "iter: 6.0, RMSE on training set: 1.5624224000154436.\n",
      "iter: 7.0, RMSE on training set: 1.52093595261701.\n",
      "iter: 8.0, RMSE on training set: 1.4837931803335602.\n",
      "iter: 9.0, RMSE on training set: 1.450403769071238.\n",
      "iter: 10.0, RMSE on training set: 1.4202584721154106.\n",
      "iter: 11.0, RMSE on training set: 1.3929198137820646.\n",
      "iter: 12.0, RMSE on training set: 1.3680133391125726.\n",
      "iter: 13.0, RMSE on training set: 1.3452195237822298.\n",
      "iter: 14.0, RMSE on training set: 1.3242664103901878.\n",
      "iter: 15.0, RMSE on training set: 1.3049229956967918.\n",
      "iter: 16.0, RMSE on training set: 1.2869933603622798.\n",
      "iter: 17.0, RMSE on training set: 1.270311508664587.\n",
      "iter: 18.0, RMSE on training set: 1.2547368697442485.\n",
      "iter: 19.0, RMSE on training set: 1.240150402785962.\n",
      "iter: 20.0, RMSE on training set: 1.226451244677157.\n",
      "iter: 21.0, RMSE on training set: 1.2135538386399758.\n",
      "iter: 22.0, RMSE on training set: 1.201385484874704.\n",
      "iter: 23.0, RMSE on training set: 1.1898842583899665.\n",
      "iter: 24.0, RMSE on training set: 1.1789972441826866.\n",
      "iter: 25.0, RMSE on training set: 1.1686790452383233.\n",
      "iter: 26.0, RMSE on training set: 1.1588905240911322.\n",
      "iter: 27.0, RMSE on training set: 1.1495977436890625.\n",
      "iter: 28.0, RMSE on training set: 1.1407710779170348.\n",
      "iter: 29.0, RMSE on training set: 1.1323844662815459.\n",
      "iter: 30.0, RMSE on training set: 1.124414790929159.\n",
      "iter: 31.0, RMSE on training set: 1.1168413573719127.\n",
      "iter: 32.0, RMSE on training set: 1.1096454630532588.\n",
      "iter: 33.0, RMSE on training set: 1.1028100402495413.\n",
      "iter: 34.0, RMSE on training set: 1.0963193618106977.\n",
      "iter: 35.0, RMSE on training set: 1.0901587999477995.\n",
      "iter: 36.0, RMSE on training set: 1.084314629720338.\n",
      "iter: 37.0, RMSE on training set: 1.0787738701041727.\n",
      "iter: 38.0, RMSE on training set: 1.0735241565673879.\n",
      "iter: 39.0, RMSE on training set: 1.0685536399745832.\n",
      "iter: 40.0, RMSE on training set: 1.063850907403266.\n",
      "iter: 41.0, RMSE on training set: 1.0594049211074015.\n",
      "iter: 42.0, RMSE on training set: 1.0552049724184565.\n",
      "iter: 43.0, RMSE on training set: 1.051240647847437.\n",
      "iter: 44.0, RMSE on training set: 1.0475018050554525.\n",
      "iter: 45.0, RMSE on training set: 1.043978556706951.\n",
      "iter: 46.0, RMSE on training set: 1.0406612605189192.\n",
      "iter: 47.0, RMSE on training set: 1.0375405140790812.\n",
      "iter: 48.0, RMSE on training set: 1.0346071532325711.\n",
      "iter: 49.0, RMSE on training set: 1.031852253034408.\n",
      "iter: 50.0, RMSE on training set: 1.0292671304378158.\n",
      "iter: 51.0, RMSE on training set: 1.026843348038971.\n",
      "iter: 52.0, RMSE on training set: 1.024572718329438.\n",
      "iter: 53.0, RMSE on training set: 1.0224473080203829.\n",
      "iter: 54.0, RMSE on training set: 1.0204594420995243.\n",
      "iter: 55.0, RMSE on training set: 1.018601707364197.\n",
      "iter: 56.0, RMSE on training set: 1.0168669552435683.\n",
      "iter: 57.0, RMSE on training set: 1.0152483037812419.\n",
      "iter: 58.0, RMSE on training set: 1.0137391386978698.\n",
      "iter: 59.0, RMSE on training set: 1.012333113493087.\n",
      "iter: 60.0, RMSE on training set: 1.0110241485784244.\n",
      "iter: 61.0, RMSE on training set: 1.009806429458823.\n",
      "iter: 62.0, RMSE on training set: 1.0086744040008462.\n",
      "iter: 63.0, RMSE on training set: 1.0076227788414163.\n",
      "iter: 64.0, RMSE on training set: 1.006646515002436.\n",
      "iter: 65.0, RMSE on training set: 1.0057408227847073.\n",
      "iter: 66.0, RMSE on training set: 1.0049011560194496.\n",
      "iter: 67.0, RMSE on training set: 1.004123205758147.\n",
      "iter: 68.0, RMSE on training set: 1.0034028934816799.\n",
      "iter: 69.0, RMSE on training set: 1.0027363639083013.\n",
      "iter: 70.0, RMSE on training set: 1.0021199774772367.\n",
      "iter: 71.0, RMSE on training set: 1.001550302580945.\n",
      "iter: 72.0, RMSE on training set: 1.0010241076145558.\n",
      "iter: 73.0, RMSE on training set: 1.0005383529060263.\n",
      "iter: 74.0, RMSE on training set: 1.0000901825852306.\n",
      "iter: 75.0, RMSE on training set: 0.9996769164447931.\n",
      "iter: 76.0, RMSE on training set: 0.9992960418399818.\n",
      "iter: 77.0, RMSE on training set: 0.9989452056696847.\n",
      "iter: 78.0, RMSE on training set: 0.9986222064752963.\n",
      "iter: 79.0, RMSE on training set: 0.9983249866894345.\n",
      "iter: 80.0, RMSE on training set: 0.9980516250617673.\n",
      "iter: 81.0, RMSE on training set: 0.9978003292849077.\n",
      "iter: 82.0, RMSE on training set: 0.9975694288393028.\n",
      "iter: 83.0, RMSE on training set: 0.9973573680723897.\n",
      "iter: 84.0, RMSE on training set: 0.9971626995238677.\n",
      "iter: 85.0, RMSE on training set: 0.9969840775059188.\n",
      "iter: 86.0, RMSE on training set: 0.9968202519444024.\n",
      "iter: 87.0, RMSE on training set: 0.9966700624845902.\n",
      "iter: 88.0, RMSE on training set: 0.9965324328627883.\n",
      "iter: 89.0, RMSE on training set: 0.9964063655432445.\n",
      "iter: 90.0, RMSE on training set: 0.9962909366180539.\n",
      "iter: 91.0, RMSE on training set: 0.996185290966291.\n",
      "iter: 92.0, RMSE on training set: 0.9960886376673755.\n",
      "RMSE on test data: 0.9960837110300151.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9476438158383205.\n",
      "iter: 1.0, RMSE on training set: 1.861165683576763.\n",
      "iter: 2.0, RMSE on training set: 1.784976298619721.\n",
      "iter: 3.0, RMSE on training set: 1.7177475643546825.\n",
      "iter: 4.0, RMSE on training set: 1.658318045636259.\n",
      "iter: 5.0, RMSE on training set: 1.6056588582854006.\n",
      "iter: 6.0, RMSE on training set: 1.5588640419379145.\n",
      "iter: 7.0, RMSE on training set: 1.5171406572370156.\n",
      "iter: 8.0, RMSE on training set: 1.4797986155012837.\n",
      "iter: 9.0, RMSE on training set: 1.4462404746343867.\n",
      "iter: 10.0, RMSE on training set: 1.415951440141924.\n",
      "iter: 11.0, RMSE on training set: 1.388489782219458.\n",
      "iter: 12.0, RMSE on training set: 1.3634778344065004.\n",
      "iter: 13.0, RMSE on training set: 1.3405936877557887.\n",
      "iter: 14.0, RMSE on training set: 1.31956364494941.\n",
      "iter: 15.0, RMSE on training set: 1.300155456150279.\n",
      "iter: 16.0, RMSE on training set: 1.2821723247767114.\n",
      "iter: 17.0, RMSE on training set: 1.2654476470966989.\n",
      "iter: 18.0, RMSE on training set: 1.2498404336686728.\n",
      "iter: 19.0, RMSE on training set: 1.235231351756323.\n",
      "iter: 20.0, RMSE on training set: 1.221519324315322.\n",
      "iter: 21.0, RMSE on training set: 1.208618621485701.\n",
      "iter: 22.0, RMSE on training set: 1.196456383445179.\n",
      "iter: 23.0, RMSE on training set: 1.1849705179707726.\n",
      "iter: 24.0, RMSE on training set: 1.1741079213607015.\n",
      "iter: 25.0, RMSE on training set: 1.1638229769502197.\n",
      "iter: 26.0, RMSE on training set: 1.1540762909559528.\n",
      "iter: 27.0, RMSE on training set: 1.1448336305803524.\n",
      "iter: 28.0, RMSE on training set: 1.1360650340741554.\n",
      "iter: 29.0, RMSE on training set: 1.127744066730397.\n",
      "iter: 30.0, RMSE on training set: 1.1198472005537656.\n",
      "iter: 31.0, RMSE on training set: 1.1123532986285845.\n",
      "iter: 32.0, RMSE on training set: 1.1052431880309783.\n",
      "iter: 33.0, RMSE on training set: 1.098499307540684.\n",
      "iter: 34.0, RMSE on training set: 1.0921054184557828.\n",
      "iter: 35.0, RMSE on training set: 1.0860463685506556.\n",
      "iter: 36.0, RMSE on training set: 1.080307900692065.\n",
      "iter: 37.0, RMSE on training set: 1.0748764988832173.\n",
      "iter: 38.0, RMSE on training set: 1.0697392655762459.\n",
      "iter: 39.0, RMSE on training set: 1.06488382500815.\n",
      "iter: 40.0, RMSE on training set: 1.0602982480958163.\n",
      "iter: 41.0, RMSE on training set: 1.0559709950909888.\n",
      "iter: 42.0, RMSE on training set: 1.0518908727620986.\n",
      "iter: 43.0, RMSE on training set: 1.0480470033520461.\n",
      "iter: 44.0, RMSE on training set: 1.044428802973226.\n",
      "iter: 45.0, RMSE on training set: 1.0410259674555868.\n",
      "iter: 46.0, RMSE on training set: 1.0378284639701765.\n",
      "iter: 47.0, RMSE on training set: 1.0348265270171635.\n",
      "iter: 48.0, RMSE on training set: 1.0320106575995476.\n",
      "iter: 49.0, RMSE on training set: 1.0293716246060893.\n",
      "iter: 50.0, RMSE on training set: 1.0269004676029991.\n",
      "iter: 51.0, RMSE on training set: 1.0245885003865423.\n",
      "iter: 52.0, RMSE on training set: 1.0224273147804834.\n",
      "iter: 53.0, RMSE on training set: 1.02040878427541.\n",
      "iter: 54.0, RMSE on training set: 1.0185250672033264.\n",
      "iter: 55.0, RMSE on training set: 1.0167686092223924.\n",
      "iter: 56.0, RMSE on training set: 1.0151321449548623.\n",
      "iter: 57.0, RMSE on training set: 1.0136086986778687.\n",
      "iter: 58.0, RMSE on training set: 1.0121915840131774.\n",
      "iter: 59.0, RMSE on training set: 1.0108744025997387.\n",
      "iter: 60.0, RMSE on training set: 1.0096510417631652.\n",
      "iter: 61.0, RMSE on training set: 1.0085156712199532.\n",
      "iter: 62.0, RMSE on training set: 1.0074627388725284.\n",
      "iter: 63.0, RMSE on training set: 1.006486965764563.\n",
      "iter: 64.0, RMSE on training set: 1.0055833402752963.\n",
      "iter: 65.0, RMSE on training set: 1.0047471116373348.\n",
      "iter: 66.0, RMSE on training set: 1.0039737828653048.\n",
      "iter: 67.0, RMSE on training set: 1.003259103183134.\n",
      "iter: 68.0, RMSE on training set: 1.0025990600362884.\n",
      "iter: 69.0, RMSE on training set: 1.0019898707722872.\n",
      "iter: 70.0, RMSE on training set: 1.0014279740687126.\n",
      "iter: 71.0, RMSE on training set: 1.0009100211829038.\n",
      "iter: 72.0, RMSE on training set: 1.000432867092074.\n",
      "iter: 73.0, RMSE on training set: 0.9999935615866367.\n",
      "iter: 74.0, RMSE on training set: 0.9995893403735359.\n",
      "iter: 75.0, RMSE on training set: 0.9992176162403608.\n",
      "iter: 76.0, RMSE on training set: 0.9988759703250508.\n",
      "iter: 77.0, RMSE on training set: 0.9985621435303912.\n",
      "iter: 78.0, RMSE on training set: 0.9982740281170179.\n",
      "iter: 79.0, RMSE on training set: 0.9980096595035934.\n",
      "iter: 80.0, RMSE on training set: 0.9977672082980834.\n",
      "iter: 81.0, RMSE on training set: 0.997544972579702.\n",
      "iter: 82.0, RMSE on training set: 0.9973413704470594.\n",
      "iter: 83.0, RMSE on training set: 0.997154932844441.\n",
      "iter: 84.0, RMSE on training set: 0.9969842966747823.\n",
      "iter: 85.0, RMSE on training set: 0.9968281982049608.\n",
      "iter: 86.0, RMSE on training set: 0.9966854667663472.\n",
      "iter: 87.0, RMSE on training set: 0.9965550187511535.\n",
      "iter: 88.0, RMSE on training set: 0.9964358519030375.\n",
      "iter: 89.0, RMSE on training set: 0.9963270398986134.\n",
      "iter: 90.0, RMSE on training set: 0.9962277272148997.\n",
      "RMSE on test data: 0.9962372006825967.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9476438158383205.\n",
      "iter: 1.0, RMSE on training set: 1.861165683576763.\n",
      "iter: 2.0, RMSE on training set: 1.784976298619721.\n",
      "iter: 3.0, RMSE on training set: 1.7177475643546825.\n",
      "iter: 4.0, RMSE on training set: 1.658318045636259.\n",
      "iter: 5.0, RMSE on training set: 1.6056588582854006.\n",
      "iter: 6.0, RMSE on training set: 1.5588640419379145.\n",
      "iter: 7.0, RMSE on training set: 1.5171406572370156.\n",
      "iter: 8.0, RMSE on training set: 1.4797986155012837.\n",
      "iter: 9.0, RMSE on training set: 1.4462404746343867.\n",
      "iter: 10.0, RMSE on training set: 1.415951440141924.\n",
      "iter: 11.0, RMSE on training set: 1.388489782219458.\n",
      "iter: 12.0, RMSE on training set: 1.3634778344065004.\n",
      "iter: 13.0, RMSE on training set: 1.3405936877557887.\n",
      "iter: 14.0, RMSE on training set: 1.31956364494941.\n",
      "iter: 15.0, RMSE on training set: 1.300155456150279.\n",
      "iter: 16.0, RMSE on training set: 1.2821723247767114.\n",
      "iter: 17.0, RMSE on training set: 1.2654476470966989.\n",
      "iter: 18.0, RMSE on training set: 1.2498404336686728.\n",
      "iter: 19.0, RMSE on training set: 1.235231351756323.\n",
      "iter: 20.0, RMSE on training set: 1.221519324315322.\n",
      "iter: 21.0, RMSE on training set: 1.208618621485701.\n",
      "iter: 22.0, RMSE on training set: 1.196456383445179.\n",
      "iter: 23.0, RMSE on training set: 1.1849705179707726.\n",
      "iter: 24.0, RMSE on training set: 1.1741079213607015.\n",
      "iter: 25.0, RMSE on training set: 1.1638229769502197.\n",
      "iter: 26.0, RMSE on training set: 1.1540762909559528.\n",
      "iter: 27.0, RMSE on training set: 1.1448336305803524.\n",
      "iter: 28.0, RMSE on training set: 1.1360650340741554.\n",
      "iter: 29.0, RMSE on training set: 1.127744066730397.\n",
      "iter: 30.0, RMSE on training set: 1.1198472005537656.\n",
      "iter: 31.0, RMSE on training set: 1.1123532986285845.\n",
      "iter: 32.0, RMSE on training set: 1.1052431880309783.\n",
      "iter: 33.0, RMSE on training set: 1.098499307540684.\n",
      "iter: 34.0, RMSE on training set: 1.0921054184557828.\n",
      "iter: 35.0, RMSE on training set: 1.0860463685506556.\n",
      "iter: 36.0, RMSE on training set: 1.080307900692065.\n",
      "iter: 37.0, RMSE on training set: 1.0748764988832173.\n",
      "iter: 38.0, RMSE on training set: 1.0697392655762459.\n",
      "iter: 39.0, RMSE on training set: 1.06488382500815.\n",
      "iter: 40.0, RMSE on training set: 1.0602982480958163.\n",
      "iter: 41.0, RMSE on training set: 1.0559709950909888.\n",
      "iter: 42.0, RMSE on training set: 1.0518908727620986.\n",
      "iter: 43.0, RMSE on training set: 1.0480470033520461.\n",
      "iter: 44.0, RMSE on training set: 1.044428802973226.\n",
      "iter: 45.0, RMSE on training set: 1.0410259674555868.\n",
      "iter: 46.0, RMSE on training set: 1.0378284639701765.\n",
      "iter: 47.0, RMSE on training set: 1.0348265270171635.\n",
      "iter: 48.0, RMSE on training set: 1.0320106575995476.\n",
      "iter: 49.0, RMSE on training set: 1.0293716246060893.\n",
      "iter: 50.0, RMSE on training set: 1.0269004676029991.\n",
      "iter: 51.0, RMSE on training set: 1.0245885003865423.\n",
      "iter: 52.0, RMSE on training set: 1.0224273147804834.\n",
      "iter: 53.0, RMSE on training set: 1.02040878427541.\n",
      "iter: 54.0, RMSE on training set: 1.0185250672033264.\n",
      "iter: 55.0, RMSE on training set: 1.0167686092223924.\n",
      "iter: 56.0, RMSE on training set: 1.0151321449548623.\n",
      "iter: 57.0, RMSE on training set: 1.0136086986778687.\n",
      "iter: 58.0, RMSE on training set: 1.0121915840131774.\n",
      "iter: 59.0, RMSE on training set: 1.0108744025997387.\n",
      "iter: 60.0, RMSE on training set: 1.0096510417631652.\n",
      "iter: 61.0, RMSE on training set: 1.0085156712199532.\n",
      "iter: 62.0, RMSE on training set: 1.0074627388725284.\n",
      "iter: 63.0, RMSE on training set: 1.006486965764563.\n",
      "iter: 64.0, RMSE on training set: 1.0055833402752963.\n",
      "iter: 65.0, RMSE on training set: 1.0047471116373348.\n",
      "iter: 66.0, RMSE on training set: 1.0039737828653048.\n",
      "iter: 67.0, RMSE on training set: 1.003259103183134.\n",
      "iter: 68.0, RMSE on training set: 1.0025990600362884.\n",
      "iter: 69.0, RMSE on training set: 1.0019898707722872.\n",
      "iter: 70.0, RMSE on training set: 1.0014279740687126.\n",
      "iter: 71.0, RMSE on training set: 1.0009100211829038.\n",
      "iter: 72.0, RMSE on training set: 1.000432867092074.\n",
      "iter: 73.0, RMSE on training set: 0.9999935615866367.\n",
      "iter: 74.0, RMSE on training set: 0.9995893403735359.\n",
      "iter: 75.0, RMSE on training set: 0.9992176162403608.\n",
      "iter: 76.0, RMSE on training set: 0.9988759703250508.\n",
      "iter: 77.0, RMSE on training set: 0.9985621435303912.\n",
      "iter: 78.0, RMSE on training set: 0.9982740281170179.\n",
      "iter: 79.0, RMSE on training set: 0.9980096595035934.\n",
      "iter: 80.0, RMSE on training set: 0.9977672082980834.\n",
      "iter: 81.0, RMSE on training set: 0.997544972579702.\n",
      "iter: 82.0, RMSE on training set: 0.9973413704470594.\n",
      "iter: 83.0, RMSE on training set: 0.997154932844441.\n",
      "iter: 84.0, RMSE on training set: 0.9969842966747823.\n",
      "iter: 85.0, RMSE on training set: 0.9968281982049608.\n",
      "iter: 86.0, RMSE on training set: 0.9966854667663472.\n",
      "iter: 87.0, RMSE on training set: 0.9965550187511535.\n",
      "iter: 88.0, RMSE on training set: 0.9964358519030375.\n",
      "iter: 89.0, RMSE on training set: 0.9963270398986134.\n",
      "iter: 90.0, RMSE on training set: 0.9962277272148997.\n",
      "RMSE on test data: 0.9962362677954301.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9476438158383205.\n",
      "iter: 1.0, RMSE on training set: 1.861165683576763.\n",
      "iter: 2.0, RMSE on training set: 1.784976298619721.\n",
      "iter: 3.0, RMSE on training set: 1.7177475643546825.\n",
      "iter: 4.0, RMSE on training set: 1.658318045636259.\n",
      "iter: 5.0, RMSE on training set: 1.6056588582854006.\n",
      "iter: 6.0, RMSE on training set: 1.5588640419379145.\n",
      "iter: 7.0, RMSE on training set: 1.5171406572370156.\n",
      "iter: 8.0, RMSE on training set: 1.4797986155012837.\n",
      "iter: 9.0, RMSE on training set: 1.4462404746343867.\n",
      "iter: 10.0, RMSE on training set: 1.415951440141924.\n",
      "iter: 11.0, RMSE on training set: 1.388489782219458.\n",
      "iter: 12.0, RMSE on training set: 1.3634778344065004.\n",
      "iter: 13.0, RMSE on training set: 1.3405936877557887.\n",
      "iter: 14.0, RMSE on training set: 1.31956364494941.\n",
      "iter: 15.0, RMSE on training set: 1.300155456150279.\n",
      "iter: 16.0, RMSE on training set: 1.2821723247767114.\n",
      "iter: 17.0, RMSE on training set: 1.2654476470966989.\n",
      "iter: 18.0, RMSE on training set: 1.2498404336686728.\n",
      "iter: 19.0, RMSE on training set: 1.235231351756323.\n",
      "iter: 20.0, RMSE on training set: 1.221519324315322.\n",
      "iter: 21.0, RMSE on training set: 1.208618621485701.\n",
      "iter: 22.0, RMSE on training set: 1.196456383445179.\n",
      "iter: 23.0, RMSE on training set: 1.1849705179707726.\n",
      "iter: 24.0, RMSE on training set: 1.1741079213607015.\n",
      "iter: 25.0, RMSE on training set: 1.1638229769502197.\n",
      "iter: 26.0, RMSE on training set: 1.1540762909559528.\n",
      "iter: 27.0, RMSE on training set: 1.1448336305803524.\n",
      "iter: 28.0, RMSE on training set: 1.1360650340741554.\n",
      "iter: 29.0, RMSE on training set: 1.127744066730397.\n",
      "iter: 30.0, RMSE on training set: 1.1198472005537656.\n",
      "iter: 31.0, RMSE on training set: 1.1123532986285845.\n",
      "iter: 32.0, RMSE on training set: 1.1052431880309783.\n",
      "iter: 33.0, RMSE on training set: 1.098499307540684.\n",
      "iter: 34.0, RMSE on training set: 1.0921054184557828.\n",
      "iter: 35.0, RMSE on training set: 1.0860463685506556.\n",
      "iter: 36.0, RMSE on training set: 1.080307900692065.\n",
      "iter: 37.0, RMSE on training set: 1.0748764988832173.\n",
      "iter: 38.0, RMSE on training set: 1.0697392655762459.\n",
      "iter: 39.0, RMSE on training set: 1.06488382500815.\n",
      "iter: 40.0, RMSE on training set: 1.0602982480958163.\n",
      "iter: 41.0, RMSE on training set: 1.0559709950909888.\n",
      "iter: 42.0, RMSE on training set: 1.0518908727620986.\n",
      "iter: 43.0, RMSE on training set: 1.0480470033520461.\n",
      "iter: 44.0, RMSE on training set: 1.044428802973226.\n",
      "iter: 45.0, RMSE on training set: 1.0410259674555868.\n",
      "iter: 46.0, RMSE on training set: 1.0378284639701765.\n",
      "iter: 47.0, RMSE on training set: 1.0348265270171635.\n",
      "iter: 48.0, RMSE on training set: 1.0320106575995476.\n",
      "iter: 49.0, RMSE on training set: 1.0293716246060893.\n",
      "iter: 50.0, RMSE on training set: 1.0269004676029991.\n",
      "iter: 51.0, RMSE on training set: 1.0245885003865423.\n",
      "iter: 52.0, RMSE on training set: 1.0224273147804834.\n",
      "iter: 53.0, RMSE on training set: 1.02040878427541.\n",
      "iter: 54.0, RMSE on training set: 1.0185250672033264.\n",
      "iter: 55.0, RMSE on training set: 1.0167686092223924.\n",
      "iter: 56.0, RMSE on training set: 1.0151321449548623.\n",
      "iter: 57.0, RMSE on training set: 1.0136086986778687.\n",
      "iter: 58.0, RMSE on training set: 1.0121915840131774.\n",
      "iter: 59.0, RMSE on training set: 1.0108744025997387.\n",
      "iter: 60.0, RMSE on training set: 1.0096510417631652.\n",
      "iter: 61.0, RMSE on training set: 1.0085156712199532.\n",
      "iter: 62.0, RMSE on training set: 1.0074627388725284.\n",
      "iter: 63.0, RMSE on training set: 1.006486965764563.\n",
      "iter: 64.0, RMSE on training set: 1.0055833402752963.\n",
      "iter: 65.0, RMSE on training set: 1.0047471116373348.\n",
      "iter: 66.0, RMSE on training set: 1.0039737828653048.\n",
      "iter: 67.0, RMSE on training set: 1.003259103183134.\n",
      "iter: 68.0, RMSE on training set: 1.0025990600362884.\n",
      "iter: 69.0, RMSE on training set: 1.0019898707722872.\n",
      "iter: 70.0, RMSE on training set: 1.0014279740687126.\n",
      "iter: 71.0, RMSE on training set: 1.0009100211829038.\n",
      "iter: 72.0, RMSE on training set: 1.000432867092074.\n",
      "iter: 73.0, RMSE on training set: 0.9999935615866367.\n",
      "iter: 74.0, RMSE on training set: 0.9995893403735359.\n",
      "iter: 75.0, RMSE on training set: 0.9992176162403608.\n",
      "iter: 76.0, RMSE on training set: 0.9988759703250508.\n",
      "iter: 77.0, RMSE on training set: 0.9985621435303912.\n",
      "iter: 78.0, RMSE on training set: 0.9982740281170179.\n",
      "iter: 79.0, RMSE on training set: 0.9980096595035934.\n",
      "iter: 80.0, RMSE on training set: 0.9977672082980834.\n",
      "iter: 81.0, RMSE on training set: 0.997544972579702.\n",
      "iter: 82.0, RMSE on training set: 0.9973413704470594.\n",
      "iter: 83.0, RMSE on training set: 0.997154932844441.\n",
      "iter: 84.0, RMSE on training set: 0.9969842966747823.\n",
      "iter: 85.0, RMSE on training set: 0.9968281982049608.\n",
      "iter: 86.0, RMSE on training set: 0.9966854667663472.\n",
      "iter: 87.0, RMSE on training set: 0.9965550187511535.\n",
      "iter: 88.0, RMSE on training set: 0.9964358519030375.\n",
      "iter: 89.0, RMSE on training set: 0.9963270398986134.\n",
      "iter: 90.0, RMSE on training set: 0.9962277272148997.\n",
      "RMSE on test data: 0.9962296250363297.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9476438158383205.\n",
      "iter: 1.0, RMSE on training set: 1.861165683576763.\n",
      "iter: 2.0, RMSE on training set: 1.784976298619721.\n",
      "iter: 3.0, RMSE on training set: 1.7177475643546825.\n",
      "iter: 4.0, RMSE on training set: 1.658318045636259.\n",
      "iter: 5.0, RMSE on training set: 1.6056588582854006.\n",
      "iter: 6.0, RMSE on training set: 1.5588640419379145.\n",
      "iter: 7.0, RMSE on training set: 1.5171406572370156.\n",
      "iter: 8.0, RMSE on training set: 1.4797986155012837.\n",
      "iter: 9.0, RMSE on training set: 1.4462404746343867.\n",
      "iter: 10.0, RMSE on training set: 1.415951440141924.\n",
      "iter: 11.0, RMSE on training set: 1.388489782219458.\n",
      "iter: 12.0, RMSE on training set: 1.3634778344065004.\n",
      "iter: 13.0, RMSE on training set: 1.3405936877557887.\n",
      "iter: 14.0, RMSE on training set: 1.31956364494941.\n",
      "iter: 15.0, RMSE on training set: 1.300155456150279.\n",
      "iter: 16.0, RMSE on training set: 1.2821723247767114.\n",
      "iter: 17.0, RMSE on training set: 1.2654476470966989.\n",
      "iter: 18.0, RMSE on training set: 1.2498404336686728.\n",
      "iter: 19.0, RMSE on training set: 1.235231351756323.\n",
      "iter: 20.0, RMSE on training set: 1.221519324315322.\n",
      "iter: 21.0, RMSE on training set: 1.208618621485701.\n",
      "iter: 22.0, RMSE on training set: 1.196456383445179.\n",
      "iter: 23.0, RMSE on training set: 1.1849705179707726.\n",
      "iter: 24.0, RMSE on training set: 1.1741079213607015.\n",
      "iter: 25.0, RMSE on training set: 1.1638229769502197.\n",
      "iter: 26.0, RMSE on training set: 1.1540762909559528.\n",
      "iter: 27.0, RMSE on training set: 1.1448336305803524.\n",
      "iter: 28.0, RMSE on training set: 1.1360650340741554.\n",
      "iter: 29.0, RMSE on training set: 1.127744066730397.\n",
      "iter: 30.0, RMSE on training set: 1.1198472005537656.\n",
      "iter: 31.0, RMSE on training set: 1.1123532986285845.\n",
      "iter: 32.0, RMSE on training set: 1.1052431880309783.\n",
      "iter: 33.0, RMSE on training set: 1.098499307540684.\n",
      "iter: 34.0, RMSE on training set: 1.0921054184557828.\n",
      "iter: 35.0, RMSE on training set: 1.0860463685506556.\n",
      "iter: 36.0, RMSE on training set: 1.080307900692065.\n",
      "iter: 37.0, RMSE on training set: 1.0748764988832173.\n",
      "iter: 38.0, RMSE on training set: 1.0697392655762459.\n",
      "iter: 39.0, RMSE on training set: 1.06488382500815.\n",
      "iter: 40.0, RMSE on training set: 1.0602982480958163.\n",
      "iter: 41.0, RMSE on training set: 1.0559709950909888.\n",
      "iter: 42.0, RMSE on training set: 1.0518908727620986.\n",
      "iter: 43.0, RMSE on training set: 1.0480470033520461.\n",
      "iter: 44.0, RMSE on training set: 1.044428802973226.\n",
      "iter: 45.0, RMSE on training set: 1.0410259674555868.\n",
      "iter: 46.0, RMSE on training set: 1.0378284639701765.\n",
      "iter: 47.0, RMSE on training set: 1.0348265270171635.\n",
      "iter: 48.0, RMSE on training set: 1.0320106575995476.\n",
      "iter: 49.0, RMSE on training set: 1.0293716246060893.\n",
      "iter: 50.0, RMSE on training set: 1.0269004676029991.\n",
      "iter: 51.0, RMSE on training set: 1.0245885003865423.\n",
      "iter: 52.0, RMSE on training set: 1.0224273147804834.\n",
      "iter: 53.0, RMSE on training set: 1.02040878427541.\n",
      "iter: 54.0, RMSE on training set: 1.0185250672033264.\n",
      "iter: 55.0, RMSE on training set: 1.0167686092223924.\n",
      "iter: 56.0, RMSE on training set: 1.0151321449548623.\n",
      "iter: 57.0, RMSE on training set: 1.0136086986778687.\n",
      "iter: 58.0, RMSE on training set: 1.0121915840131774.\n",
      "iter: 59.0, RMSE on training set: 1.0108744025997387.\n",
      "iter: 60.0, RMSE on training set: 1.0096510417631652.\n",
      "iter: 61.0, RMSE on training set: 1.0085156712199532.\n",
      "iter: 62.0, RMSE on training set: 1.0074627388725284.\n",
      "iter: 63.0, RMSE on training set: 1.006486965764563.\n",
      "iter: 64.0, RMSE on training set: 1.0055833402752963.\n",
      "iter: 65.0, RMSE on training set: 1.0047471116373348.\n",
      "iter: 66.0, RMSE on training set: 1.0039737828653048.\n",
      "iter: 67.0, RMSE on training set: 1.003259103183134.\n",
      "iter: 68.0, RMSE on training set: 1.0025990600362884.\n",
      "iter: 69.0, RMSE on training set: 1.0019898707722872.\n",
      "iter: 70.0, RMSE on training set: 1.0014279740687126.\n",
      "iter: 71.0, RMSE on training set: 1.0009100211829038.\n",
      "iter: 72.0, RMSE on training set: 1.000432867092074.\n",
      "iter: 73.0, RMSE on training set: 0.9999935615866367.\n",
      "iter: 74.0, RMSE on training set: 0.9995893403735359.\n",
      "iter: 75.0, RMSE on training set: 0.9992176162403608.\n",
      "iter: 76.0, RMSE on training set: 0.9988759703250508.\n",
      "iter: 77.0, RMSE on training set: 0.9985621435303912.\n",
      "iter: 78.0, RMSE on training set: 0.9982740281170179.\n",
      "iter: 79.0, RMSE on training set: 0.9980096595035934.\n",
      "iter: 80.0, RMSE on training set: 0.9977672082980834.\n",
      "iter: 81.0, RMSE on training set: 0.997544972579702.\n",
      "iter: 82.0, RMSE on training set: 0.9973413704470594.\n",
      "iter: 83.0, RMSE on training set: 0.997154932844441.\n",
      "iter: 84.0, RMSE on training set: 0.9969842966747823.\n",
      "iter: 85.0, RMSE on training set: 0.9968281982049608.\n",
      "iter: 86.0, RMSE on training set: 0.9966854667663472.\n",
      "iter: 87.0, RMSE on training set: 0.9965550187511535.\n",
      "iter: 88.0, RMSE on training set: 0.9964358519030375.\n",
      "iter: 89.0, RMSE on training set: 0.9963270398986134.\n",
      "iter: 90.0, RMSE on training set: 0.9962277272148997.\n",
      "RMSE on test data: 0.9962183651032012.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.9476438158383205.\n",
      "iter: 1.0, RMSE on training set: 1.861165683576763.\n",
      "iter: 2.0, RMSE on training set: 1.784976298619721.\n",
      "iter: 3.0, RMSE on training set: 1.7177475643546825.\n",
      "iter: 4.0, RMSE on training set: 1.658318045636259.\n",
      "iter: 5.0, RMSE on training set: 1.6056588582854006.\n",
      "iter: 6.0, RMSE on training set: 1.5588640419379145.\n",
      "iter: 7.0, RMSE on training set: 1.5171406572370156.\n",
      "iter: 8.0, RMSE on training set: 1.4797986155012837.\n",
      "iter: 9.0, RMSE on training set: 1.4462404746343867.\n",
      "iter: 10.0, RMSE on training set: 1.415951440141924.\n",
      "iter: 11.0, RMSE on training set: 1.388489782219458.\n",
      "iter: 12.0, RMSE on training set: 1.3634778344065004.\n",
      "iter: 13.0, RMSE on training set: 1.3405936877557887.\n",
      "iter: 14.0, RMSE on training set: 1.31956364494941.\n",
      "iter: 15.0, RMSE on training set: 1.300155456150279.\n",
      "iter: 16.0, RMSE on training set: 1.2821723247767114.\n",
      "iter: 17.0, RMSE on training set: 1.2654476470966989.\n",
      "iter: 18.0, RMSE on training set: 1.2498404336686728.\n",
      "iter: 19.0, RMSE on training set: 1.235231351756323.\n",
      "iter: 20.0, RMSE on training set: 1.221519324315322.\n",
      "iter: 21.0, RMSE on training set: 1.208618621485701.\n",
      "iter: 22.0, RMSE on training set: 1.196456383445179.\n",
      "iter: 23.0, RMSE on training set: 1.1849705179707726.\n",
      "iter: 24.0, RMSE on training set: 1.1741079213607015.\n",
      "iter: 25.0, RMSE on training set: 1.1638229769502197.\n",
      "iter: 26.0, RMSE on training set: 1.1540762909559528.\n",
      "iter: 27.0, RMSE on training set: 1.1448336305803524.\n",
      "iter: 28.0, RMSE on training set: 1.1360650340741554.\n",
      "iter: 29.0, RMSE on training set: 1.127744066730397.\n",
      "iter: 30.0, RMSE on training set: 1.1198472005537656.\n",
      "iter: 31.0, RMSE on training set: 1.1123532986285845.\n",
      "iter: 32.0, RMSE on training set: 1.1052431880309783.\n",
      "iter: 33.0, RMSE on training set: 1.098499307540684.\n",
      "iter: 34.0, RMSE on training set: 1.0921054184557828.\n",
      "iter: 35.0, RMSE on training set: 1.0860463685506556.\n",
      "iter: 36.0, RMSE on training set: 1.080307900692065.\n",
      "iter: 37.0, RMSE on training set: 1.0748764988832173.\n",
      "iter: 38.0, RMSE on training set: 1.0697392655762459.\n",
      "iter: 39.0, RMSE on training set: 1.06488382500815.\n",
      "iter: 40.0, RMSE on training set: 1.0602982480958163.\n",
      "iter: 41.0, RMSE on training set: 1.0559709950909888.\n",
      "iter: 42.0, RMSE on training set: 1.0518908727620986.\n",
      "iter: 43.0, RMSE on training set: 1.0480470033520461.\n",
      "iter: 44.0, RMSE on training set: 1.044428802973226.\n",
      "iter: 45.0, RMSE on training set: 1.0410259674555868.\n",
      "iter: 46.0, RMSE on training set: 1.0378284639701765.\n",
      "iter: 47.0, RMSE on training set: 1.0348265270171635.\n",
      "iter: 48.0, RMSE on training set: 1.0320106575995476.\n",
      "iter: 49.0, RMSE on training set: 1.0293716246060893.\n",
      "iter: 50.0, RMSE on training set: 1.0269004676029991.\n",
      "iter: 51.0, RMSE on training set: 1.0245885003865423.\n",
      "iter: 52.0, RMSE on training set: 1.0224273147804834.\n",
      "iter: 53.0, RMSE on training set: 1.02040878427541.\n",
      "iter: 54.0, RMSE on training set: 1.0185250672033264.\n",
      "iter: 55.0, RMSE on training set: 1.0167686092223924.\n",
      "iter: 56.0, RMSE on training set: 1.0151321449548623.\n",
      "iter: 57.0, RMSE on training set: 1.0136086986778687.\n",
      "iter: 58.0, RMSE on training set: 1.0121915840131774.\n",
      "iter: 59.0, RMSE on training set: 1.0108744025997387.\n",
      "iter: 60.0, RMSE on training set: 1.0096510417631652.\n",
      "iter: 61.0, RMSE on training set: 1.0085156712199532.\n",
      "iter: 62.0, RMSE on training set: 1.0074627388725284.\n",
      "iter: 63.0, RMSE on training set: 1.006486965764563.\n",
      "iter: 64.0, RMSE on training set: 1.0055833402752963.\n",
      "iter: 65.0, RMSE on training set: 1.0047471116373348.\n",
      "iter: 66.0, RMSE on training set: 1.0039737828653048.\n",
      "iter: 67.0, RMSE on training set: 1.003259103183134.\n",
      "iter: 68.0, RMSE on training set: 1.0025990600362884.\n",
      "iter: 69.0, RMSE on training set: 1.0019898707722872.\n",
      "iter: 70.0, RMSE on training set: 1.0014279740687126.\n",
      "iter: 71.0, RMSE on training set: 1.0009100211829038.\n",
      "iter: 72.0, RMSE on training set: 1.000432867092074.\n",
      "iter: 73.0, RMSE on training set: 0.9999935615866367.\n",
      "iter: 74.0, RMSE on training set: 0.9995893403735359.\n",
      "iter: 75.0, RMSE on training set: 0.9992176162403608.\n",
      "iter: 76.0, RMSE on training set: 0.9988759703250508.\n",
      "iter: 77.0, RMSE on training set: 0.9985621435303912.\n",
      "iter: 78.0, RMSE on training set: 0.9982740281170179.\n",
      "iter: 79.0, RMSE on training set: 0.9980096595035934.\n",
      "iter: 80.0, RMSE on training set: 0.9977672082980834.\n",
      "iter: 81.0, RMSE on training set: 0.997544972579702.\n",
      "iter: 82.0, RMSE on training set: 0.9973413704470594.\n",
      "iter: 83.0, RMSE on training set: 0.997154932844441.\n",
      "iter: 84.0, RMSE on training set: 0.9969842966747823.\n",
      "iter: 85.0, RMSE on training set: 0.9968281982049608.\n",
      "iter: 86.0, RMSE on training set: 0.9966854667663472.\n",
      "iter: 87.0, RMSE on training set: 0.9965550187511535.\n",
      "iter: 88.0, RMSE on training set: 0.9964358519030375.\n",
      "iter: 89.0, RMSE on training set: 0.9963270398986134.\n",
      "iter: 90.0, RMSE on training set: 0.9962277272148997.\n",
      "RMSE on test data: 0.9962227446946215.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEVCAYAAACmMTGfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c426f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 4   # K in the lecture notes\n",
    "lambda_user_arr = [0.01, 0.1, 1, 10]\n",
    "lambda_item = 0.7\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "for i, lambda_user in enumerate(lambda_user_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings,  num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lambda_item=0.01\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.995728383964671.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957218196221085.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957711920016705.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957917965555064.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957528590618204.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957708444956962.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957945439993396.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957510452764944.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957561527774413.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600884532752853.\n",
      "iter: 1.0, RMSE on training set: 2.059975489789081.\n",
      "iter: 2.0, RMSE on training set: 1.9719940549614947.\n",
      "iter: 3.0, RMSE on training set: 1.8944981469247968.\n",
      "iter: 4.0, RMSE on training set: 1.82610220237677.\n",
      "iter: 5.0, RMSE on training set: 1.765589430137033.\n",
      "iter: 6.0, RMSE on training set: 1.7118962522386145.\n",
      "iter: 7.0, RMSE on training set: 1.6640971291473738.\n",
      "iter: 8.0, RMSE on training set: 1.6213898783493914.\n",
      "iter: 9.0, RMSE on training set: 1.5830816863616493.\n",
      "iter: 10.0, RMSE on training set: 1.5485759889125805.\n",
      "iter: 11.0, RMSE on training set: 1.5173603525115549.\n",
      "iter: 12.0, RMSE on training set: 1.4889954406938957.\n",
      "iter: 13.0, RMSE on training set: 1.463105098356067.\n",
      "iter: 14.0, RMSE on training set: 1.4393675438339046.\n",
      "iter: 15.0, RMSE on training set: 1.417507623843442.\n",
      "iter: 16.0, RMSE on training set: 1.3972900618955861.\n",
      "iter: 17.0, RMSE on training set: 1.3785136156041504.\n",
      "iter: 18.0, RMSE on training set: 1.3610060509302264.\n",
      "iter: 19.0, RMSE on training set: 1.3446198400700429.\n",
      "iter: 20.0, RMSE on training set: 1.3292284926781146.\n",
      "iter: 21.0, RMSE on training set: 1.3147234359308018.\n",
      "iter: 22.0, RMSE on training set: 1.3010113663813503.\n",
      "iter: 23.0, RMSE on training set: 1.2880120047367043.\n",
      "iter: 24.0, RMSE on training set: 1.2756561929627224.\n",
      "iter: 25.0, RMSE on training set: 1.2638842810770092.\n",
      "iter: 26.0, RMSE on training set: 1.252644758362572.\n",
      "iter: 27.0, RMSE on training set: 1.241893090397481.\n",
      "iter: 28.0, RMSE on training set: 1.2315907291971822.\n",
      "iter: 29.0, RMSE on training set: 1.2217042689144764.\n",
      "iter: 30.0, RMSE on training set: 1.2122047239792542.\n",
      "iter: 31.0, RMSE on training set: 1.2030669103472296.\n",
      "iter: 32.0, RMSE on training set: 1.1942689137334792.\n",
      "iter: 33.0, RMSE on training set: 1.1857916314041999.\n",
      "iter: 34.0, RMSE on training set: 1.1776183763569377.\n",
      "iter: 35.0, RMSE on training set: 1.1697345345992158.\n",
      "iter: 36.0, RMSE on training set: 1.1621272677945058.\n",
      "iter: 37.0, RMSE on training set: 1.1547852548330817.\n",
      "iter: 38.0, RMSE on training set: 1.1476984669466648.\n",
      "iter: 39.0, RMSE on training set: 1.1408579718572887.\n",
      "iter: 40.0, RMSE on training set: 1.1342557631649268.\n",
      "iter: 41.0, RMSE on training set: 1.127884611763343.\n",
      "iter: 42.0, RMSE on training set: 1.1217379365544646.\n",
      "iter: 43.0, RMSE on training set: 1.1158096921306677.\n",
      "iter: 44.0, RMSE on training set: 1.1100942714302864.\n",
      "iter: 45.0, RMSE on training set: 1.104586421658109.\n",
      "iter: 46.0, RMSE on training set: 1.0992811720069098.\n",
      "iter: 47.0, RMSE on training set: 1.0941737719196567.\n",
      "iter: 48.0, RMSE on training set: 1.089259638793277.\n",
      "iter: 49.0, RMSE on training set: 1.0845343141445087.\n",
      "iter: 50.0, RMSE on training set: 1.0799934273429517.\n",
      "iter: 51.0, RMSE on training set: 1.0756326660787583.\n",
      "iter: 52.0, RMSE on training set: 1.0714477527869812.\n",
      "iter: 53.0, RMSE on training set: 1.0674344263077806.\n",
      "iter: 54.0, RMSE on training set: 1.0635884281245178.\n",
      "iter: 55.0, RMSE on training set: 1.0599054925867661.\n",
      "iter: 56.0, RMSE on training set: 1.0563813405863505.\n",
      "iter: 57.0, RMSE on training set: 1.0530116762075445.\n",
      "iter: 58.0, RMSE on training set: 1.0497921859165238.\n",
      "iter: 59.0, RMSE on training set: 1.0467185398923338.\n",
      "iter: 60.0, RMSE on training set: 1.0437863951351631.\n",
      "iter: 61.0, RMSE on training set: 1.0409914000200193.\n",
      "iter: 62.0, RMSE on training set: 1.0383291999961128.\n",
      "iter: 63.0, RMSE on training set: 1.0357954441637331.\n",
      "iter: 64.0, RMSE on training set: 1.0333857924903562.\n",
      "iter: 65.0, RMSE on training set: 1.0310959234552175.\n",
      "iter: 66.0, RMSE on training set: 1.0289215419365527.\n",
      "iter: 67.0, RMSE on training set: 1.0268583871784795.\n",
      "iter: 68.0, RMSE on training set: 1.0249022406956405.\n",
      "iter: 69.0, RMSE on training set: 1.0230489339937907.\n",
      "iter: 70.0, RMSE on training set: 1.0212943560035428.\n",
      "iter: 71.0, RMSE on training set: 1.0196344601425555.\n",
      "iter: 72.0, RMSE on training set: 1.018065270938286.\n",
      "iter: 73.0, RMSE on training set: 1.0165828901588634.\n",
      "iter: 74.0, RMSE on training set: 1.0151835024135665.\n",
      "iter: 75.0, RMSE on training set: 1.0138633801966996.\n",
      "iter: 76.0, RMSE on training set: 1.012618888359425.\n",
      "iter: 77.0, RMSE on training set: 1.0114464880034395.\n",
      "iter: 78.0, RMSE on training set: 1.0103427397982847.\n",
      "iter: 79.0, RMSE on training set: 1.0093043067308929.\n",
      "iter: 80.0, RMSE on training set: 1.008327956301629.\n",
      "iter: 81.0, RMSE on training set: 1.0074105621858815.\n",
      "iter: 82.0, RMSE on training set: 1.0065491053841922.\n",
      "iter: 83.0, RMSE on training set: 1.0057406748871311.\n",
      "iter: 84.0, RMSE on training set: 1.0049824678835788.\n",
      "iter: 85.0, RMSE on training set: 1.0042717895429858.\n",
      "iter: 86.0, RMSE on training set: 1.003606052403396.\n",
      "iter: 87.0, RMSE on training set: 1.002982775397739.\n",
      "iter: 88.0, RMSE on training set: 1.002399582551178.\n",
      "iter: 89.0, RMSE on training set: 1.0018542013821188.\n",
      "iter: 90.0, RMSE on training set: 1.001344461038995.\n",
      "iter: 91.0, RMSE on training set: 1.0008682902041837.\n",
      "iter: 92.0, RMSE on training set: 1.0004237147953794.\n",
      "iter: 93.0, RMSE on training set: 1.0000088554935656.\n",
      "iter: 94.0, RMSE on training set: 0.9996219251253387.\n",
      "iter: 95.0, RMSE on training set: 0.9992612259258663.\n",
      "iter: 96.0, RMSE on training set: 0.9989251467071208.\n",
      "iter: 97.0, RMSE on training set: 0.9986121599543636.\n",
      "iter: 98.0, RMSE on training set: 0.9983208188720825.\n",
      "iter: 99.0, RMSE on training set: 0.9980497543988002.\n",
      "iter: 100.0, RMSE on training set: 0.997797672208362.\n",
      "iter: 101.0, RMSE on training set: 0.9975633497135329.\n",
      "iter: 102.0, RMSE on training set: 0.9973456330859873.\n",
      "iter: 103.0, RMSE on training set: 0.997143434305116.\n",
      "iter: 104.0, RMSE on training set: 0.9969557282465146.\n",
      "iter: 105.0, RMSE on training set: 0.9967815498195565.\n",
      "iter: 106.0, RMSE on training set: 0.9966199911621482.\n",
      "iter: 107.0, RMSE on training set: 0.9964701988995232.\n",
      "iter: 108.0, RMSE on training set: 0.9963313714728951.\n",
      "iter: 109.0, RMSE on training set: 0.9962027565427415.\n",
      "iter: 110.0, RMSE on training set: 0.9960836484706376.\n",
      "iter: 111.0, RMSE on training set: 0.9959733858826968.\n",
      "iter: 112.0, RMSE on training set: 0.9958713493169451.\n",
      "iter: 113.0, RMSE on training set: 0.9957769589562266.\n",
      "RMSE on test data: 0.9957802410889371.\n",
      "Running lambda_item=0.1\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9957508363287402.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9957440789519799.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9957935872867538.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9958143266901308.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9957753180821785.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9957933709301432.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9958170695075306.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9957735788658221.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9957787754701835.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088340126461.\n",
      "iter: 1.0, RMSE on training set: 2.0599749690987266.\n",
      "iter: 2.0, RMSE on training set: 1.971993299075104.\n",
      "iter: 3.0, RMSE on training set: 1.8944972028187124.\n",
      "iter: 4.0, RMSE on training set: 1.8261011234195366.\n",
      "iter: 5.0, RMSE on training set: 1.765588261614957.\n",
      "iter: 6.0, RMSE on training set: 1.711895028536532.\n",
      "iter: 7.0, RMSE on training set: 1.6640958746094743.\n",
      "iter: 8.0, RMSE on training set: 1.6213886094102474.\n",
      "iter: 9.0, RMSE on training set: 1.583080413852836.\n",
      "iter: 10.0, RMSE on training set: 1.5485747200384385.\n",
      "iter: 11.0, RMSE on training set: 1.5173590923277362.\n",
      "iter: 12.0, RMSE on training set: 1.4889941931058688.\n",
      "iter: 13.0, RMSE on training set: 1.4631038667316265.\n",
      "iter: 14.0, RMSE on training set: 1.439366331337828.\n",
      "iter: 15.0, RMSE on training set: 1.4175064335873255.\n",
      "iter: 16.0, RMSE on training set: 1.397288896972605.\n",
      "iter: 17.0, RMSE on training set: 1.378512479057201.\n",
      "iter: 18.0, RMSE on training set: 1.361004945686421.\n",
      "iter: 19.0, RMSE on training set: 1.3446187688648334.\n",
      "iter: 20.0, RMSE on training set: 1.3292274579872003.\n",
      "iter: 21.0, RMSE on training set: 1.3147224399257884.\n",
      "iter: 22.0, RMSE on training set: 1.3010104109242913.\n",
      "iter: 23.0, RMSE on training set: 1.2880110914277192.\n",
      "iter: 24.0, RMSE on training set: 1.275655323252626.\n",
      "iter: 25.0, RMSE on training set: 1.2638834564522519.\n",
      "iter: 26.0, RMSE on training set: 1.2526439806022416.\n",
      "iter: 27.0, RMSE on training set: 1.2418923618918596.\n",
      "iter: 28.0, RMSE on training set: 1.2315900533055806.\n",
      "iter: 29.0, RMSE on training set: 1.2217036503290357.\n",
      "iter: 30.0, RMSE on training set: 1.2122041690541236.\n",
      "iter: 31.0, RMSE on training set: 1.2030664273516294.\n",
      "iter: 32.0, RMSE on training set: 1.1942685129941553.\n",
      "iter: 33.0, RMSE on training set: 1.1857913253166767.\n",
      "iter: 34.0, RMSE on training set: 1.1776181792626037.\n",
      "iter: 35.0, RMSE on training set: 1.1697344625416606.\n",
      "iter: 36.0, RMSE on training set: 1.1621273381794155.\n",
      "iter: 37.0, RMSE on training set: 1.1547854860188358.\n",
      "iter: 38.0, RMSE on training set: 1.147698877788588.\n",
      "iter: 39.0, RMSE on training set: 1.1408585812212528.\n",
      "iter: 40.0, RMSE on training set: 1.1342565894209118.\n",
      "iter: 41.0, RMSE on training set: 1.1278856722713284.\n",
      "iter: 42.0, RMSE on training set: 1.1217392471650691.\n",
      "iter: 43.0, RMSE on training set: 1.1158112667384297.\n",
      "iter: 44.0, RMSE on training set: 1.1100961216312006.\n",
      "iter: 45.0, RMSE on training set: 1.1045885565662017.\n",
      "iter: 46.0, RMSE on training set: 1.0992835982706597.\n",
      "iter: 47.0, RMSE on training set: 1.0941764939479888.\n",
      "iter: 48.0, RMSE on training set: 1.0892626591610064.\n",
      "iter: 49.0, RMSE on training set: 1.0845376341120714.\n",
      "iter: 50.0, RMSE on training set: 1.0799970474077747.\n",
      "iter: 51.0, RMSE on training set: 1.0756365864809565.\n",
      "iter: 52.0, RMSE on training set: 1.0714519739156556.\n",
      "iter: 53.0, RMSE on training set: 1.0674389489851543.\n",
      "iter: 54.0, RMSE on training set: 1.0635932537716977.\n",
      "iter: 55.0, RMSE on training set: 1.059910623290398.\n",
      "iter: 56.0, RMSE on training set: 1.0563867790895998.\n",
      "iter: 57.0, RMSE on training set: 1.0530174258456682.\n",
      "iter: 58.0, RMSE on training set: 1.0497982505122672.\n",
      "iter: 59.0, RMSE on training set: 1.0467249236228875.\n",
      "iter: 60.0, RMSE on training set: 1.043793102381374.\n",
      "iter: 61.0, RMSE on training set: 1.040998435208906.\n",
      "iter: 62.0, RMSE on training set: 1.0383365674477492.\n",
      "iter: 63.0, RMSE on training set: 1.03580314795219.\n",
      "iter: 64.0, RMSE on training set: 1.0333938363256583.\n",
      "iter: 65.0, RMSE on training set: 1.0311043105899251.\n",
      "iter: 66.0, RMSE on training set: 1.0289302750977356.\n",
      "iter: 67.0, RMSE on training set: 1.0268674685239398.\n",
      "iter: 68.0, RMSE on training set: 1.0249116717925368.\n",
      "iter: 69.0, RMSE on training set: 1.0230587158177478.\n",
      "iter: 70.0, RMSE on training set: 1.0213044889565934.\n",
      "iter: 71.0, RMSE on training set: 1.019644944088318.\n",
      "iter: 72.0, RMSE on training set: 1.0180761052525016.\n",
      "iter: 73.0, RMSE on training set: 1.0165940737928403.\n",
      "iter: 74.0, RMSE on training set: 1.0151950339672466.\n",
      "iter: 75.0, RMSE on training set: 1.013875257997296.\n",
      "iter: 76.0, RMSE on training set: 1.0126311105409966.\n",
      "iter: 77.0, RMSE on training set: 1.0114590525824982.\n",
      "iter: 78.0, RMSE on training set: 1.0103556447407174.\n",
      "iter: 79.0, RMSE on training set: 1.00931755000604.\n",
      "iter: 80.0, RMSE on training set: 1.008341535920268.\n",
      "iter: 81.0, RMSE on training set: 1.0074244762200537.\n",
      "iter: 82.0, RMSE on training set: 1.0065633519680175.\n",
      "iter: 83.0, RMSE on training set: 1.0057552521990571.\n",
      "iter: 84.0, RMSE on training set: 1.0049973741116456.\n",
      "iter: 85.0, RMSE on training set: 1.00428702283567.\n",
      "iter: 86.0, RMSE on training set: 1.0036216108093898.\n",
      "iter: 87.0, RMSE on training set: 1.0029986567986213.\n",
      "iter: 88.0, RMSE on training set: 1.0024157845912451.\n",
      "iter: 89.0, RMSE on training set: 1.0018707213997957.\n",
      "iter: 90.0, RMSE on training set: 1.0013612960041158.\n",
      "iter: 91.0, RMSE on training set: 1.00088543666511.\n",
      "iter: 92.0, RMSE on training set: 1.000441168839361.\n",
      "iter: 93.0, RMSE on training set: 1.000026612723021.\n",
      "iter: 94.0, RMSE on training set: 0.9996399806518508.\n",
      "iter: 95.0, RMSE on training set: 0.9992795743827044.\n",
      "iter: 96.0, RMSE on training set: 0.9989437822801016.\n",
      "iter: 97.0, RMSE on training set: 0.9986310764298733.\n",
      "iter: 98.0, RMSE on training set: 0.9983400097002155.\n",
      "iter: 99.0, RMSE on training set: 0.9980692127688299.\n",
      "iter: 100.0, RMSE on training set: 0.997817391133261.\n",
      "iter: 101.0, RMSE on training set: 0.9975833221199402.\n",
      "iter: 102.0, RMSE on training set: 0.9973658519059937.\n",
      "iter: 103.0, RMSE on training set: 0.99716389256638.\n",
      "iter: 104.0, RMSE on training set: 0.9969764191575864.\n",
      "iter: 105.0, RMSE on training set: 0.9968024668477499.\n",
      "iter: 106.0, RMSE on training set: 0.9966411281018628.\n",
      "iter: 107.0, RMSE on training set: 0.9964915499295011.\n",
      "iter: 108.0, RMSE on training set: 0.9963529312014312.\n",
      "iter: 109.0, RMSE on training set: 0.9962245200403758.\n",
      "iter: 110.0, RMSE on training set: 0.9961056112902784.\n",
      "iter: 111.0, RMSE on training set: 0.9959955440674786.\n",
      "iter: 112.0, RMSE on training set: 0.9958936993963857.\n",
      "iter: 113.0, RMSE on training set: 0.9957994979314926.\n",
      "RMSE on test data: 0.9958026391334988.\n",
      "Running lambda_item=0.5\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9957599413118707.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9957531455306344.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9958026981330512.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9958233931283267.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9957844292130317.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9958025315136666.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9958262241315798.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9957827211425572.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9957877987855278.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600881472529028.\n",
      "iter: 1.0, RMSE on training set: 2.059973125740319.\n",
      "iter: 2.0, RMSE on training set: 1.9719906707533503.\n",
      "iter: 3.0, RMSE on training set: 1.8944940421255967.\n",
      "iter: 4.0, RMSE on training set: 1.8260976667086157.\n",
      "iter: 5.0, RMSE on training set: 1.7655846826177508.\n",
      "iter: 6.0, RMSE on training set: 1.7118914408120713.\n",
      "iter: 7.0, RMSE on training set: 1.6640923456659122.\n",
      "iter: 8.0, RMSE on training set: 1.621385175975551.\n",
      "iter: 9.0, RMSE on training set: 1.5830770944625856.\n",
      "iter: 10.0, RMSE on training set: 1.5485715239310025.\n",
      "iter: 11.0, RMSE on training set: 1.5173560249729887.\n",
      "iter: 12.0, RMSE on training set: 1.4889912591914807.\n",
      "iter: 13.0, RMSE on training set: 1.4631010714702763.\n",
      "iter: 14.0, RMSE on training set: 1.4393636807999894.\n",
      "iter: 15.0, RMSE on training set: 1.4175039345610074.\n",
      "iter: 16.0, RMSE on training set: 1.3972865566725468.\n",
      "iter: 17.0, RMSE on training set: 1.378510304870015.\n",
      "iter: 18.0, RMSE on training set: 1.36100294503989.\n",
      "iter: 19.0, RMSE on training set: 1.344616949239119.\n",
      "iter: 20.0, RMSE on training set: 1.3292258270356947.\n",
      "iter: 21.0, RMSE on training set: 1.3147210056417042.\n",
      "iter: 22.0, RMSE on training set: 1.3010091817763065.\n",
      "iter: 23.0, RMSE on training set: 1.2880100763946438.\n",
      "iter: 24.0, RMSE on training set: 1.275654531710599.\n",
      "iter: 25.0, RMSE on training set: 1.2638828979024963.\n",
      "iter: 26.0, RMSE on training set: 1.2526436642644736.\n",
      "iter: 27.0, RMSE on training set: 1.241892296219887.\n",
      "iter: 28.0, RMSE on training set: 1.2315902455013559.\n",
      "iter: 29.0, RMSE on training set: 1.221704105936686.\n",
      "iter: 30.0, RMSE on training set: 1.2122048917065085.\n",
      "iter: 31.0, RMSE on training set: 1.2030674187209816.\n",
      "iter: 32.0, RMSE on training set: 1.1942697729690297.\n",
      "iter: 33.0, RMSE on training set: 1.1857928523941121.\n",
      "iter: 34.0, RMSE on training set: 1.1776199711123791.\n",
      "iter: 35.0, RMSE on training set: 1.1697365166741212.\n",
      "iter: 36.0, RMSE on training set: 1.1621296526339822.\n",
      "iter: 37.0, RMSE on training set: 1.1547880599896645.\n",
      "iter: 38.0, RMSE on training set: 1.1477017121164308.\n",
      "iter: 39.0, RMSE on training set: 1.1408616787034807.\n",
      "iter: 40.0, RMSE on training set: 1.13425995492001.\n",
      "iter: 41.0, RMSE on training set: 1.127889312630551.\n",
      "iter: 42.0, RMSE on training set: 1.1217431709637793.\n",
      "iter: 43.0, RMSE on training set: 1.1158154839355745.\n",
      "iter: 44.0, RMSE on training set: 1.1101006431519211.\n",
      "iter: 45.0, RMSE on training set: 1.104593393883793.\n",
      "iter: 46.0, RMSE on training set: 1.0992887630255934.\n",
      "iter: 47.0, RMSE on training set: 1.0941819976302924.\n",
      "iter: 48.0, RMSE on training set: 1.0892685128653952.\n",
      "iter: 49.0, RMSE on training set: 1.0845438483603598.\n",
      "iter: 50.0, RMSE on training set: 1.0800036320227857.\n",
      "iter: 51.0, RMSE on training set: 1.0756435504915989.\n",
      "iter: 52.0, RMSE on training set: 1.071459325473569.\n",
      "iter: 53.0, RMSE on training set: 1.0674466952776371.\n",
      "iter: 54.0, RMSE on training set: 1.0636014009214703.\n",
      "iter: 55.0, RMSE on training set: 1.0599191762384597.\n",
      "iter: 56.0, RMSE on training set: 1.0563957414618845.\n",
      "iter: 57.0, RMSE on training set: 1.0530267998074965.\n",
      "iter: 58.0, RMSE on training set: 1.0498080366167137.\n",
      "iter: 59.0, RMSE on training set: 1.0467351206606117.\n",
      "iter: 60.0, RMSE on training set: 1.0438037072403155.\n",
      "iter: 61.0, RMSE on training set: 1.0410094427524443.\n",
      "iter: 62.0, RMSE on training set: 1.0383479704194474.\n",
      "iter: 63.0, RMSE on training set: 1.0358149369140204.\n",
      "iter: 64.0, RMSE on training set: 1.0334059996347482.\n",
      "iter: 65.0, RMSE on training set: 1.0311168344166628.\n",
      "iter: 66.0, RMSE on training set: 1.0289431434856853.\n",
      "iter: 67.0, RMSE on training set: 1.0268806634899783.\n",
      "iter: 68.0, RMSE on training set: 1.0249251734638503.\n",
      "iter: 69.0, RMSE on training set: 1.023072502601192.\n",
      "iter: 70.0, RMSE on training set: 1.0213185377352276.\n",
      "iter: 71.0, RMSE on training set: 1.0196592304396694.\n",
      "iter: 72.0, RMSE on training set: 1.018090603683154.\n",
      "iter: 73.0, RMSE on training set: 1.0166087579841503.\n",
      "iter: 74.0, RMSE on training set: 1.0152098770272773.\n",
      "iter: 75.0, RMSE on training set: 1.0138902327143717.\n",
      "iter: 76.0, RMSE on training set: 1.0126461896345635.\n",
      "iter: 77.0, RMSE on training set: 1.0114742089473194.\n",
      "iter: 78.0, RMSE on training set: 1.0103708516806857.\n",
      "iter: 79.0, RMSE on training set: 1.0093327814542576.\n",
      "iter: 80.0, RMSE on training set: 1.00835676664235.\n",
      "iter: 81.0, RMSE on training set: 1.007439681997946.\n",
      "iter: 82.0, RMSE on training set: 1.0065785097620377.\n",
      "iter: 83.0, RMSE on training set: 1.0057703402861868.\n",
      "iter: 84.0, RMSE on training set: 1.005012372198549.\n",
      "iter: 85.0, RMSE on training set: 1.0043019121453312.\n",
      "iter: 86.0, RMSE on training set: 1.0036363741407137.\n",
      "iter: 87.0, RMSE on training set: 1.0030132785588435.\n",
      "iter: 88.0, RMSE on training set: 1.002430250801538.\n",
      "iter: 89.0, RMSE on training set: 1.001885019674981.\n",
      "iter: 90.0, RMSE on training set: 1.0013754155079955.\n",
      "iter: 91.0, RMSE on training set: 1.0008993680434781.\n",
      "iter: 92.0, RMSE on training set: 1.000454904133297.\n",
      "iter: 93.0, RMSE on training set: 1.0000401452655943.\n",
      "iter: 94.0, RMSE on training set: 0.9996533049517532.\n",
      "iter: 95.0, RMSE on training set: 0.9992926859987014.\n",
      "iter: 96.0, RMSE on training set: 0.9989566776903844.\n",
      "iter: 97.0, RMSE on training set: 0.9986437529004845.\n",
      "iter: 98.0, RMSE on training set: 0.9983524651566628.\n",
      "iter: 99.0, RMSE on training set: 0.9980814456747874.\n",
      "iter: 100.0, RMSE on training set: 0.9978294003799021.\n",
      "iter: 101.0, RMSE on training set: 0.997595106928961.\n",
      "iter: 102.0, RMSE on training set: 0.9973774117487759.\n",
      "iter: 103.0, RMSE on training set: 0.9971752271010638.\n",
      "iter: 104.0, RMSE on training set: 0.9969875281850478.\n",
      "iter: 105.0, RMSE on training set: 0.9968133502867422.\n",
      "iter: 106.0, RMSE on training set: 0.9966517859827786.\n",
      "iter: 107.0, RMSE on training set: 0.9965019824054995.\n",
      "iter: 108.0, RMSE on training set: 0.9963631385750434.\n",
      "iter: 109.0, RMSE on training set: 0.996234502803131.\n",
      "iter: 110.0, RMSE on training set: 0.9961153701724862.\n",
      "iter: 111.0, RMSE on training set: 0.9960050800949859.\n",
      "iter: 112.0, RMSE on training set: 0.9959030139510333.\n",
      "iter: 113.0, RMSE on training set: 0.9958085928119369.\n",
      "RMSE on test data: 0.9958116551782037.\n",
      "Running lambda_item=1\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9957605828019688.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9957538038457093.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9958033510485435.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9958240376977608.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9957850678135177.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9958032008812142.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9958268738079341.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9957833842496725.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9957884189199748.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.160088551526519.\n",
      "iter: 1.0, RMSE on training set: 2.059971749456932.\n",
      "iter: 2.0, RMSE on training set: 1.971988776903825.\n",
      "iter: 3.0, RMSE on training set: 1.8944920018656928.\n",
      "iter: 4.0, RMSE on training set: 1.8260957446652093.\n",
      "iter: 5.0, RMSE on training set: 1.765583023301847.\n",
      "iter: 6.0, RMSE on training set: 1.7118901025083126.\n",
      "iter: 7.0, RMSE on training set: 1.664091335586583.\n",
      "iter: 8.0, RMSE on training set: 1.6213844768284171.\n",
      "iter: 9.0, RMSE on training set: 1.5830766813917385.\n",
      "iter: 10.0, RMSE on training set: 1.5485713736718023.\n",
      "iter: 11.0, RMSE on training set: 1.5173561197277479.\n",
      "iter: 12.0, RMSE on training set: 1.4889915874319692.\n",
      "iter: 13.0, RMSE on training set: 1.4631016272546342.\n",
      "iter: 14.0, RMSE on training set: 1.4393644625848019.\n",
      "iter: 15.0, RMSE on training set: 1.417504944019839.\n",
      "iter: 16.0, RMSE on training set: 1.397287797718233.\n",
      "iter: 17.0, RMSE on training set: 1.3785117828947648.\n",
      "iter: 18.0, RMSE on training set: 1.3610046663029303.\n",
      "iter: 19.0, RMSE on training set: 1.3446189203170635.\n",
      "iter: 20.0, RMSE on training set: 1.3292280542804804.\n",
      "iter: 21.0, RMSE on training set: 1.3147234946401023.\n",
      "iter: 22.0, RMSE on training set: 1.3010119368513788.\n",
      "iter: 23.0, RMSE on training set: 1.2880131002164559.\n",
      "iter: 24.0, RMSE on training set: 1.275657825083782.\n",
      "iter: 25.0, RMSE on training set: 1.2638864597748378.\n",
      "iter: 26.0, RMSE on training set: 1.2526474919639556.\n",
      "iter: 27.0, RMSE on training set: 1.241896385890067.\n",
      "iter: 28.0, RMSE on training set: 1.2315945926766108.\n",
      "iter: 29.0, RMSE on training set: 1.2217087061843128.\n",
      "iter: 30.0, RMSE on training set: 1.2122097412613722.\n",
      "iter: 31.0, RMSE on training set: 1.2030725150469226.\n",
      "iter: 32.0, RMSE on training set: 1.1942751151953195.\n",
      "iter: 33.0, RMSE on training set: 1.185798441592.\n",
      "iter: 34.0, RMSE on training set: 1.1776258103944877.\n",
      "iter: 35.0, RMSE on training set: 1.1697426111172537.\n",
      "iter: 36.0, RMSE on training set: 1.162136009042912.\n",
      "iter: 37.0, RMSE on training set: 1.1547946865339307.\n",
      "iter: 38.0, RMSE on training set: 1.1477086178823108.\n",
      "iter: 39.0, RMSE on training set: 1.1408688732074863.\n",
      "iter: 40.0, RMSE on training set: 1.1342674476280141.\n",
      "iter: 41.0, RMSE on training set: 1.1278971125189592.\n",
      "iter: 42.0, RMSE on training set: 1.1217512861478391.\n",
      "iter: 43.0, RMSE on training set: 1.115823921377472.\n",
      "iter: 44.0, RMSE on training set: 1.1101094084498024.\n",
      "iter: 45.0, RMSE on training set: 1.104602491133679.\n",
      "iter: 46.0, RMSE on training set: 1.0992981947420517.\n",
      "iter: 47.0, RMSE on training set: 1.0941917647085893.\n",
      "iter: 48.0, RMSE on training set: 1.089278614567404.\n",
      "iter: 49.0, RMSE on training set: 1.0845542823082235.\n",
      "iter: 50.0, RMSE on training set: 1.0800143941877305.\n",
      "iter: 51.0, RMSE on training set: 1.0756546351698828.\n",
      "iter: 52.0, RMSE on training set: 1.0714707252467957.\n",
      "iter: 53.0, RMSE on training set: 1.0674584009599326.\n",
      "iter: 54.0, RMSE on training set: 1.0636134015006848.\n",
      "iter: 55.0, RMSE on training set: 1.0599314588219206.\n",
      "iter: 56.0, RMSE on training set: 1.0564082912388817.\n",
      "iter: 57.0, RMSE on training set: 1.0530396000404063.\n",
      "iter: 58.0, RMSE on training set: 1.0498210686706462.\n",
      "iter: 59.0, RMSE on training set: 1.0467483640781277.\n",
      "iter: 60.0, RMSE on training set: 1.0438171398637603.\n",
      "iter: 61.0, RMSE on training set: 1.041023040892439.\n",
      "iter: 62.0, RMSE on training set: 1.0383617090646013.\n",
      "iter: 63.0, RMSE on training set: 1.0358287899743532.\n",
      "iter: 64.0, RMSE on training set: 1.0334199402097006.\n",
      "iter: 65.0, RMSE on training set: 1.031130835077894.\n",
      "iter: 66.0, RMSE on training set: 1.0289571765649277.\n",
      "iter: 67.0, RMSE on training set: 1.0268947013627394.\n",
      "iter: 68.0, RMSE on training set: 1.0249391888206907.\n",
      "iter: 69.0, RMSE on training set: 1.023086468699342.\n",
      "iter: 70.0, RMSE on training set: 1.0213324286244885.\n",
      "iter: 71.0, RMSE on training set: 1.0196730211577862.\n",
      "iter: 72.0, RMSE on training set: 1.0181042704171037.\n",
      "iter: 73.0, RMSE on training set: 1.0166222781950731.\n",
      "iter: 74.0, RMSE on training set: 1.0152232295380368.\n",
      "iter: 75.0, RMSE on training set: 1.013903397759972.\n",
      "iter: 76.0, RMSE on training set: 1.0126591488768106.\n",
      "iter: 77.0, RMSE on training set: 1.0114869454561848.\n",
      "iter: 78.0, RMSE on training set: 1.0103833498858084.\n",
      "iter: 79.0, RMSE on training set: 1.0093450270708118.\n",
      "iter: 80.0, RMSE on training set: 1.0083687465762194.\n",
      "iter: 81.0, RMSE on training set: 1.007451384235636.\n",
      "iter: 82.0, RMSE on training set: 1.0065899232510862.\n",
      "iter: 83.0, RMSE on training set: 1.005781454812016.\n",
      "iter: 84.0, RMSE on training set: 1.005023178263636.\n",
      "iter: 85.0, RMSE on training set: 1.0043124008563566.\n",
      "iter: 86.0, RMSE on training set: 1.0036465371089318.\n",
      "iter: 87.0, RMSE on training set: 1.003023107818317.\n",
      "iter: 88.0, RMSE on training set: 1.0024397387490969.\n",
      "iter: 89.0, RMSE on training set: 1.0018941590348678.\n",
      "iter: 90.0, RMSE on training set: 1.0013841993231138.\n",
      "iter: 91.0, RMSE on training set: 1.0009077896940524.\n",
      "iter: 92.0, RMSE on training set: 1.0004629573825885.\n",
      "iter: 93.0, RMSE on training set: 1.0000478243311541.\n",
      "iter: 94.0, RMSE on training set: 0.999660604599606.\n",
      "iter: 95.0, RMSE on training set: 0.999299601656803.\n",
      "iter: 96.0, RMSE on training set: 0.9989632055768416.\n",
      "iter: 97.0, RMSE on training set: 0.9986498901613157.\n",
      "iter: 98.0, RMSE on training set: 0.9983582100073727.\n",
      "iter: 99.0, RMSE on training set: 0.9980867975397759.\n",
      "iter: 100.0, RMSE on training set: 0.9978343600236873.\n",
      "iter: 101.0, RMSE on training set: 0.9975996765734297.\n",
      "iter: 102.0, RMSE on training set: 0.9973815951711277.\n",
      "iter: 103.0, RMSE on training set: 0.9971790297077769.\n",
      "iter: 104.0, RMSE on training set: 0.9969909570580673.\n",
      "iter: 105.0, RMSE on training set: 0.9968164141990828.\n",
      "iter: 106.0, RMSE on training set: 0.9966544953818517.\n",
      "iter: 107.0, RMSE on training set: 0.9965043493636457.\n",
      "iter: 108.0, RMSE on training set: 0.9963651767079136.\n",
      "iter: 109.0, RMSE on training set: 0.9962362271576971.\n",
      "iter: 110.0, RMSE on training set: 0.996116797087494.\n",
      "iter: 111.0, RMSE on training set: 0.9960062270375778.\n",
      "iter: 112.0, RMSE on training set: 0.995903899333966.\n",
      "iter: 113.0, RMSE on training set: 0.9958092357963838.\n",
      "RMSE on test data: 0.9958122912303761.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEVCAYAAABJ81qhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1119e6160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 5   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item_arr = [0.01, 0.1, 0.5, 1]\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "\n",
    "for i, lambda_item in enumerate(lambda_item_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_item={n}'.format(n=lambda_item))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings,  num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_item_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_item_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma_arr = [0.01, 0.1, 1]\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.5\n",
    "\n",
    "train_rmse_mean = np.zeros(len(gamma_arr))\n",
    "train_rmse_std = np.zeros(len(gamma_arr))\n",
    "validation_rmse_mean = np.zeros(len(gamma_arr))\n",
    "validation_rmse_std = np.zeros(len(gamma_arr))\n",
    "\n",
    "for i, gamma in enumerate(gamma_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running gamma={n}'.format(n=gamma))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(gamma_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(gamma_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(gamma_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(gamma_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "#### 1. Compare SGD, ALS with the best set of parameters (based on above results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
