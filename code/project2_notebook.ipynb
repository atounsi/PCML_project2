{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from helpers import *\n",
    "from plots import *\n",
    "from plots import *\n",
    "from split_data import *\n",
    "from recommender import *\n",
    "from cross_validation import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\u001c",
    "\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "path_dataset = \"../data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnz_r,nnz_c = ratings.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nz = list(zip(nnz_r, nnz_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "under =0\n",
    "up = 0\n",
    "for d, n in nz:\n",
    "    if ratings[d,n] > 5:\n",
    "        up = up+1\n",
    "    if ratings[d,n] < 1:\n",
    "        under = under +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncs45l1n/HyRRptq0/AuwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103e4fcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_ratings, train_validation, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "#plot_train_test_data(train_validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Matrix factorisation using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run run.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run run.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods\n",
    "### CCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "number of items: 10000, number of users: 1000\n",
      "Preprocessing data\n",
      "Splitting data into train and test sets\n",
      "Training model\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.093432775120109.\n",
      "iter: 1.0, RMSE on training set: 1.0101849106969572.\n",
      "iter: 2.0, RMSE on training set: 0.9913295912081788.\n",
      "iter: 3.0, RMSE on training set: 0.9869897338042543.\n",
      "iter: 4.0, RMSE on training set: 0.9859527104933422.\n",
      "iter: 5.0, RMSE on training set: 0.9856933115214811.\n",
      "iter: 6.0, RMSE on training set: 0.985624653358203.\n",
      "RMSE on test data: 1.0058753685813262.\n",
      "RMSE on train data: 0.985624653358203.\n",
      "RMSE on test data: 1.0058753685813262.\n"
     ]
    }
   ],
   "source": [
    "%run run.py 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "number of items: 10000, number of users: 1000\n",
      "Preprocessing data\n",
      "Training model\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903381028555411.\n",
      "iter: 1.0, RMSE on training set: 0.9903381028555411.\n",
      "RMSE on test data: 1.0006146604733976.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902355659276648.\n",
      "iter: 1.0, RMSE on training set: 0.9902355659276648.\n",
      "RMSE on test data: 1.001516435731386.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990304070453025.\n",
      "iter: 1.0, RMSE on training set: 0.990304070453025.\n",
      "RMSE on test data: 1.0008886985480645.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904600070297657.\n",
      "iter: 1.0, RMSE on training set: 0.9904600070297657.\n",
      "RMSE on test data: 0.9996070655212942.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901286780738563.\n",
      "iter: 1.0, RMSE on training set: 0.9901286780738563.\n",
      "RMSE on test data: 1.0025492130242937.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9899779777887293.\n",
      "iter: 1.0, RMSE on training set: 0.9899779777887293.\n",
      "RMSE on test data: 1.0038028049885088.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.991086526513082.\n",
      "iter: 1.0, RMSE on training set: 0.991086526513082.\n",
      "RMSE on test data: 0.9938829321600344.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990186368084433.\n",
      "iter: 1.0, RMSE on training set: 0.990186368084433.\n",
      "RMSE on test data: 1.0019244944948391.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901099003300312.\n",
      "iter: 1.0, RMSE on training set: 0.9901099003300312.\n",
      "RMSE on test data: 1.0025467760373596.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.99079203642962.\n",
      "iter: 1.0, RMSE on training set: 0.99079203642962.\n",
      "RMSE on test data: 0.9965466102987659.\n",
      "RMSE on train data: 0.991086526513082.\n",
      "RMSE on test data: 0.9938829321600344.\n",
      "Loading test data\n",
      "number of items: 10000, number of users: 1000\n",
      "Generate predictions\n",
      "Creating submission file\n"
     ]
    }
   ],
   "source": [
    "%run run.py 3 --submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running num_features=1\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904900809435744.\n",
      "iter: 1.0, RMSE on training set: 0.9904900809435744.\n",
      "RMSE on test data: 1.0006430527407613.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903873201539922.\n",
      "iter: 1.0, RMSE on training set: 0.9903873201539922.\n",
      "RMSE on test data: 1.0015265151970847.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904558225938457.\n",
      "iter: 1.0, RMSE on training set: 0.9904558225938457.\n",
      "RMSE on test data: 1.0009699185394834.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906118679454422.\n",
      "iter: 1.0, RMSE on training set: 0.9906118679454422.\n",
      "RMSE on test data: 0.9997708187658811.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902800428221178.\n",
      "iter: 1.0, RMSE on training set: 0.9902800428221178.\n",
      "RMSE on test data: 1.0026917472323662.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901292704030457.\n",
      "iter: 1.0, RMSE on training set: 0.9901292704030457.\n",
      "RMSE on test data: 1.0038381445272881.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9912379318005341.\n",
      "iter: 1.0, RMSE on training set: 0.9912379318005341.\n",
      "RMSE on test data: 0.9939965569926406.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903381115430898.\n",
      "iter: 1.0, RMSE on training set: 0.9903381115430898.\n",
      "RMSE on test data: 1.0020360383559628.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902613842784617.\n",
      "iter: 1.0, RMSE on training set: 0.9902613842784617.\n",
      "RMSE on test data: 1.0026886060959928.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909437597253823.\n",
      "iter: 1.0, RMSE on training set: 0.9909437597253823.\n",
      "RMSE on test data: 0.9967189551934174.\n",
      "Running num_features=4\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0814501140566293.\n",
      "iter: 1.0, RMSE on training set: 1.0344345005360815.\n",
      "iter: 2.0, RMSE on training set: 1.0105935712175822.\n",
      "iter: 3.0, RMSE on training set: 0.996510196583157.\n",
      "iter: 4.0, RMSE on training set: 0.9882692181030674.\n",
      "iter: 5.0, RMSE on training set: 0.9834888269458077.\n",
      "iter: 6.0, RMSE on training set: 0.9807357737351757.\n",
      "iter: 7.0, RMSE on training set: 0.979161580726277.\n",
      "iter: 8.0, RMSE on training set: 0.978268860098014.\n",
      "iter: 9.0, RMSE on training set: 0.9777679085990005.\n",
      "iter: 10.0, RMSE on training set: 0.9774907926713219.\n",
      "iter: 11.0, RMSE on training set: 0.9773405753058031.\n",
      "iter: 12.0, RMSE on training set: 0.9772615497836432.\n",
      "RMSE on test data: 1.0164540357940843.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0813979050731815.\n",
      "iter: 1.0, RMSE on training set: 1.0343417951812084.\n",
      "iter: 2.0, RMSE on training set: 1.0104957279503448.\n",
      "iter: 3.0, RMSE on training set: 0.9964181610879732.\n",
      "iter: 4.0, RMSE on training set: 0.9881855717121646.\n",
      "iter: 5.0, RMSE on training set: 0.9834127640129435.\n",
      "iter: 6.0, RMSE on training set: 0.9806654929513352.\n",
      "iter: 7.0, RMSE on training set: 0.9790952977430292.\n",
      "iter: 8.0, RMSE on training set: 0.9782051481704795.\n",
      "iter: 9.0, RMSE on training set: 0.977705739300102.\n",
      "iter: 10.0, RMSE on training set: 0.977429472820833.\n",
      "iter: 11.0, RMSE on training set: 0.9772796630271965.\n",
      "iter: 12.0, RMSE on training set: 0.9772007795499793.\n",
      "RMSE on test data: 1.0172159689810225.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0815759252850092.\n",
      "iter: 1.0, RMSE on training set: 1.0344014173728147.\n",
      "iter: 2.0, RMSE on training set: 1.010533168773614.\n",
      "iter: 3.0, RMSE on training set: 0.9964437400582316.\n",
      "iter: 4.0, RMSE on training set: 0.9882062260008204.\n",
      "iter: 5.0, RMSE on training set: 0.9834317423329042.\n",
      "iter: 6.0, RMSE on training set: 0.9806841236014999.\n",
      "iter: 7.0, RMSE on training set: 0.9791140472865273.\n",
      "iter: 8.0, RMSE on training set: 0.9782241388138003.\n",
      "iter: 9.0, RMSE on training set: 0.9777249764042066.\n",
      "iter: 10.0, RMSE on training set: 0.9774489333602291.\n",
      "iter: 11.0, RMSE on training set: 0.9772993238455844.\n",
      "iter: 12.0, RMSE on training set: 0.9772206227760072.\n",
      "RMSE on test data: 1.0173175041653408.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0814489216877976.\n",
      "iter: 1.0, RMSE on training set: 1.0344120409991335.\n",
      "iter: 2.0, RMSE on training set: 1.0105933439039512.\n",
      "iter: 3.0, RMSE on training set: 0.9965302318571084.\n",
      "iter: 4.0, RMSE on training set: 0.9883039861013969.\n",
      "iter: 5.0, RMSE on training set: 0.9835334795978667.\n",
      "iter: 6.0, RMSE on training set: 0.9807866252850457.\n",
      "iter: 7.0, RMSE on training set: 0.979216049511375.\n",
      "iter: 8.0, RMSE on training set: 0.9783252607292932.\n",
      "iter: 9.0, RMSE on training set: 0.9778252035033427.\n",
      "iter: 10.0, RMSE on training set: 0.9775483788159587.\n",
      "iter: 11.0, RMSE on training set: 0.9773981261143455.\n",
      "iter: 12.0, RMSE on training set: 0.9773189070210544.\n",
      "RMSE on test data: 1.0163047114241224.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.081423246810819.\n",
      "iter: 1.0, RMSE on training set: 1.0343257753158046.\n",
      "iter: 2.0, RMSE on training set: 1.0104086697512502.\n",
      "iter: 3.0, RMSE on training set: 0.996268779268365.\n",
      "iter: 4.0, RMSE on training set: 0.9879928234771662.\n",
      "iter: 5.0, RMSE on training set: 0.983191293304531.\n",
      "iter: 6.0, RMSE on training set: 0.9804255041674159.\n",
      "iter: 7.0, RMSE on training set: 0.9788435850037633.\n",
      "iter: 8.0, RMSE on training set: 0.9779461087826551.\n",
      "iter: 9.0, RMSE on training set: 0.9774421596781897.\n",
      "iter: 10.0, RMSE on training set: 0.9771630874763256.\n",
      "iter: 11.0, RMSE on training set: 0.977011533727407.\n",
      "iter: 12.0, RMSE on training set: 0.9769315451578398.\n",
      "RMSE on test data: 1.0203179743486646.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0810489208057974.\n",
      "iter: 1.0, RMSE on training set: 1.0340913733074588.\n",
      "iter: 2.0, RMSE on training set: 1.0102200284898002.\n",
      "iter: 3.0, RMSE on training set: 0.9961250387651902.\n",
      "iter: 4.0, RMSE on training set: 0.987884844388805.\n",
      "iter: 5.0, RMSE on training set: 0.9831097428058712.\n",
      "iter: 6.0, RMSE on training set: 0.9803625470481692.\n",
      "iter: 7.0, RMSE on training set: 0.9787932473063715.\n",
      "iter: 8.0, RMSE on training set: 0.9779041067940337.\n",
      "iter: 9.0, RMSE on training set: 0.9774055487486646.\n",
      "iter: 10.0, RMSE on training set: 0.9771299064982866.\n",
      "iter: 11.0, RMSE on training set: 0.976980509765457.\n",
      "iter: 12.0, RMSE on training set: 0.9769018702177694.\n",
      "RMSE on test data: 1.0199687479421526.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0818589389941002.\n",
      "iter: 1.0, RMSE on training set: 1.0349245100054985.\n",
      "iter: 2.0, RMSE on training set: 1.01115626912648.\n",
      "iter: 3.0, RMSE on training set: 0.9971270535379652.\n",
      "iter: 4.0, RMSE on training set: 0.9889214342500884.\n",
      "iter: 5.0, RMSE on training set: 0.9841633379247664.\n",
      "iter: 6.0, RMSE on training set: 0.9814239841693978.\n",
      "iter: 7.0, RMSE on training set: 0.9798580085363018.\n",
      "iter: 8.0, RMSE on training set: 0.9789701001598828.\n",
      "iter: 9.0, RMSE on training set: 0.978471889454757.\n",
      "iter: 10.0, RMSE on training set: 0.978196276137488.\n",
      "iter: 11.0, RMSE on training set: 0.9780468335306545.\n",
      "iter: 12.0, RMSE on training set: 0.977968162518082.\n",
      "RMSE on test data: 1.010390672701321.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.081110037295384.\n",
      "iter: 1.0, RMSE on training set: 1.0341527170417613.\n",
      "iter: 2.0, RMSE on training set: 1.0103471313981263.\n",
      "iter: 3.0, RMSE on training set: 0.9962924685791491.\n",
      "iter: 4.0, RMSE on training set: 0.9880707659013361.\n",
      "iter: 5.0, RMSE on training set: 0.9833030049238417.\n",
      "iter: 6.0, RMSE on training set: 0.9805582171564046.\n",
      "iter: 7.0, RMSE on training set: 0.9789894010888708.\n",
      "iter: 8.0, RMSE on training set: 0.9781001433302022.\n",
      "iter: 9.0, RMSE on training set: 0.9776013865809109.\n",
      "iter: 10.0, RMSE on training set: 0.9773256279273116.\n",
      "iter: 11.0, RMSE on training set: 0.9771762187766909.\n",
      "iter: 12.0, RMSE on training set: 0.9770976457413824.\n",
      "RMSE on test data: 1.0180005144471906.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.081233483694028.\n",
      "iter: 1.0, RMSE on training set: 1.0342077500322928.\n",
      "iter: 2.0, RMSE on training set: 1.0103293187674125.\n",
      "iter: 3.0, RMSE on training set: 0.9962213361542696.\n",
      "iter: 4.0, RMSE on training set: 0.987965905966434.\n",
      "iter: 5.0, RMSE on training set: 0.9831771726847358.\n",
      "iter: 6.0, RMSE on training set: 0.9804191487442214.\n",
      "iter: 7.0, RMSE on training set: 0.9788418348213841.\n",
      "iter: 8.0, RMSE on training set: 0.9779470307876081.\n",
      "iter: 9.0, RMSE on training set: 0.9774446001601621.\n",
      "iter: 10.0, RMSE on training set: 0.9771663754245096.\n",
      "iter: 11.0, RMSE on training set: 0.9770152906125159.\n",
      "iter: 12.0, RMSE on training set: 0.9769355658246466.\n",
      "RMSE on test data: 1.0193194864639021.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0815520381757302.\n",
      "iter: 1.0, RMSE on training set: 1.0346587920588926.\n",
      "iter: 2.0, RMSE on training set: 1.0108835258938094.\n",
      "iter: 3.0, RMSE on training set: 0.9968443576255559.\n",
      "iter: 4.0, RMSE on training set: 0.9886327436200465.\n",
      "iter: 5.0, RMSE on training set: 0.9838719073609937.\n",
      "iter: 6.0, RMSE on training set: 0.9811317961546185.\n",
      "iter: 7.0, RMSE on training set: 0.9795660416293223.\n",
      "iter: 8.0, RMSE on training set: 0.9786787062860123.\n",
      "iter: 9.0, RMSE on training set: 0.9781810853881299.\n",
      "iter: 10.0, RMSE on training set: 0.9779059301098207.\n",
      "iter: 11.0, RMSE on training set: 0.9777567707775191.\n",
      "iter: 12.0, RMSE on training set: 0.9776782178817663.\n",
      "RMSE on test data: 1.0132167521365243.\n",
      "Running num_features=7\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0791763318783265.\n",
      "iter: 1.0, RMSE on training set: 1.0129861934727962.\n",
      "iter: 2.0, RMSE on training set: 0.9991262210636557.\n",
      "iter: 3.0, RMSE on training set: 0.9895408724235086.\n",
      "iter: 4.0, RMSE on training set: 0.9824688430862772.\n",
      "iter: 5.0, RMSE on training set: 0.9772576664221989.\n",
      "iter: 6.0, RMSE on training set: 0.9734369125618242.\n",
      "iter: 7.0, RMSE on training set: 0.9706510748999836.\n",
      "iter: 8.0, RMSE on training set: 0.9686318735069923.\n",
      "iter: 9.0, RMSE on training set: 0.9671779678943683.\n",
      "iter: 10.0, RMSE on training set: 0.9661390225942886.\n",
      "iter: 11.0, RMSE on training set: 0.9654032821045551.\n",
      "iter: 12.0, RMSE on training set: 0.9648879964522901.\n",
      "iter: 13.0, RMSE on training set: 0.964532116632812.\n",
      "iter: 14.0, RMSE on training set: 0.9642907642859238.\n",
      "iter: 15.0, RMSE on training set: 0.9641310692413348.\n",
      "iter: 16.0, RMSE on training set: 0.9640290510136218.\n",
      "iter: 17.0, RMSE on training set: 0.963967291069516.\n",
      "RMSE on test data: 1.0359264426328314.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.079513656867835.\n",
      "iter: 1.0, RMSE on training set: 1.01306420201027.\n",
      "iter: 2.0, RMSE on training set: 0.9991922155769215.\n",
      "iter: 3.0, RMSE on training set: 0.9895977436959993.\n",
      "iter: 4.0, RMSE on training set: 0.9825172602823552.\n",
      "iter: 5.0, RMSE on training set: 0.9772999723305539.\n",
      "iter: 6.0, RMSE on training set: 0.973474885439146.\n",
      "iter: 7.0, RMSE on training set: 0.9706858099730714.\n",
      "iter: 8.0, RMSE on training set: 0.9686640259985566.\n",
      "iter: 9.0, RMSE on training set: 0.9672079339409809.\n",
      "iter: 10.0, RMSE on training set: 0.966167051179404.\n",
      "iter: 11.0, RMSE on training set: 0.9654295423743479.\n",
      "iter: 12.0, RMSE on training set: 0.964912617162153.\n",
      "iter: 13.0, RMSE on training set: 0.9645552081662953.\n",
      "iter: 14.0, RMSE on training set: 0.9643124300352752.\n",
      "iter: 15.0, RMSE on training set: 0.9641514107338744.\n",
      "iter: 16.0, RMSE on training set: 0.9640481695517965.\n",
      "iter: 17.0, RMSE on training set: 0.9639852875803905.\n",
      "RMSE on test data: 1.0358485273315303.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0793680796742267.\n",
      "iter: 1.0, RMSE on training set: 1.0128658883670683.\n",
      "iter: 2.0, RMSE on training set: 0.9989946784490353.\n",
      "iter: 3.0, RMSE on training set: 0.9894220145767479.\n",
      "iter: 4.0, RMSE on training set: 0.9823630543486356.\n",
      "iter: 5.0, RMSE on training set: 0.9771634260143388.\n",
      "iter: 6.0, RMSE on training set: 0.97335238248865.\n",
      "iter: 7.0, RMSE on training set: 0.970574393073812.\n",
      "iter: 8.0, RMSE on training set: 0.9685613381998297.\n",
      "iter: 9.0, RMSE on training set: 0.9671121235225891.\n",
      "iter: 10.0, RMSE on training set: 0.966076676134755.\n",
      "iter: 11.0, RMSE on training set: 0.9653434852926263.\n",
      "iter: 12.0, RMSE on training set: 0.9648300139797447.\n",
      "iter: 13.0, RMSE on training set: 0.9644753906431826.\n",
      "iter: 14.0, RMSE on training set: 0.9642348802576658.\n",
      "iter: 15.0, RMSE on training set: 0.9640757256079396.\n",
      "iter: 16.0, RMSE on training set: 0.9639740333708761.\n",
      "iter: 17.0, RMSE on training set: 0.9639124510081812.\n",
      "RMSE on test data: 1.0370862270288057.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0791117334107714.\n",
      "iter: 1.0, RMSE on training set: 1.0129787614786903.\n",
      "iter: 2.0, RMSE on training set: 0.9991610388306142.\n",
      "iter: 3.0, RMSE on training set: 0.9895952357810855.\n",
      "iter: 4.0, RMSE on training set: 0.9825333094644917.\n",
      "iter: 5.0, RMSE on training set: 0.9773271402015148.\n",
      "iter: 6.0, RMSE on training set: 0.9735082994408221.\n",
      "iter: 7.0, RMSE on training set: 0.970722467505037.\n",
      "iter: 8.0, RMSE on training set: 0.9687021405258928.\n",
      "iter: 9.0, RMSE on training set: 0.9672464903044576.\n",
      "iter: 10.0, RMSE on training set: 0.9662055146593009.\n",
      "iter: 11.0, RMSE on training set: 0.9654676699584887.\n",
      "iter: 12.0, RMSE on training set: 0.9649503361026947.\n",
      "iter: 13.0, RMSE on training set: 0.9645925390285801.\n",
      "iter: 14.0, RMSE on training set: 0.9643494390268774.\n",
      "iter: 15.0, RMSE on training set: 0.9641881811059024.\n",
      "iter: 16.0, RMSE on training set: 0.9640847852455858.\n",
      "iter: 17.0, RMSE on training set: 0.9640218245928823.\n",
      "RMSE on test data: 1.0376501143010035.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0791127388925068.\n",
      "iter: 1.0, RMSE on training set: 1.0126956604330348.\n",
      "iter: 2.0, RMSE on training set: 0.9988096495150344.\n",
      "iter: 3.0, RMSE on training set: 0.989215773649846.\n",
      "iter: 4.0, RMSE on training set: 0.9821368564503856.\n",
      "iter: 5.0, RMSE on training set: 0.976919045681308.\n",
      "iter: 6.0, RMSE on training set: 0.9730920706869095.\n",
      "iter: 7.0, RMSE on training set: 0.9703005639561582.\n",
      "iter: 8.0, RMSE on training set: 0.9682762874341703.\n",
      "iter: 9.0, RMSE on training set: 0.9668178802763017.\n",
      "iter: 10.0, RMSE on training set: 0.965774958826997.\n",
      "iter: 11.0, RMSE on training set: 0.9650357108655397.\n",
      "iter: 12.0, RMSE on training set: 0.9645173303460686.\n",
      "iter: 13.0, RMSE on training set: 0.9641587166350473.\n",
      "iter: 14.0, RMSE on training set: 0.9639149454303101.\n",
      "iter: 15.0, RMSE on training set: 0.9637531063875409.\n",
      "iter: 16.0, RMSE on training set: 0.9636491842025622.\n",
      "iter: 17.0, RMSE on training set: 0.9635857302899176.\n",
      "RMSE on test data: 1.0409560509119544.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0789770983014988.\n",
      "iter: 1.0, RMSE on training set: 1.0126668323531618.\n",
      "iter: 2.0, RMSE on training set: 0.9988217700157391.\n",
      "iter: 3.0, RMSE on training set: 0.9892496055377626.\n",
      "iter: 4.0, RMSE on training set: 0.9821866494572685.\n",
      "iter: 5.0, RMSE on training set: 0.9769824851497781.\n",
      "iter: 6.0, RMSE on training set: 0.9731674734487484.\n",
      "iter: 7.0, RMSE on training set: 0.9703862173009732.\n",
      "iter: 8.0, RMSE on training set: 0.9683705096080569.\n",
      "iter: 9.0, RMSE on training set: 0.9669191183820665.\n",
      "iter: 10.0, RMSE on training set: 0.9658818448894348.\n",
      "iter: 11.0, RMSE on training set: 0.9651470800226325.\n",
      "iter: 12.0, RMSE on training set: 0.9646322152470211.\n",
      "iter: 13.0, RMSE on training set: 0.9642763292229688.\n",
      "iter: 14.0, RMSE on training set: 0.9640346534640211.\n",
      "iter: 15.0, RMSE on training set: 0.9638744090840587.\n",
      "iter: 16.0, RMSE on training set: 0.9637716893588248.\n",
      "iter: 17.0, RMSE on training set: 0.9637091339704436.\n",
      "RMSE on test data: 1.0390650094255465.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0796230433735705.\n",
      "iter: 1.0, RMSE on training set: 1.0136934395714645.\n",
      "iter: 2.0, RMSE on training set: 0.9998727305500618.\n",
      "iter: 3.0, RMSE on training set: 0.9903199300870468.\n",
      "iter: 4.0, RMSE on training set: 0.9832712147061425.\n",
      "iter: 5.0, RMSE on training set: 0.9780769020569426.\n",
      "iter: 6.0, RMSE on training set: 0.9742682518638934.\n",
      "iter: 7.0, RMSE on training set: 0.9714909143057738.\n",
      "iter: 8.0, RMSE on training set: 0.9694774782469844.\n",
      "iter: 9.0, RMSE on training set: 0.9680272822221728.\n",
      "iter: 10.0, RMSE on training set: 0.9669905228110324.\n",
      "iter: 11.0, RMSE on training set: 0.9662558589419431.\n",
      "iter: 12.0, RMSE on training set: 0.9657408600086645.\n",
      "iter: 13.0, RMSE on training set: 0.9653847199134269.\n",
      "iter: 14.0, RMSE on training set: 0.9651427424350545.\n",
      "iter: 15.0, RMSE on training set: 0.9649821919223411.\n",
      "iter: 16.0, RMSE on training set: 0.9648791856072944.\n",
      "iter: 17.0, RMSE on training set: 0.9648163746011996.\n",
      "RMSE on test data: 1.0279965952936398.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0794720152316113.\n",
      "iter: 1.0, RMSE on training set: 1.0129201898504105.\n",
      "iter: 2.0, RMSE on training set: 0.9990520240280866.\n",
      "iter: 3.0, RMSE on training set: 0.9894788322109435.\n",
      "iter: 4.0, RMSE on training set: 0.9824149329097722.\n",
      "iter: 5.0, RMSE on training set: 0.977208911968289.\n",
      "iter: 6.0, RMSE on training set: 0.9733913966156842.\n",
      "iter: 7.0, RMSE on training set: 0.9706074668398744.\n",
      "iter: 8.0, RMSE on training set: 0.9685892238383914.\n",
      "iter: 9.0, RMSE on training set: 0.9671356059538753.\n",
      "iter: 10.0, RMSE on training set: 0.9660964877236591.\n",
      "iter: 11.0, RMSE on training set: 0.9653602711333867.\n",
      "iter: 12.0, RMSE on training set: 0.9648443226116505.\n",
      "iter: 13.0, RMSE on training set: 0.9644876775130494.\n",
      "iter: 14.0, RMSE on training set: 0.9642455172296721.\n",
      "iter: 15.0, RMSE on training set: 0.9640850127787132.\n",
      "iter: 16.0, RMSE on training set: 0.9639822111001635.\n",
      "iter: 17.0, RMSE on training set: 0.9639197110594381.\n",
      "RMSE on test data: 1.0365228704397071.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0789312383495024.\n",
      "iter: 1.0, RMSE on training set: 1.0126105507877237.\n",
      "iter: 2.0, RMSE on training set: 0.9988119225041217.\n",
      "iter: 3.0, RMSE on training set: 0.9892640840085579.\n",
      "iter: 4.0, RMSE on training set: 0.9822171559991486.\n",
      "iter: 5.0, RMSE on training set: 0.9770235437603276.\n",
      "iter: 6.0, RMSE on training set: 0.9732151467712993.\n",
      "iter: 7.0, RMSE on training set: 0.9704378557757563.\n",
      "iter: 8.0, RMSE on training set: 0.9684243857013226.\n",
      "iter: 9.0, RMSE on training set: 0.9669741426163376.\n",
      "iter: 10.0, RMSE on training set: 0.9659373565635585.\n",
      "iter: 11.0, RMSE on training set: 0.9652026993360968.\n",
      "iter: 12.0, RMSE on training set: 0.9646877421326634.\n",
      "iter: 13.0, RMSE on training set: 0.9643316756350333.\n",
      "iter: 14.0, RMSE on training set: 0.9640897987845201.\n",
      "iter: 15.0, RMSE on training set: 0.9639293711493556.\n",
      "iter: 16.0, RMSE on training set: 0.9638265059152005.\n",
      "iter: 17.0, RMSE on training set: 0.9637638510503461.\n",
      "RMSE on test data: 1.0376623121767954.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.0796115637137733.\n",
      "iter: 1.0, RMSE on training set: 1.0134221265282692.\n",
      "iter: 2.0, RMSE on training set: 0.9995667569364919.\n",
      "iter: 3.0, RMSE on training set: 0.9899843008289374.\n",
      "iter: 4.0, RMSE on training set: 0.9829146782781759.\n",
      "iter: 5.0, RMSE on training set: 0.9777061557066092.\n",
      "iter: 6.0, RMSE on training set: 0.9738880569230713.\n",
      "iter: 7.0, RMSE on training set: 0.9711046269619507.\n",
      "iter: 8.0, RMSE on training set: 0.9690874544578401.\n",
      "iter: 9.0, RMSE on training set: 0.9676351502125841.\n",
      "iter: 10.0, RMSE on training set: 0.9665973821829171.\n",
      "iter: 11.0, RMSE on training set: 0.9658624293053265.\n",
      "iter: 12.0, RMSE on training set: 0.9653475922134807.\n",
      "iter: 13.0, RMSE on training set: 0.9649918784302833.\n",
      "iter: 14.0, RMSE on training set: 0.96475046545741.\n",
      "iter: 15.0, RMSE on training set: 0.9645905344887475.\n",
      "iter: 16.0, RMSE on training set: 0.9644881500102307.\n",
      "iter: 17.0, RMSE on training set: 0.9644259314429527.\n",
      "RMSE on test data: 1.0321649465893645.\n",
      "Running num_features=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1118213776100123.\n",
      "iter: 1.0, RMSE on training set: 0.9941981031298104.\n",
      "iter: 2.0, RMSE on training set: 0.9822784553425377.\n",
      "iter: 3.0, RMSE on training set: 0.9755823402863614.\n",
      "iter: 4.0, RMSE on training set: 0.9704098891834642.\n",
      "iter: 5.0, RMSE on training set: 0.9662847777496292.\n",
      "iter: 6.0, RMSE on training set: 0.9629883233170392.\n",
      "iter: 7.0, RMSE on training set: 0.9603621083182499.\n",
      "iter: 8.0, RMSE on training set: 0.95827949083418.\n",
      "iter: 9.0, RMSE on training set: 0.9566371170397795.\n",
      "iter: 10.0, RMSE on training set: 0.9553503296292405.\n",
      "iter: 11.0, RMSE on training set: 0.9543498341571307.\n",
      "iter: 12.0, RMSE on training set: 0.953579023049903.\n",
      "iter: 13.0, RMSE on training set: 0.952991763000914.\n",
      "iter: 14.0, RMSE on training set: 0.9525505519008307.\n",
      "iter: 15.0, RMSE on training set: 0.9522249832961982.\n",
      "iter: 16.0, RMSE on training set: 0.9519904698586775.\n",
      "iter: 17.0, RMSE on training set: 0.9518271851968817.\n",
      "iter: 18.0, RMSE on training set: 0.9517191891997359.\n",
      "iter: 19.0, RMSE on training set: 0.951653707045171.\n",
      "RMSE on test data: 1.0561352925817622.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1115073918180196.\n",
      "iter: 1.0, RMSE on training set: 0.9940717408737579.\n",
      "iter: 2.0, RMSE on training set: 0.9821985155219192.\n",
      "iter: 3.0, RMSE on training set: 0.9754886842648903.\n",
      "iter: 4.0, RMSE on training set: 0.970301112538951.\n",
      "iter: 5.0, RMSE on training set: 0.9661643165099371.\n",
      "iter: 6.0, RMSE on training set: 0.9628587488473699.\n",
      "iter: 7.0, RMSE on training set: 0.9602252322001419.\n",
      "iter: 8.0, RMSE on training set: 0.9581366879603186.\n",
      "iter: 9.0, RMSE on training set: 0.9564894707307922.\n",
      "iter: 10.0, RMSE on training set: 0.9551986991579994.\n",
      "iter: 11.0, RMSE on training set: 0.9541948992988998.\n",
      "iter: 12.0, RMSE on training set: 0.9534213199751701.\n",
      "iter: 13.0, RMSE on training set: 0.9528317143627355.\n",
      "iter: 14.0, RMSE on training set: 0.9523884916968177.\n",
      "iter: 15.0, RMSE on training set: 0.9520611769590226.\n",
      "iter: 16.0, RMSE on training set: 0.9518251301656105.\n",
      "iter: 17.0, RMSE on training set: 0.9516604846667255.\n",
      "iter: 18.0, RMSE on training set: 0.9515512696331521.\n",
      "iter: 19.0, RMSE on training set: 0.9514846867963817.\n",
      "RMSE on test data: 1.0608460469040473.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1126057825812552.\n",
      "iter: 1.0, RMSE on training set: 0.9940847807892689.\n",
      "iter: 2.0, RMSE on training set: 0.982122929464535.\n",
      "iter: 3.0, RMSE on training set: 0.9754247503279275.\n",
      "iter: 4.0, RMSE on training set: 0.9702518243532067.\n",
      "iter: 5.0, RMSE on training set: 0.9661274054033472.\n",
      "iter: 6.0, RMSE on training set: 0.962831938191794.\n",
      "iter: 7.0, RMSE on training set: 0.960206612599786.\n",
      "iter: 8.0, RMSE on training set: 0.958124684800286.\n",
      "iter: 9.0, RMSE on training set: 0.956482799909728.\n",
      "iter: 10.0, RMSE on training set: 0.9551963264133149.\n",
      "iter: 11.0, RMSE on training set: 0.9541960001745218.\n",
      "iter: 12.0, RMSE on training set: 0.9534252427420794.\n",
      "iter: 13.0, RMSE on training set: 0.9528379473883671.\n",
      "iter: 14.0, RMSE on training set: 0.9523966356974268.\n",
      "iter: 15.0, RMSE on training set: 0.9520709219186376.\n",
      "iter: 16.0, RMSE on training set: 0.9518362364284587.\n",
      "iter: 17.0, RMSE on training set: 0.9516727676234313.\n",
      "iter: 18.0, RMSE on training set: 0.9515645874282043.\n",
      "iter: 19.0, RMSE on training set: 0.9514989305345275.\n",
      "RMSE on test data: 1.0606855184408275.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1120727623614457.\n",
      "iter: 1.0, RMSE on training set: 0.9944635189340938.\n",
      "iter: 2.0, RMSE on training set: 0.9825100249027852.\n",
      "iter: 3.0, RMSE on training set: 0.9757752789480597.\n",
      "iter: 4.0, RMSE on training set: 0.9705673357110897.\n",
      "iter: 5.0, RMSE on training set: 0.9664113602219191.\n",
      "iter: 6.0, RMSE on training set: 0.9630881359668817.\n",
      "iter: 7.0, RMSE on training set: 0.9604387581940775.\n",
      "iter: 8.0, RMSE on training set: 0.9583362106347796.\n",
      "iter: 9.0, RMSE on training set: 0.9566767924328771.\n",
      "iter: 10.0, RMSE on training set: 0.9553755075322806.\n",
      "iter: 11.0, RMSE on training set: 0.9543627391538636.\n",
      "iter: 12.0, RMSE on training set: 0.9535815830658708.\n",
      "iter: 13.0, RMSE on training set: 0.952985639246325.\n",
      "iter: 14.0, RMSE on training set: 0.9525371693205282.\n",
      "iter: 15.0, RMSE on training set: 0.9522055595562056.\n",
      "iter: 16.0, RMSE on training set: 0.9519660420058357.\n",
      "iter: 17.0, RMSE on training set: 0.9517986337003261.\n",
      "iter: 18.0, RMSE on training set: 0.9516872593644742.\n",
      "iter: 19.0, RMSE on training set: 0.9516190279219451.\n",
      "RMSE on test data: 1.0609035938358076.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1115307229489513.\n",
      "iter: 1.0, RMSE on training set: 0.9938418728071687.\n",
      "iter: 2.0, RMSE on training set: 0.9819041490568395.\n",
      "iter: 3.0, RMSE on training set: 0.9751814691810373.\n",
      "iter: 4.0, RMSE on training set: 0.9699894516895036.\n",
      "iter: 5.0, RMSE on training set: 0.9658486008406553.\n",
      "iter: 6.0, RMSE on training set: 0.9625382180667156.\n",
      "iter: 7.0, RMSE on training set: 0.9598994555207526.\n",
      "iter: 8.0, RMSE on training set: 0.9578056158238379.\n",
      "iter: 9.0, RMSE on training set: 0.9561533115867157.\n",
      "iter: 10.0, RMSE on training set: 0.9548578129029935.\n",
      "iter: 11.0, RMSE on training set: 0.9538497243523658.\n",
      "iter: 12.0, RMSE on training set: 0.9530723256625323.\n",
      "iter: 13.0, RMSE on training set: 0.952479370288926.\n",
      "iter: 14.0, RMSE on training set: 0.9520332484322788.\n",
      "iter: 15.0, RMSE on training set: 0.9517034543896808.\n",
      "iter: 16.0, RMSE on training set: 0.9514653111173703.\n",
      "iter: 17.0, RMSE on training set: 0.9512989121660036.\n",
      "iter: 18.0, RMSE on training set: 0.9511882466382694.\n",
      "iter: 19.0, RMSE on training set: 0.9511204775565921.\n",
      "RMSE on test data: 1.064565624135474.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1118584222128878.\n",
      "iter: 1.0, RMSE on training set: 0.9937244761386972.\n",
      "iter: 2.0, RMSE on training set: 0.9818105356920079.\n",
      "iter: 3.0, RMSE on training set: 0.9751305854780469.\n",
      "iter: 4.0, RMSE on training set: 0.9699691085510924.\n",
      "iter: 5.0, RMSE on training set: 0.9658524840568781.\n",
      "iter: 6.0, RMSE on training set: 0.9625626179293404.\n",
      "iter: 7.0, RMSE on training set: 0.9599413575891963.\n",
      "iter: 8.0, RMSE on training set: 0.9578623301283709.\n",
      "iter: 9.0, RMSE on training set: 0.9562224307037002.\n",
      "iter: 10.0, RMSE on training set: 0.9549372170676315.\n",
      "iter: 11.0, RMSE on training set: 0.9539375764419137.\n",
      "iter: 12.0, RMSE on training set: 0.9531670536837742.\n",
      "iter: 13.0, RMSE on training set: 0.9525796428694305.\n",
      "iter: 14.0, RMSE on training set: 0.9521379477471635.\n",
      "iter: 15.0, RMSE on training set: 0.9518116491435712.\n",
      "iter: 16.0, RMSE on training set: 0.9515762309920877.\n",
      "iter: 17.0, RMSE on training set: 0.9514119244340711.\n",
      "iter: 18.0, RMSE on training set: 0.9513028352280959.\n",
      "iter: 19.0, RMSE on training set: 0.9512362246112238.\n",
      "RMSE on test data: 1.0620240854650853.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.112850437544104.\n",
      "iter: 1.0, RMSE on training set: 0.9949859294767238.\n",
      "iter: 2.0, RMSE on training set: 0.9830568899701736.\n",
      "iter: 3.0, RMSE on training set: 0.976354503721614.\n",
      "iter: 4.0, RMSE on training set: 0.9711802186834636.\n",
      "iter: 5.0, RMSE on training set: 0.9670542913487709.\n",
      "iter: 6.0, RMSE on training set: 0.9637569355517112.\n",
      "iter: 7.0, RMSE on training set: 0.9611294495768637.\n",
      "iter: 8.0, RMSE on training set: 0.9590451856343336.\n",
      "iter: 9.0, RMSE on training set: 0.9574008674719607.\n",
      "iter: 10.0, RMSE on training set: 0.9561119291755207.\n",
      "iter: 11.0, RMSE on training set: 0.9551091611701867.\n",
      "iter: 12.0, RMSE on training set: 0.9543360287942128.\n",
      "iter: 13.0, RMSE on training set: 0.9537464583640475.\n",
      "iter: 14.0, RMSE on training set: 0.9533029943185036.\n",
      "iter: 15.0, RMSE on training set: 0.9529752647542368.\n",
      "iter: 16.0, RMSE on training set: 0.9527387064489948.\n",
      "iter: 17.0, RMSE on training set: 0.9525735084105827.\n",
      "iter: 18.0, RMSE on training set: 0.9524637389316603.\n",
      "iter: 19.0, RMSE on training set: 0.9523966261580317.\n",
      "RMSE on test data: 1.0506329136250998.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.11169075943064.\n",
      "iter: 1.0, RMSE on training set: 0.994264002994706.\n",
      "iter: 2.0, RMSE on training set: 0.9823598969639091.\n",
      "iter: 3.0, RMSE on training set: 0.9756387711264963.\n",
      "iter: 4.0, RMSE on training set: 0.9704438483756419.\n",
      "iter: 5.0, RMSE on training set: 0.966298949399211.\n",
      "iter: 6.0, RMSE on training set: 0.9629845903856507.\n",
      "iter: 7.0, RMSE on training set: 0.9603421829330301.\n",
      "iter: 8.0, RMSE on training set: 0.9582450409109564.\n",
      "iter: 9.0, RMSE on training set: 0.956589740137885.\n",
      "iter: 10.0, RMSE on training set: 0.9552915201188728.\n",
      "iter: 11.0, RMSE on training set: 0.9542809673429036.\n",
      "iter: 12.0, RMSE on training set: 0.9535013513774924.\n",
      "iter: 13.0, RMSE on training set: 0.9529064183838797.\n",
      "iter: 14.0, RMSE on training set: 0.9524585509693457.\n",
      "iter: 15.0, RMSE on training set: 0.9521272340801504.\n",
      "iter: 16.0, RMSE on training set: 0.9518877792025919.\n",
      "iter: 17.0, RMSE on training set: 0.9517202665281582.\n",
      "iter: 18.0, RMSE on training set: 0.951608670403734.\n",
      "iter: 19.0, RMSE on training set: 0.9515401382621285.\n",
      "RMSE on test data: 1.0604387870317318.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1120605490514353.\n",
      "iter: 1.0, RMSE on training set: 0.994086232418826.\n",
      "iter: 2.0, RMSE on training set: 0.9821461422558575.\n",
      "iter: 3.0, RMSE on training set: 0.9754258505934217.\n",
      "iter: 4.0, RMSE on training set: 0.9702362821991021.\n",
      "iter: 5.0, RMSE on training set: 0.9660986820743755.\n",
      "iter: 6.0, RMSE on training set: 0.9627922164206044.\n",
      "iter: 7.0, RMSE on training set: 0.9601575963634104.\n",
      "iter: 8.0, RMSE on training set: 0.9580678080753251.\n",
      "iter: 9.0, RMSE on training set: 0.9564192858390244.\n",
      "iter: 10.0, RMSE on training set: 0.9551272169246267.\n",
      "iter: 11.0, RMSE on training set: 0.9541221802097947.\n",
      "iter: 12.0, RMSE on training set: 0.9533474618583974.\n",
      "iter: 13.0, RMSE on training set: 0.9527568387527442.\n",
      "iter: 14.0, RMSE on training set: 0.9523127325881247.\n",
      "iter: 15.0, RMSE on training set: 0.9519846720231321.\n",
      "iter: 16.0, RMSE on training set: 0.9517480142623773.\n",
      "iter: 17.0, RMSE on training set: 0.9515828853535966.\n",
      "iter: 18.0, RMSE on training set: 0.9514733043242378.\n",
      "iter: 19.0, RMSE on training set: 0.9514064612220172.\n",
      "RMSE on test data: 1.060873778679192.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1127686524735743.\n",
      "iter: 1.0, RMSE on training set: 0.9946725707412305.\n",
      "iter: 2.0, RMSE on training set: 0.9826952756285826.\n",
      "iter: 3.0, RMSE on training set: 0.975972961450315.\n",
      "iter: 4.0, RMSE on training set: 0.9707846630159358.\n",
      "iter: 5.0, RMSE on training set: 0.9666474799238216.\n",
      "iter: 6.0, RMSE on training set: 0.9633407908895077.\n",
      "iter: 7.0, RMSE on training set: 0.9607056404459846.\n",
      "iter: 8.0, RMSE on training set: 0.9586151515108101.\n",
      "iter: 9.0, RMSE on training set: 0.9569658211257999.\n",
      "iter: 10.0, RMSE on training set: 0.9556728728690236.\n",
      "iter: 11.0, RMSE on training set: 0.9546669099805131.\n",
      "iter: 12.0, RMSE on training set: 0.9538912359206089.\n",
      "iter: 13.0, RMSE on training set: 0.953299640178153.\n",
      "iter: 14.0, RMSE on training set: 0.9528545538113469.\n",
      "iter: 15.0, RMSE on training set: 0.9525255125614723.\n",
      "iter: 16.0, RMSE on training set: 0.9522878790798449.\n",
      "iter: 17.0, RMSE on training set: 0.9521217836414326.\n",
      "iter: 18.0, RMSE on training set: 0.9520112485390134.\n",
      "iter: 19.0, RMSE on training set: 0.9519434662780001.\n",
      "RMSE on test data: 1.0559514963116388.\n",
      "Running num_features=13\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1775561936358248.\n",
      "iter: 1.0, RMSE on training set: 0.9818700188487675.\n",
      "iter: 2.0, RMSE on training set: 0.9658973441094907.\n",
      "iter: 3.0, RMSE on training set: 0.960212656219015.\n",
      "iter: 4.0, RMSE on training set: 0.9562072935515278.\n",
      "iter: 5.0, RMSE on training set: 0.9529655908083371.\n",
      "iter: 6.0, RMSE on training set: 0.9502692662535187.\n",
      "iter: 7.0, RMSE on training set: 0.9480159502982256.\n",
      "iter: 8.0, RMSE on training set: 0.9461353681211745.\n",
      "iter: 9.0, RMSE on training set: 0.9445716313145195.\n",
      "iter: 10.0, RMSE on training set: 0.9432779281813417.\n",
      "iter: 11.0, RMSE on training set: 0.9422143048296803.\n",
      "iter: 12.0, RMSE on training set: 0.9413464207917157.\n",
      "iter: 13.0, RMSE on training set: 0.9406446925694659.\n",
      "iter: 14.0, RMSE on training set: 0.9400836287332767.\n",
      "iter: 15.0, RMSE on training set: 0.9396412802152424.\n",
      "iter: 16.0, RMSE on training set: 0.939298771529158.\n",
      "iter: 17.0, RMSE on training set: 0.9390430603726548.\n",
      "iter: 18.0, RMSE on training set: 0.9388573361605851.\n",
      "iter: 19.0, RMSE on training set: 0.9387296851468625.\n",
      "iter: 20.0, RMSE on training set: 0.9386499034915385.\n",
      "RMSE on test data: 1.0817581725153.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1766202198380349.\n",
      "iter: 1.0, RMSE on training set: 0.9815125883741633.\n",
      "iter: 2.0, RMSE on training set: 0.9656195625476202.\n",
      "iter: 3.0, RMSE on training set: 0.9599780271622715.\n",
      "iter: 4.0, RMSE on training set: 0.95600155093899.\n",
      "iter: 5.0, RMSE on training set: 0.9527797430939373.\n",
      "iter: 6.0, RMSE on training set: 0.9500974095875732.\n",
      "iter: 7.0, RMSE on training set: 0.9478541944330906.\n",
      "iter: 8.0, RMSE on training set: 0.9459811406932618.\n",
      "iter: 9.0, RMSE on training set: 0.9444231861988245.\n",
      "iter: 10.0, RMSE on training set: 0.9431340276745239.\n",
      "iter: 11.0, RMSE on training set: 0.942074029462376.\n",
      "iter: 12.0, RMSE on training set: 0.9412090592509502.\n",
      "iter: 13.0, RMSE on training set: 0.9405096774626552.\n",
      "iter: 14.0, RMSE on training set: 0.9399504979060428.\n",
      "iter: 15.0, RMSE on training set: 0.939509652258103.\n",
      "iter: 16.0, RMSE on training set: 0.9391683292589676.\n",
      "iter: 17.0, RMSE on training set: 0.9389103736711312.\n",
      "iter: 18.0, RMSE on training set: 0.9387219357689361.\n",
      "iter: 19.0, RMSE on training set: 0.9385911646762356.\n",
      "iter: 20.0, RMSE on training set: 0.9385079401786927.\n",
      "RMSE on test data: 1.084103845041347.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1773304439437535.\n",
      "iter: 1.0, RMSE on training set: 0.9813510811390194.\n",
      "iter: 2.0, RMSE on training set: 0.9655050240119778.\n",
      "iter: 3.0, RMSE on training set: 0.9598521637133598.\n",
      "iter: 4.0, RMSE on training set: 0.9558518010137919.\n",
      "iter: 5.0, RMSE on training set: 0.9526089050310412.\n",
      "iter: 6.0, RMSE on training set: 0.9499104043966082.\n",
      "iter: 7.0, RMSE on training set: 0.9476552610957132.\n",
      "iter: 8.0, RMSE on training set: 0.9457734468198012.\n",
      "iter: 9.0, RMSE on training set: 0.944209040289228.\n",
      "iter: 10.0, RMSE on training set: 0.9429151284961981.\n",
      "iter: 11.0, RMSE on training set: 0.9418516499382172.\n",
      "iter: 12.0, RMSE on training set: 0.9409841696730289.\n",
      "iter: 13.0, RMSE on training set: 0.9402830268300295.\n",
      "iter: 14.0, RMSE on training set: 0.9397226684951057.\n",
      "iter: 15.0, RMSE on training set: 0.9392810974485611.\n",
      "iter: 16.0, RMSE on training set: 0.9389394008374787.\n",
      "iter: 17.0, RMSE on training set: 0.9386813423421712.\n",
      "iter: 18.0, RMSE on training set: 0.938493007094027.\n",
      "iter: 19.0, RMSE on training set: 0.938362491782724.\n",
      "iter: 20.0, RMSE on training set: 0.9382796340726705.\n",
      "RMSE on test data: 1.088567649729546.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1773166944529578.\n",
      "iter: 1.0, RMSE on training set: 0.9817339744130325.\n",
      "iter: 2.0, RMSE on training set: 0.9657889531605847.\n",
      "iter: 3.0, RMSE on training set: 0.9601109180286997.\n",
      "iter: 4.0, RMSE on training set: 0.9561026527547801.\n",
      "iter: 5.0, RMSE on training set: 0.9528556220171375.\n",
      "iter: 6.0, RMSE on training set: 0.9501531086883387.\n",
      "iter: 7.0, RMSE on training set: 0.9478934066980994.\n",
      "iter: 8.0, RMSE on training set: 0.9460066245481755.\n",
      "iter: 9.0, RMSE on training set: 0.9444370948311288.\n",
      "iter: 10.0, RMSE on training set: 0.9431381110567183.\n",
      "iter: 11.0, RMSE on training set: 0.9420697504437011.\n",
      "iter: 12.0, RMSE on training set: 0.9411976621023898.\n",
      "iter: 13.0, RMSE on training set: 0.9404922317401205.\n",
      "iter: 14.0, RMSE on training set: 0.9399279287702007.\n",
      "iter: 15.0, RMSE on training set: 0.9394827628107755.\n",
      "iter: 16.0, RMSE on training set: 0.9391378178514074.\n",
      "iter: 17.0, RMSE on training set: 0.9388768478547693.\n",
      "iter: 18.0, RMSE on training set: 0.9386859238798374.\n",
      "iter: 19.0, RMSE on training set: 0.938553125668775.\n",
      "iter: 20.0, RMSE on training set: 0.9384721241301729.\n",
      "RMSE on test data: 1.0885262726227496.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.177126322373679.\n",
      "iter: 1.0, RMSE on training set: 0.9812033145489365.\n",
      "iter: 2.0, RMSE on training set: 0.9653358967841748.\n",
      "iter: 3.0, RMSE on training set: 0.9596950734817716.\n",
      "iter: 4.0, RMSE on training set: 0.9557054202150611.\n",
      "iter: 5.0, RMSE on training set: 0.9524673022845328.\n",
      "iter: 6.0, RMSE on training set: 0.9497690156139299.\n",
      "iter: 7.0, RMSE on training set: 0.9475112650315153.\n",
      "iter: 8.0, RMSE on training set: 0.945625313022457.\n",
      "iter: 9.0, RMSE on training set: 0.9440560737748093.\n",
      "iter: 10.0, RMSE on training set: 0.9427571455002285.\n",
      "iter: 11.0, RMSE on training set: 0.9416887673504168.\n",
      "iter: 12.0, RMSE on training set: 0.9408166727325503.\n",
      "iter: 13.0, RMSE on training set: 0.9401112875336888.\n",
      "iter: 14.0, RMSE on training set: 0.9395470960106935.\n",
      "iter: 15.0, RMSE on training set: 0.9391021084437057.\n",
      "iter: 16.0, RMSE on training set: 0.9387574020908034.\n",
      "iter: 17.0, RMSE on training set: 0.9384967208896674.\n",
      "iter: 18.0, RMSE on training set: 0.9383061249798239.\n",
      "iter: 19.0, RMSE on training set: 0.9381736836127297.\n",
      "iter: 20.0, RMSE on training set: 0.938093101370773.\n",
      "RMSE on test data: 1.090719494696516.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1771266738085095.\n",
      "iter: 1.0, RMSE on training set: 0.9813418286267679.\n",
      "iter: 2.0, RMSE on training set: 0.9653979692106577.\n",
      "iter: 3.0, RMSE on training set: 0.9597534269398479.\n",
      "iter: 4.0, RMSE on training set: 0.9557691876672452.\n",
      "iter: 5.0, RMSE on training set: 0.9525388987694279.\n",
      "iter: 6.0, RMSE on training set: 0.9498494861464797.\n",
      "iter: 7.0, RMSE on training set: 0.9476006831291155.\n",
      "iter: 8.0, RMSE on training set: 0.9457231136958469.\n",
      "iter: 9.0, RMSE on training set: 0.9441613393197915.\n",
      "iter: 10.0, RMSE on training set: 0.9428688162029365.\n",
      "iter: 11.0, RMSE on training set: 0.941805769698534.\n",
      "iter: 12.0, RMSE on training set: 0.9409379889373795.\n",
      "iter: 13.0, RMSE on training set: 0.9402359886921733.\n",
      "iter: 14.0, RMSE on training set: 0.9396743542751566.\n",
      "iter: 15.0, RMSE on training set: 0.9392311976866087.\n",
      "iter: 16.0, RMSE on training set: 0.9388876926531876.\n",
      "iter: 17.0, RMSE on training set: 0.9386276716079767.\n",
      "iter: 18.0, RMSE on training set: 0.9384372742671273.\n",
      "iter: 19.0, RMSE on training set: 0.9383046405474852.\n",
      "iter: 20.0, RMSE on training set: 0.938219642172459.\n",
      "RMSE on test data: 1.0883399915933498.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1774549542689392.\n",
      "iter: 1.0, RMSE on training set: 0.9822003357764082.\n",
      "iter: 2.0, RMSE on training set: 0.9663438591308373.\n",
      "iter: 3.0, RMSE on training set: 0.9607127799240988.\n",
      "iter: 4.0, RMSE on training set: 0.9567392271591586.\n",
      "iter: 5.0, RMSE on training set: 0.9535186027497327.\n",
      "iter: 6.0, RMSE on training set: 0.9508367441592608.\n",
      "iter: 7.0, RMSE on training set: 0.9485935716675399.\n",
      "iter: 8.0, RMSE on training set: 0.9467202166384757.\n",
      "iter: 9.0, RMSE on training set: 0.9451616616191207.\n",
      "iter: 10.0, RMSE on training set: 0.9438716437967374.\n",
      "iter: 11.0, RMSE on training set: 0.9428105701259797.\n",
      "iter: 12.0, RMSE on training set: 0.9419443509738281.\n",
      "iter: 13.0, RMSE on training set: 0.9412435868140445.\n",
      "iter: 14.0, RMSE on training set: 0.9406829271789174.\n",
      "iter: 15.0, RMSE on training set: 0.940240534465906.\n",
      "iter: 16.0, RMSE on training set: 0.9398976231664594.\n",
      "iter: 17.0, RMSE on training set: 0.939638059226305.\n",
      "iter: 18.0, RMSE on training set: 0.9394480100770383.\n",
      "iter: 19.0, RMSE on training set: 0.9393156385357971.\n",
      "iter: 20.0, RMSE on training set: 0.9392308351445318.\n",
      "RMSE on test data: 1.0777264610050583.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.176963760629575.\n",
      "iter: 1.0, RMSE on training set: 0.9814677782309397.\n",
      "iter: 2.0, RMSE on training set: 0.9655795650946363.\n",
      "iter: 3.0, RMSE on training set: 0.959942200860277.\n",
      "iter: 4.0, RMSE on training set: 0.9559622669699347.\n",
      "iter: 5.0, RMSE on training set: 0.9527378349287758.\n",
      "iter: 6.0, RMSE on training set: 0.950054094533303.\n",
      "iter: 7.0, RMSE on training set: 0.9478100571398874.\n",
      "iter: 8.0, RMSE on training set: 0.9459363575682975.\n",
      "iter: 9.0, RMSE on training set: 0.9443777814512724.\n",
      "iter: 10.0, RMSE on training set: 0.9430879976750317.\n",
      "iter: 11.0, RMSE on training set: 0.9420273867935968.\n",
      "iter: 12.0, RMSE on training set: 0.941161842129907.\n",
      "iter: 13.0, RMSE on training set: 0.9404619470294817.\n",
      "iter: 14.0, RMSE on training set: 0.9399023322508077.\n",
      "iter: 15.0, RMSE on training set: 0.9394611403764196.\n",
      "iter: 16.0, RMSE on training set: 0.9391195659504153.\n",
      "iter: 17.0, RMSE on training set: 0.9388614555665931.\n",
      "iter: 18.0, RMSE on training set: 0.9386764042745285.\n",
      "iter: 19.0, RMSE on training set: 0.9385493231585649.\n",
      "iter: 20.0, RMSE on training set: 0.9384700505692463.\n",
      "RMSE on test data: 1.086786828033254.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1776175714132742.\n",
      "iter: 1.0, RMSE on training set: 0.9816142692515911.\n",
      "iter: 2.0, RMSE on training set: 0.9656557120211473.\n",
      "iter: 3.0, RMSE on training set: 0.9599599478025086.\n",
      "iter: 4.0, RMSE on training set: 0.9559443192533863.\n",
      "iter: 5.0, RMSE on training set: 0.9526940380754123.\n",
      "iter: 6.0, RMSE on training set: 0.9499902235579567.\n",
      "iter: 7.0, RMSE on training set: 0.9477301952128719.\n",
      "iter: 8.0, RMSE on training set: 0.9458436041525329.\n",
      "iter: 9.0, RMSE on training set: 0.944274551489087.\n",
      "iter: 10.0, RMSE on training set: 0.9429762049266551.\n",
      "iter: 11.0, RMSE on training set: 0.9419085671090326.\n",
      "iter: 12.0, RMSE on training set: 0.9410372381485091.\n",
      "iter: 13.0, RMSE on training set: 0.9403325676119952.\n",
      "iter: 14.0, RMSE on training set: 0.9397689959217104.\n",
      "iter: 15.0, RMSE on training set: 0.9393245086901746.\n",
      "iter: 16.0, RMSE on training set: 0.938980170263375.\n",
      "iter: 17.0, RMSE on training set: 0.9387197191853677.\n",
      "iter: 18.0, RMSE on training set: 0.9385292152088806.\n",
      "iter: 19.0, RMSE on training set: 0.9384005239447918.\n",
      "iter: 20.0, RMSE on training set: 0.9383198448507283.\n",
      "RMSE on test data: 1.0857727797815089.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.1777699616493145.\n",
      "iter: 1.0, RMSE on training set: 0.9818875325066659.\n",
      "iter: 2.0, RMSE on training set: 0.9659438524278733.\n",
      "iter: 3.0, RMSE on training set: 0.9602821790934165.\n",
      "iter: 4.0, RMSE on training set: 0.9562925148843473.\n",
      "iter: 5.0, RMSE on training set: 0.9530631321753606.\n",
      "iter: 6.0, RMSE on training set: 0.9503772177867422.\n",
      "iter: 7.0, RMSE on training set: 0.9481329193603545.\n",
      "iter: 8.0, RMSE on training set: 0.9462602919824009.\n",
      "iter: 9.0, RMSE on training set: 0.9447036818400482.\n",
      "iter: 10.0, RMSE on training set: 0.9434164403739426.\n",
      "iter: 11.0, RMSE on training set: 0.9423587238413093.\n",
      "iter: 12.0, RMSE on training set: 0.9414962666573591.\n",
      "iter: 13.0, RMSE on training set: 0.9407995376252322.\n",
      "iter: 14.0, RMSE on training set: 0.9402430832923503.\n",
      "iter: 15.0, RMSE on training set: 0.939804983460158.\n",
      "iter: 16.0, RMSE on training set: 0.9394663856987592.\n",
      "iter: 17.0, RMSE on training set: 0.9392111017634968.\n",
      "iter: 18.0, RMSE on training set: 0.9390252555697316.\n",
      "iter: 19.0, RMSE on training set: 0.9388969755006465.\n",
      "iter: 20.0, RMSE on training set: 0.9388200298481225.\n",
      "RMSE on test data: 1.083950163558204.\n",
      "Running num_features=16\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.2748332297959213.\n",
      "iter: 1.0, RMSE on training set: 0.9765417001192256.\n",
      "iter: 2.0, RMSE on training set: 0.9510773287905009.\n",
      "iter: 3.0, RMSE on training set: 0.9447280466724078.\n",
      "iter: 4.0, RMSE on training set: 0.9411501049940967.\n",
      "iter: 5.0, RMSE on training set: 0.938438246354791.\n",
      "iter: 6.0, RMSE on training set: 0.9362008820304463.\n",
      "iter: 7.0, RMSE on training set: 0.9343089919159162.\n",
      "iter: 8.0, RMSE on training set: 0.932698901095619.\n",
      "iter: 9.0, RMSE on training set: 0.9313288834588859.\n",
      "iter: 10.0, RMSE on training set: 0.9301669291438536.\n",
      "iter: 11.0, RMSE on training set: 0.9291865265138879.\n",
      "iter: 12.0, RMSE on training set: 0.928364913908455.\n",
      "iter: 13.0, RMSE on training set: 0.9276822126289215.\n",
      "iter: 14.0, RMSE on training set: 0.9271228041412458.\n",
      "iter: 15.0, RMSE on training set: 0.9266697039124387.\n",
      "iter: 16.0, RMSE on training set: 0.9263092443869078.\n",
      "iter: 17.0, RMSE on training set: 0.9260292475485193.\n",
      "iter: 18.0, RMSE on training set: 0.9258189050331137.\n",
      "iter: 19.0, RMSE on training set: 0.9256686437532401.\n",
      "iter: 20.0, RMSE on training set: 0.9255699942914882.\n",
      "RMSE on test data: 1.1123530590265325.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.2734148997126757.\n",
      "iter: 1.0, RMSE on training set: 0.976179919067642.\n",
      "iter: 2.0, RMSE on training set: 0.9509347306022601.\n",
      "iter: 3.0, RMSE on training set: 0.9446242288829596.\n",
      "iter: 4.0, RMSE on training set: 0.9410648801709712.\n",
      "iter: 5.0, RMSE on training set: 0.9383661277766066.\n",
      "iter: 6.0, RMSE on training set: 0.9361386870448533.\n",
      "iter: 7.0, RMSE on training set: 0.9342542996795766.\n",
      "iter: 8.0, RMSE on training set: 0.9326497160353949.\n",
      "iter: 9.0, RMSE on training set: 0.9312835412197744.\n",
      "iter: 10.0, RMSE on training set: 0.930124053082127.\n",
      "iter: 11.0, RMSE on training set: 0.929144997176005.\n",
      "iter: 12.0, RMSE on training set: 0.9283238366945998.\n",
      "iter: 13.0, RMSE on training set: 0.9276408834838045.\n",
      "iter: 14.0, RMSE on training set: 0.9270806660650339.\n",
      "iter: 15.0, RMSE on training set: 0.9266263245625518.\n",
      "iter: 16.0, RMSE on training set: 0.9262642877236759.\n",
      "iter: 17.0, RMSE on training set: 0.9259824625678025.\n",
      "iter: 18.0, RMSE on training set: 0.9257701073081048.\n",
      "iter: 19.0, RMSE on training set: 0.9256177015978923.\n",
      "iter: 20.0, RMSE on training set: 0.9255168179239376.\n",
      "iter: 21.0, RMSE on training set: 0.9254600059101966.\n",
      "RMSE on test data: 1.111913539450158.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.274587146694873.\n",
      "iter: 1.0, RMSE on training set: 0.976311408237142.\n",
      "iter: 2.0, RMSE on training set: 0.9508566696982708.\n",
      "iter: 3.0, RMSE on training set: 0.9444526222533051.\n",
      "iter: 4.0, RMSE on training set: 0.940843190293707.\n",
      "iter: 5.0, RMSE on training set: 0.9381128764428428.\n",
      "iter: 6.0, RMSE on training set: 0.9358630941342281.\n",
      "iter: 7.0, RMSE on training set: 0.9339618268734987.\n",
      "iter: 8.0, RMSE on training set: 0.9323440599270691.\n",
      "iter: 9.0, RMSE on training set: 0.9309674230690237.\n",
      "iter: 10.0, RMSE on training set: 0.9297995786216399.\n",
      "iter: 11.0, RMSE on training set: 0.9288138432678353.\n",
      "iter: 12.0, RMSE on training set: 0.9279873621718597.\n",
      "iter: 13.0, RMSE on training set: 0.9273002023008254.\n",
      "iter: 14.0, RMSE on training set: 0.9267348230106566.\n",
      "iter: 15.0, RMSE on training set: 0.9262778717550363.\n",
      "iter: 16.0, RMSE on training set: 0.9259138171792575.\n",
      "iter: 17.0, RMSE on training set: 0.9256305218343919.\n",
      "iter: 18.0, RMSE on training set: 0.9254171717054175.\n",
      "iter: 19.0, RMSE on training set: 0.9252641723602469.\n",
      "iter: 20.0, RMSE on training set: 0.9251630320381156.\n",
      "iter: 21.0, RMSE on training set: 0.9251062449693357.\n",
      "RMSE on test data: 1.1207760561685358.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.2750973507174566.\n",
      "iter: 1.0, RMSE on training set: 0.9762372062493111.\n",
      "iter: 2.0, RMSE on training set: 0.9509053465676334.\n",
      "iter: 3.0, RMSE on training set: 0.9445690026359252.\n",
      "iter: 4.0, RMSE on training set: 0.9409929688514463.\n",
      "iter: 5.0, RMSE on training set: 0.9382835433529532.\n",
      "iter: 6.0, RMSE on training set: 0.9360486589179644.\n",
      "iter: 7.0, RMSE on training set: 0.9341587025515301.\n",
      "iter: 8.0, RMSE on training set: 0.9325497631279969.\n",
      "iter: 9.0, RMSE on training set: 0.9311800765695247.\n",
      "iter: 10.0, RMSE on training set: 0.9300176884355248.\n",
      "iter: 11.0, RMSE on training set: 0.929036187867761.\n",
      "iter: 12.0, RMSE on training set: 0.9282129293335587.\n",
      "iter: 13.0, RMSE on training set: 0.9275281478896217.\n",
      "iter: 14.0, RMSE on training set: 0.9269662934179992.\n",
      "iter: 15.0, RMSE on training set: 0.9265104981874475.\n",
      "iter: 16.0, RMSE on training set: 0.9261471709624391.\n",
      "iter: 17.0, RMSE on training set: 0.9258641958585467.\n",
      "iter: 18.0, RMSE on training set: 0.9256508153758168.\n",
      "iter: 19.0, RMSE on training set: 0.9254974978378245.\n",
      "iter: 20.0, RMSE on training set: 0.9253958072491958.\n",
      "iter: 21.0, RMSE on training set: 0.9253382865671306.\n",
      "RMSE on test data: 1.1197296675836366.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.2738083192068972.\n",
      "iter: 1.0, RMSE on training set: 0.9758084982843389.\n",
      "iter: 2.0, RMSE on training set: 0.9504374859591079.\n",
      "iter: 3.0, RMSE on training set: 0.9441328529237304.\n",
      "iter: 4.0, RMSE on training set: 0.9405723803655408.\n",
      "iter: 5.0, RMSE on training set: 0.9378702646887375.\n",
      "iter: 6.0, RMSE on training set: 0.935640039203659.\n",
      "iter: 7.0, RMSE on training set: 0.9337538557073997.\n",
      "iter: 8.0, RMSE on training set: 0.9321482861560826.\n",
      "iter: 9.0, RMSE on training set: 0.930781738416895.\n",
      "iter: 10.0, RMSE on training set: 0.9296223586696918.\n",
      "iter: 11.0, RMSE on training set: 0.9286438038773395.\n",
      "iter: 12.0, RMSE on training set: 0.927823470468493.\n",
      "iter: 13.0, RMSE on training set: 0.9271416152467307.\n",
      "iter: 14.0, RMSE on training set: 0.9265827534524028.\n",
      "iter: 15.0, RMSE on training set: 0.926129946232202.\n",
      "iter: 16.0, RMSE on training set: 0.9257695906585458.\n",
      "iter: 17.0, RMSE on training set: 0.9254895647000766.\n",
      "iter: 18.0, RMSE on training set: 0.9252791026748943.\n",
      "iter: 19.0, RMSE on training set: 0.9251286642343794.\n",
      "iter: 20.0, RMSE on training set: 0.9250298050332317.\n",
      "RMSE on test data: 1.122705690079611.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.2733420280570384.\n",
      "iter: 1.0, RMSE on training set: 0.9760353533136856.\n",
      "iter: 2.0, RMSE on training set: 0.9505407589660796.\n",
      "iter: 3.0, RMSE on training set: 0.9441861100392615.\n",
      "iter: 4.0, RMSE on training set: 0.9406114758127927.\n",
      "iter: 5.0, RMSE on training set: 0.9379026571792128.\n",
      "iter: 6.0, RMSE on training set: 0.9356666791103467.\n",
      "iter: 7.0, RMSE on training set: 0.9337749374591927.\n",
      "iter: 8.0, RMSE on training set: 0.9321642043586524.\n",
      "iter: 9.0, RMSE on training set: 0.9307930326371543.\n",
      "iter: 10.0, RMSE on training set: 0.9296295725440827.\n",
      "iter: 11.0, RMSE on training set: 0.9286474106171179.\n",
      "iter: 12.0, RMSE on training set: 0.9278238510088491.\n",
      "iter: 13.0, RMSE on training set: 0.927139062128546.\n",
      "iter: 14.0, RMSE on training set: 0.9265755742759998.\n",
      "iter: 15.0, RMSE on training set: 0.9261200832449193.\n",
      "iter: 16.0, RMSE on training set: 0.9257570991507713.\n",
      "iter: 17.0, RMSE on training set: 0.9254745347553713.\n",
      "iter: 18.0, RMSE on training set: 0.9252616131852408.\n",
      "iter: 19.0, RMSE on training set: 0.925108773513134.\n",
      "iter: 20.0, RMSE on training set: 0.9250075549761619.\n",
      "iter: 21.0, RMSE on training set: 0.9249504806980009.\n",
      "RMSE on test data: 1.1207787873496597.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.275497271498326.\n",
      "iter: 1.0, RMSE on training set: 0.9771813868610463.\n",
      "iter: 2.0, RMSE on training set: 0.9517269083141824.\n",
      "iter: 3.0, RMSE on training set: 0.9453453861271611.\n",
      "iter: 4.0, RMSE on training set: 0.9417552984795339.\n",
      "iter: 5.0, RMSE on training set: 0.9390395595064043.\n",
      "iter: 6.0, RMSE on training set: 0.9368006132903213.\n",
      "iter: 7.0, RMSE on training set: 0.9349076111802558.\n",
      "iter: 8.0, RMSE on training set: 0.9332962769423796.\n",
      "iter: 9.0, RMSE on training set: 0.9319246826776068.\n",
      "iter: 10.0, RMSE on training set: 0.9307607720419016.\n",
      "iter: 11.0, RMSE on training set: 0.9297780574785061.\n",
      "iter: 12.0, RMSE on training set: 0.9289538319194933.\n",
      "iter: 13.0, RMSE on training set: 0.9282682808771422.\n",
      "iter: 14.0, RMSE on training set: 0.9277039625767783.\n",
      "iter: 15.0, RMSE on training set: 0.927247556465986.\n",
      "iter: 16.0, RMSE on training set: 0.9268836466771665.\n",
      "iter: 17.0, RMSE on training set: 0.9266001645244831.\n",
      "iter: 18.0, RMSE on training set: 0.9263863513565832.\n",
      "iter: 19.0, RMSE on training set: 0.9262326601134099.\n",
      "iter: 20.0, RMSE on training set: 0.9261306398977125.\n",
      "iter: 21.0, RMSE on training set: 0.9260728202103898.\n",
      "RMSE on test data: 1.1078511515076122.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.2743976705946594.\n",
      "iter: 1.0, RMSE on training set: 0.9764689540658631.\n",
      "iter: 2.0, RMSE on training set: 0.9509866401604289.\n",
      "iter: 3.0, RMSE on training set: 0.9446019398688096.\n",
      "iter: 4.0, RMSE on training set: 0.9410124122944826.\n",
      "iter: 5.0, RMSE on training set: 0.9382982055863495.\n",
      "iter: 6.0, RMSE on training set: 0.9360604878350343.\n",
      "iter: 7.0, RMSE on training set: 0.9341680756643677.\n",
      "iter: 8.0, RMSE on training set: 0.9325567961278515.\n",
      "iter: 9.0, RMSE on training set: 0.9311849213226496.\n",
      "iter: 10.0, RMSE on training set: 0.9300205765526243.\n",
      "iter: 11.0, RMSE on training set: 0.9290374101220881.\n",
      "iter: 12.0, RMSE on training set: 0.9282128069185542.\n",
      "iter: 13.0, RMSE on training set: 0.9275270093747566.\n",
      "iter: 14.0, RMSE on training set: 0.9269644991883025.\n",
      "iter: 15.0, RMSE on training set: 0.9265083599702415.\n",
      "iter: 16.0, RMSE on training set: 0.9261449750659584.\n",
      "iter: 17.0, RMSE on training set: 0.9258622002947584.\n",
      "iter: 18.0, RMSE on training set: 0.9256492475212429.\n",
      "iter: 19.0, RMSE on training set: 0.9254965541911986.\n",
      "iter: 20.0, RMSE on training set: 0.9253956545783288.\n",
      "iter: 21.0, RMSE on training set: 0.9253390637880751.\n",
      "RMSE on test data: 1.1212845394604143.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.274906960425923.\n",
      "iter: 1.0, RMSE on training set: 0.9764390175541846.\n",
      "iter: 2.0, RMSE on training set: 0.9510102079712032.\n",
      "iter: 3.0, RMSE on training set: 0.9446273225032322.\n",
      "iter: 4.0, RMSE on training set: 0.9410187693134004.\n",
      "iter: 5.0, RMSE on training set: 0.9382842710111607.\n",
      "iter: 6.0, RMSE on training set: 0.9360297356677348.\n",
      "iter: 7.0, RMSE on training set: 0.9341240333193696.\n",
      "iter: 8.0, RMSE on training set: 0.9325022010213894.\n",
      "iter: 9.0, RMSE on training set: 0.9311217838742146.\n",
      "iter: 10.0, RMSE on training set: 0.929950370322786.\n",
      "iter: 11.0, RMSE on training set: 0.9289612277054355.\n",
      "iter: 12.0, RMSE on training set: 0.9281314695140604.\n",
      "iter: 13.0, RMSE on training set: 0.9274427781598412.\n",
      "iter: 14.0, RMSE on training set: 0.9268763648500886.\n",
      "iter: 15.0, RMSE on training set: 0.9264167506516464.\n",
      "iter: 16.0, RMSE on training set: 0.9260501665459503.\n",
      "iter: 17.0, RMSE on training set: 0.9257643900886725.\n",
      "iter: 18.0, RMSE on training set: 0.9255485878366005.\n",
      "iter: 19.0, RMSE on training set: 0.9253931650198217.\n",
      "iter: 20.0, RMSE on training set: 0.9252896324104312.\n",
      "iter: 21.0, RMSE on training set: 0.9252304877629707.\n",
      "RMSE on test data: 1.1148135646969142.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.2751908876810767.\n",
      "iter: 1.0, RMSE on training set: 0.9767085190377252.\n",
      "iter: 2.0, RMSE on training set: 0.9512638281733633.\n",
      "iter: 3.0, RMSE on training set: 0.9448840476394271.\n",
      "iter: 4.0, RMSE on training set: 0.9412856162693432.\n",
      "iter: 5.0, RMSE on training set: 0.9385634791351648.\n",
      "iter: 6.0, RMSE on training set: 0.9363211027586144.\n",
      "iter: 7.0, RMSE on training set: 0.9344267201019412.\n",
      "iter: 8.0, RMSE on training set: 0.9328153547846327.\n",
      "iter: 9.0, RMSE on training set: 0.9314446471800942.\n",
      "iter: 10.0, RMSE on training set: 0.9302822719719475.\n",
      "iter: 11.0, RMSE on training set: 0.9293015637976292.\n",
      "iter: 12.0, RMSE on training set: 0.9284796887433263.\n",
      "iter: 13.0, RMSE on training set: 0.9277982760986357.\n",
      "iter: 14.0, RMSE on training set: 0.9272386586892747.\n",
      "iter: 15.0, RMSE on training set: 0.9267853944485129.\n",
      "iter: 16.0, RMSE on training set: 0.926424743608045.\n",
      "iter: 17.0, RMSE on training set: 0.9261445110810459.\n",
      "iter: 18.0, RMSE on training set: 0.9259338885318934.\n",
      "iter: 19.0, RMSE on training set: 0.9257833047268684.\n",
      "iter: 20.0, RMSE on training set: 0.9256842925112663.\n",
      "RMSE on test data: 1.114702457935656.\n",
      "Running num_features=20\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4113070431317014.\n",
      "iter: 1.0, RMSE on training set: 0.9801468756914272.\n",
      "iter: 2.0, RMSE on training set: 0.9353607469126302.\n",
      "iter: 3.0, RMSE on training set: 0.9256626343887315.\n",
      "iter: 4.0, RMSE on training set: 0.9215178478550102.\n",
      "iter: 5.0, RMSE on training set: 0.9188742825315502.\n",
      "iter: 6.0, RMSE on training set: 0.9168553248469911.\n",
      "iter: 7.0, RMSE on training set: 0.9151960771193979.\n",
      "iter: 8.0, RMSE on training set: 0.9137913965666785.\n",
      "iter: 9.0, RMSE on training set: 0.912588601566604.\n",
      "iter: 10.0, RMSE on training set: 0.911555674395689.\n",
      "iter: 11.0, RMSE on training set: 0.910670728173747.\n",
      "iter: 12.0, RMSE on training set: 0.9099160500158653.\n",
      "iter: 13.0, RMSE on training set: 0.9092772506056547.\n",
      "iter: 14.0, RMSE on training set: 0.908741927612482.\n",
      "iter: 15.0, RMSE on training set: 0.9082991400974194.\n",
      "iter: 16.0, RMSE on training set: 0.9079391248037578.\n",
      "iter: 17.0, RMSE on training set: 0.9076531000914709.\n",
      "iter: 18.0, RMSE on training set: 0.9074331292920933.\n",
      "iter: 19.0, RMSE on training set: 0.9072720189705152.\n",
      "iter: 20.0, RMSE on training set: 0.9071632390000564.\n",
      "iter: 21.0, RMSE on training set: 0.907100856911907.\n",
      "RMSE on test data: 1.1577475679604916.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.409573589875478.\n",
      "iter: 1.0, RMSE on training set: 0.9793171359732202.\n",
      "iter: 2.0, RMSE on training set: 0.9349987266173935.\n",
      "iter: 3.0, RMSE on training set: 0.925360926023389.\n",
      "iter: 4.0, RMSE on training set: 0.921243856650043.\n",
      "iter: 5.0, RMSE on training set: 0.9186204837231423.\n",
      "iter: 6.0, RMSE on training set: 0.9166165493423827.\n",
      "iter: 7.0, RMSE on training set: 0.9149686187731556.\n",
      "iter: 8.0, RMSE on training set: 0.9135727114435845.\n",
      "iter: 9.0, RMSE on training set: 0.912376949218286.\n",
      "iter: 10.0, RMSE on training set: 0.9113502369753553.\n",
      "iter: 11.0, RMSE on training set: 0.9104704020534082.\n",
      "iter: 12.0, RMSE on training set: 0.9097202510249169.\n",
      "iter: 13.0, RMSE on training set: 0.9090855021474873.\n",
      "iter: 14.0, RMSE on training set: 0.9085538202588093.\n",
      "iter: 15.0, RMSE on training set: 0.9081143309649651.\n",
      "iter: 16.0, RMSE on training set: 0.9077573217816229.\n",
      "iter: 17.0, RMSE on training set: 0.9074740500322076.\n",
      "iter: 18.0, RMSE on training set: 0.9072566093512743.\n",
      "iter: 19.0, RMSE on training set: 0.9070978304024465.\n",
      "iter: 20.0, RMSE on training set: 0.9069912026387452.\n",
      "iter: 21.0, RMSE on training set: 0.906930809793092.\n",
      "RMSE on test data: 1.1594546786674313.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4120527649805397.\n",
      "iter: 1.0, RMSE on training set: 0.9797212655584817.\n",
      "iter: 2.0, RMSE on training set: 0.9350247834780121.\n",
      "iter: 3.0, RMSE on training set: 0.9253427149618618.\n",
      "iter: 4.0, RMSE on training set: 0.9212009762593241.\n",
      "iter: 5.0, RMSE on training set: 0.9185625551853305.\n",
      "iter: 6.0, RMSE on training set: 0.9165490454588882.\n",
      "iter: 7.0, RMSE on training set: 0.9148943867460904.\n",
      "iter: 8.0, RMSE on training set: 0.9134931916702353.\n",
      "iter: 9.0, RMSE on training set: 0.9122928848790021.\n",
      "iter: 10.0, RMSE on training set: 0.9112616373307684.\n",
      "iter: 11.0, RMSE on training set: 0.9103771201935456.\n",
      "iter: 12.0, RMSE on training set: 0.9096228613163531.\n",
      "iter: 13.0, RMSE on training set: 0.9089840402792558.\n",
      "iter: 14.0, RMSE on training set: 0.908448391936735.\n",
      "iter: 15.0, RMSE on training set: 0.9080050743114633.\n",
      "iter: 16.0, RMSE on training set: 0.9076443767551894.\n",
      "iter: 17.0, RMSE on training set: 0.9073575540395805.\n",
      "iter: 18.0, RMSE on training set: 0.9071366958081531.\n",
      "iter: 19.0, RMSE on training set: 0.9069746283905764.\n",
      "iter: 20.0, RMSE on training set: 0.9068648368951632.\n",
      "iter: 21.0, RMSE on training set: 0.9068014008453089.\n",
      "RMSE on test data: 1.1661987592706238.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4108050402658665.\n",
      "iter: 1.0, RMSE on training set: 0.9794251350319332.\n",
      "iter: 2.0, RMSE on training set: 0.9348158537807985.\n",
      "iter: 3.0, RMSE on training set: 0.9251638812286077.\n",
      "iter: 4.0, RMSE on training set: 0.9210560534610982.\n",
      "iter: 5.0, RMSE on training set: 0.918441552572304.\n",
      "iter: 6.0, RMSE on training set: 0.916444740461665.\n",
      "iter: 7.0, RMSE on training set: 0.9148024556884491.\n",
      "iter: 8.0, RMSE on training set: 0.9134110008572895.\n",
      "iter: 9.0, RMSE on training set: 0.9122187092739215.\n",
      "iter: 10.0, RMSE on training set: 0.9111946147972817.\n",
      "iter: 11.0, RMSE on training set: 0.9103166668327339.\n",
      "iter: 12.0, RMSE on training set: 0.9095677758679661.\n",
      "iter: 13.0, RMSE on training set: 0.9089337350155896.\n",
      "iter: 14.0, RMSE on training set: 0.9084022773015321.\n",
      "iter: 15.0, RMSE on training set: 0.9079625901229751.\n",
      "iter: 16.0, RMSE on training set: 0.9076050171787475.\n",
      "iter: 17.0, RMSE on training set: 0.9073208670647784.\n",
      "iter: 18.0, RMSE on training set: 0.9071022802271407.\n",
      "iter: 19.0, RMSE on training set: 0.9069421299990065.\n",
      "iter: 20.0, RMSE on training set: 0.9068339446241082.\n",
      "iter: 21.0, RMSE on training set: 0.9067718429856624.\n",
      "RMSE on test data: 1.163498545513094.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4109005049216448.\n",
      "iter: 1.0, RMSE on training set: 0.9796830924160406.\n",
      "iter: 2.0, RMSE on training set: 0.9348745950657397.\n",
      "iter: 3.0, RMSE on training set: 0.9251779122715996.\n",
      "iter: 4.0, RMSE on training set: 0.9210212661458734.\n",
      "iter: 5.0, RMSE on training set: 0.9183657110844065.\n",
      "iter: 6.0, RMSE on training set: 0.9163362026761869.\n",
      "iter: 7.0, RMSE on training set: 0.9146679938412207.\n",
      "iter: 8.0, RMSE on training set: 0.9132556081931369.\n",
      "iter: 9.0, RMSE on training set: 0.912046041209132.\n",
      "iter: 10.0, RMSE on training set: 0.9110070705614759.\n",
      "iter: 11.0, RMSE on training set: 0.9101166860190171.\n",
      "iter: 12.0, RMSE on training set: 0.9093570918865798.\n",
      "iter: 13.0, RMSE on training set: 0.9087138639719049.\n",
      "iter: 14.0, RMSE on training set: 0.9081745738668165.\n",
      "iter: 15.0, RMSE on training set: 0.9077282569253868.\n",
      "iter: 16.0, RMSE on training set: 0.907365126816882.\n",
      "iter: 17.0, RMSE on training set: 0.9070763784617779.\n",
      "iter: 18.0, RMSE on training set: 0.9068540512346084.\n",
      "iter: 19.0, RMSE on training set: 0.906690927410669.\n",
      "iter: 20.0, RMSE on training set: 0.9065804525336023.\n",
      "iter: 21.0, RMSE on training set: 0.9065166700492364.\n",
      "RMSE on test data: 1.170489023815864.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4104913479945311.\n",
      "iter: 1.0, RMSE on training set: 0.9792204831785107.\n",
      "iter: 2.0, RMSE on training set: 0.9346115551024337.\n",
      "iter: 3.0, RMSE on training set: 0.9249016637429743.\n",
      "iter: 4.0, RMSE on training set: 0.9207519884427505.\n",
      "iter: 5.0, RMSE on training set: 0.9181118852016517.\n",
      "iter: 6.0, RMSE on training set: 0.9160995789847416.\n",
      "iter: 7.0, RMSE on training set: 0.9144481827494333.\n",
      "iter: 8.0, RMSE on training set: 0.913051730943918.\n",
      "iter: 9.0, RMSE on training set: 0.9118571199081827.\n",
      "iter: 10.0, RMSE on training set: 0.9108320951692283.\n",
      "iter: 11.0, RMSE on training set: 0.9099546483600484.\n",
      "iter: 12.0, RMSE on training set: 0.9092069862186835.\n",
      "iter: 13.0, RMSE on training set: 0.9085747001616751.\n",
      "iter: 14.0, RMSE on training set: 0.9080453896708705.\n",
      "iter: 15.0, RMSE on training set: 0.90760812936007.\n",
      "iter: 16.0, RMSE on training set: 0.9072531789131099.\n",
      "iter: 17.0, RMSE on training set: 0.9069717831446577.\n",
      "iter: 18.0, RMSE on training set: 0.9067560332290401.\n",
      "iter: 19.0, RMSE on training set: 0.9065987636964438.\n",
      "iter: 20.0, RMSE on training set: 0.9064934717466547.\n",
      "iter: 21.0, RMSE on training set: 0.9064342511401028.\n",
      "RMSE on test data: 1.1673766428946455.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4118740227361428.\n",
      "iter: 1.0, RMSE on training set: 0.9804082987514858.\n",
      "iter: 2.0, RMSE on training set: 0.9357632822329053.\n",
      "iter: 3.0, RMSE on training set: 0.9260791618190061.\n",
      "iter: 4.0, RMSE on training set: 0.9219465996694917.\n",
      "iter: 5.0, RMSE on training set: 0.9193188180861137.\n",
      "iter: 6.0, RMSE on training set: 0.9173148006041264.\n",
      "iter: 7.0, RMSE on training set: 0.9156685842420499.\n",
      "iter: 8.0, RMSE on training set: 0.9142750959766897.\n",
      "iter: 9.0, RMSE on training set: 0.9130819384482858.\n",
      "iter: 10.0, RMSE on training set: 0.9120573608445552.\n",
      "iter: 11.0, RMSE on training set: 0.9111790688875985.\n",
      "iter: 12.0, RMSE on training set: 0.9104306732986107.\n",
      "iter: 13.0, RMSE on training set: 0.9097973114280922.\n",
      "iter: 14.0, RMSE on training set: 0.9092667428304285.\n",
      "iter: 15.0, RMSE on training set: 0.9088281531149759.\n",
      "iter: 16.0, RMSE on training set: 0.908471859024908.\n",
      "iter: 17.0, RMSE on training set: 0.9081891412260714.\n",
      "iter: 18.0, RMSE on training set: 0.9079721131761072.\n",
      "iter: 19.0, RMSE on training set: 0.9078136227721274.\n",
      "iter: 20.0, RMSE on training set: 0.9077071744319338.\n",
      "iter: 21.0, RMSE on training set: 0.9076468648366955.\n",
      "RMSE on test data: 1.1535195820374953.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4110983469149245.\n",
      "iter: 1.0, RMSE on training set: 0.979860589835064.\n",
      "iter: 2.0, RMSE on training set: 0.935299286923896.\n",
      "iter: 3.0, RMSE on training set: 0.9255948533476003.\n",
      "iter: 4.0, RMSE on training set: 0.9214369025862765.\n",
      "iter: 5.0, RMSE on training set: 0.9187881980923764.\n",
      "iter: 6.0, RMSE on training set: 0.9167680114924742.\n",
      "iter: 7.0, RMSE on training set: 0.9151089013655578.\n",
      "iter: 8.0, RMSE on training set: 0.9137046236153146.\n",
      "iter: 9.0, RMSE on training set: 0.9125020322613646.\n",
      "iter: 10.0, RMSE on training set: 0.9114689508362349.\n",
      "iter: 11.0, RMSE on training set: 0.910583437945473.\n",
      "iter: 12.0, RMSE on training set: 0.9098278707706733.\n",
      "iter: 13.0, RMSE on training set: 0.9091879499584578.\n",
      "iter: 14.0, RMSE on training set: 0.9086513537587356.\n",
      "iter: 15.0, RMSE on training set: 0.9082072112071317.\n",
      "iter: 16.0, RMSE on training set: 0.9078458160808431.\n",
      "iter: 17.0, RMSE on training set: 0.9075584313567151.\n",
      "iter: 18.0, RMSE on training set: 0.9073371541567735.\n",
      "iter: 19.0, RMSE on training set: 0.9071748158936924.\n",
      "iter: 20.0, RMSE on training set: 0.9070649041397747.\n",
      "iter: 21.0, RMSE on training set: 0.9070014985112278.\n",
      "RMSE on test data: 1.1665617308964573.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4092285184463733.\n",
      "iter: 1.0, RMSE on training set: 0.9796745427941038.\n",
      "iter: 2.0, RMSE on training set: 0.9350213631873409.\n",
      "iter: 3.0, RMSE on training set: 0.9253246257685157.\n",
      "iter: 4.0, RMSE on training set: 0.9211736691293856.\n",
      "iter: 5.0, RMSE on training set: 0.9185256613824117.\n",
      "iter: 6.0, RMSE on training set: 0.91650348488666.\n",
      "iter: 7.0, RMSE on training set: 0.9148421497563604.\n",
      "iter: 8.0, RMSE on training set: 0.9134363669280305.\n",
      "iter: 9.0, RMSE on training set: 0.9122332171686548.\n",
      "iter: 10.0, RMSE on training set: 0.9112008318978136.\n",
      "iter: 11.0, RMSE on training set: 0.9103165175305372.\n",
      "iter: 12.0, RMSE on training set: 0.9095627234052734.\n",
      "iter: 13.0, RMSE on training set: 0.9089249306674798.\n",
      "iter: 14.0, RMSE on training set: 0.9083906480142663.\n",
      "iter: 15.0, RMSE on training set: 0.907948897732539.\n",
      "iter: 16.0, RMSE on training set: 0.9075898980110939.\n",
      "iter: 17.0, RMSE on training set: 0.9073048587650003.\n",
      "iter: 18.0, RMSE on training set: 0.9070858403919312.\n",
      "iter: 19.0, RMSE on training set: 0.906925649531697.\n",
      "iter: 20.0, RMSE on training set: 0.906817757661423.\n",
      "iter: 21.0, RMSE on training set: 0.9067562345641594.\n",
      "RMSE on test data: 1.1607104343458534.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 1.4109932956498352.\n",
      "iter: 1.0, RMSE on training set: 0.9799597273609414.\n",
      "iter: 2.0, RMSE on training set: 0.9353023537522481.\n",
      "iter: 3.0, RMSE on training set: 0.925605940926342.\n",
      "iter: 4.0, RMSE on training set: 0.9214648716834705.\n",
      "iter: 5.0, RMSE on training set: 0.9188294064077898.\n",
      "iter: 6.0, RMSE on training set: 0.9168190964300267.\n",
      "iter: 7.0, RMSE on training set: 0.9151682427400748.\n",
      "iter: 8.0, RMSE on training set: 0.9137716562110223.\n",
      "iter: 9.0, RMSE on training set: 0.9125766206708936.\n",
      "iter: 10.0, RMSE on training set: 0.9115513940255171.\n",
      "iter: 11.0, RMSE on training set: 0.9106734276662537.\n",
      "iter: 12.0, RMSE on training set: 0.9099252687805128.\n",
      "iter: 13.0, RMSE on training set: 0.9092924696703667.\n",
      "iter: 14.0, RMSE on training set: 0.9087626019837088.\n",
      "iter: 15.0, RMSE on training set: 0.9083247438404168.\n",
      "iter: 16.0, RMSE on training set: 0.9079691635680693.\n",
      "iter: 17.0, RMSE on training set: 0.9076871161904297.\n",
      "iter: 18.0, RMSE on training set: 0.9074707026481832.\n",
      "iter: 19.0, RMSE on training set: 0.9073127659947897.\n",
      "iter: 20.0, RMSE on training set: 0.9072068104362786.\n",
      "iter: 21.0, RMSE on training set: 0.9071469352356406.\n",
      "RMSE on test data: 1.1581407807137525.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKRKFBgiIlIlCgwREakSBYaIiFTJ/wfmmISHqsss6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b495be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 3     # 0-SGD 1-ALS\n",
    "K = 10         ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features_arr = [1, 4, 7, 10, 13, 16, 20]   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.7\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(num_features_arr))\n",
    "train_rmse_std = np.zeros(len(num_features_arr))\n",
    "validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "for i, num_features in enumerate(num_features_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running num_features={n}'.format(n=num_features))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation_minimalist(ratings, method, K, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    ## Calculate mean and standard deviation    \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(num_features_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(num_features_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Number of features (K)'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121a9d9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('ccdplus_CV_num_features.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tab = pd.DataFrame()\n",
    "tab['numFeatures'] = num_features_arr;\n",
    "tab['train RMSE mean'] = train_rmse_mean\n",
    "tab['train RMSE std'] = train_rmse_std\n",
    "tab['test RMSE mean'] = validation_rmse_mean\n",
    "tab['test RMSE std'] = validation_rmse_std\n",
    "tab.to_csv('ccdplus_CV_num_features.csv', sep = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lambda_user=0.01\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905120402314358.\n",
      "iter: 1.0, RMSE on training set: 0.9905120402314358.\n",
      "RMSE on test data: 1.0006561439583412.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904092461006758.\n",
      "iter: 1.0, RMSE on training set: 0.9904092461006758.\n",
      "RMSE on test data: 1.001538376604576.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990477750872439.\n",
      "iter: 1.0, RMSE on training set: 0.990477750872439.\n",
      "RMSE on test data: 1.0009871106146258.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990633808868161.\n",
      "iter: 1.0, RMSE on training set: 0.990633808868161.\n",
      "RMSE on test data: 0.9997944327774473.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903019178461998.\n",
      "iter: 1.0, RMSE on training set: 0.9903019178461998.\n",
      "RMSE on test data: 1.002713425327614.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901511335445377.\n",
      "iter: 1.0, RMSE on training set: 0.9901511335445377.\n",
      "RMSE on test data: 1.0038517055431884.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9912598091680765.\n",
      "iter: 1.0, RMSE on training set: 0.9912598091680765.\n",
      "RMSE on test data: 0.9940159328051871.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903600395912844.\n",
      "iter: 1.0, RMSE on training set: 0.9903600395912844.\n",
      "RMSE on test data: 1.0020550722113446.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.99028327767053.\n",
      "iter: 1.0, RMSE on training set: 0.99028327767053.\n",
      "RMSE on test data: 1.0027100978914476.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909656779879887.\n",
      "iter: 1.0, RMSE on training set: 0.9909656779879887.\n",
      "RMSE on test data: 0.9967427881955783.\n",
      "Running lambda_user=0.05\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905120678908838.\n",
      "iter: 1.0, RMSE on training set: 0.9905120678908838.\n",
      "RMSE on test data: 1.0006562255005609.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904092735902689.\n",
      "iter: 1.0, RMSE on training set: 0.9904092735902689.\n",
      "RMSE on test data: 1.0015383855551212.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904777785137003.\n",
      "iter: 1.0, RMSE on training set: 0.9904777785137003.\n",
      "RMSE on test data: 1.0009870631481883.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906338363526014.\n",
      "iter: 1.0, RMSE on training set: 0.9906338363526014.\n",
      "RMSE on test data: 0.9997944110116537.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903019453740118.\n",
      "iter: 1.0, RMSE on training set: 0.9903019453740118.\n",
      "RMSE on test data: 1.0027133598519016.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901511610289252.\n",
      "iter: 1.0, RMSE on training set: 0.9901511610289252.\n",
      "RMSE on test data: 1.0038517039961792.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9912598366037527.\n",
      "iter: 1.0, RMSE on training set: 0.9912598366037527.\n",
      "RMSE on test data: 0.9940159836548446.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990360067098852.\n",
      "iter: 1.0, RMSE on training set: 0.990360067098852.\n",
      "RMSE on test data: 1.0020552256487638.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902833052038181.\n",
      "iter: 1.0, RMSE on training set: 0.9902833052038181.\n",
      "RMSE on test data: 1.0027100848206405.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909657055289416.\n",
      "iter: 1.0, RMSE on training set: 0.9909657055289416.\n",
      "RMSE on test data: 0.996742840051587.\n",
      "Running lambda_user=0.1\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905121029046234.\n",
      "iter: 1.0, RMSE on training set: 0.9905121029046234.\n",
      "RMSE on test data: 1.000656327834608.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990409308393274.\n",
      "iter: 1.0, RMSE on training set: 0.990409308393274.\n",
      "RMSE on test data: 1.001538397168614.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990477813505825.\n",
      "iter: 1.0, RMSE on training set: 0.990477813505825.\n",
      "RMSE on test data: 1.0009870042414368.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906338711523204.\n",
      "iter: 1.0, RMSE on training set: 0.9906338711523204.\n",
      "RMSE on test data: 0.9997943842790069.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990301980235211.\n",
      "iter: 1.0, RMSE on training set: 0.990301980235211.\n",
      "RMSE on test data: 1.00271327856126.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901511958290181.\n",
      "iter: 1.0, RMSE on training set: 0.9901511958290181.\n",
      "RMSE on test data: 1.0038517025209015.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9912598713430716.\n",
      "iter: 1.0, RMSE on training set: 0.9912598713430716.\n",
      "RMSE on test data: 0.9940160476864421.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903601019297815.\n",
      "iter: 1.0, RMSE on training set: 0.9903601019297815.\n",
      "RMSE on test data: 1.0020554179246537.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902833400636146.\n",
      "iter: 1.0, RMSE on training set: 0.9902833400636146.\n",
      "RMSE on test data: 1.002710068930317.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909657403999528.\n",
      "iter: 1.0, RMSE on training set: 0.9909657403999528.\n",
      "RMSE on test data: 0.9967429053437655.\n",
      "Running lambda_user=0.5\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905124005556959.\n",
      "iter: 1.0, RMSE on training set: 0.9905124005556959.\n",
      "RMSE on test data: 1.0006571627313472.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990409604421394.\n",
      "iter: 1.0, RMSE on training set: 0.990409604421394.\n",
      "RMSE on test data: 1.0015385070600757.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904781110286278.\n",
      "iter: 1.0, RMSE on training set: 0.9904781110286278.\n",
      "RMSE on test data: 1.0009865500120179.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906341672750579.\n",
      "iter: 1.0, RMSE on training set: 0.9906341672750579.\n",
      "RMSE on test data: 0.9997941893360726.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903022771393979.\n",
      "iter: 1.0, RMSE on training set: 0.9903022771393979.\n",
      "RMSE on test data: 1.0027126503257677.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901514919745998.\n",
      "iter: 1.0, RMSE on training set: 0.9901514919745998.\n",
      "RMSE on test data: 1.003851709011782.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9912601670068182.\n",
      "iter: 1.0, RMSE on training set: 0.9912601670068182.\n",
      "RMSE on test data: 0.9940165786716683.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903603983941611.\n",
      "iter: 1.0, RMSE on training set: 0.9903603983941611.\n",
      "RMSE on test data: 1.002056975233352.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902836366326417.\n",
      "iter: 1.0, RMSE on training set: 0.9902836366326417.\n",
      "RMSE on test data: 1.0027099597028795.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909660371231904.\n",
      "iter: 1.0, RMSE on training set: 0.9909660371231904.\n",
      "RMSE on test data: 0.9967434465312713.\n",
      "Running lambda_user=1\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905128162989016.\n",
      "iter: 1.0, RMSE on training set: 0.9905128162989016.\n",
      "RMSE on test data: 1.000658246785128.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904100182911897.\n",
      "iter: 1.0, RMSE on training set: 0.9904100182911897.\n",
      "RMSE on test data: 1.001538686743703.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904785267226357.\n",
      "iter: 1.0, RMSE on training set: 0.9904785267226357.\n",
      "RMSE on test data: 1.000986024655221.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906345815407502.\n",
      "iter: 1.0, RMSE on training set: 0.9906345815407502.\n",
      "RMSE on test data: 0.9997939926407559.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903026931010065.\n",
      "iter: 1.0, RMSE on training set: 0.9903026931010065.\n",
      "RMSE on test data: 1.0027119199188623.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901519063290504.\n",
      "iter: 1.0, RMSE on training set: 0.9901519063290504.\n",
      "RMSE on test data: 1.0038517626366654.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9912605807687651.\n",
      "iter: 1.0, RMSE on training set: 0.9912605807687651.\n",
      "RMSE on test data: 0.9940172890014812.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903608133162772.\n",
      "iter: 1.0, RMSE on training set: 0.9903608133162772.\n",
      "RMSE on test data: 1.0020589693214763.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902840513932656.\n",
      "iter: 1.0, RMSE on training set: 0.9902840513932656.\n",
      "RMSE on test data: 1.002709867750834.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990966452234093.\n",
      "iter: 1.0, RMSE on training set: 0.990966452234093.\n",
      "RMSE on test data: 0.9967441699643542.\n",
      "Running lambda_user=5\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905178548770419.\n",
      "iter: 1.0, RMSE on training set: 0.9905178548770419.\n",
      "RMSE on test data: 1.000668510639178.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904150477000061.\n",
      "iter: 1.0, RMSE on training set: 0.9904150477000061.\n",
      "RMSE on test data: 1.0015417888143956.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990483569256478.\n",
      "iter: 1.0, RMSE on training set: 0.990483569256478.\n",
      "RMSE on test data: 1.0009834924040089.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906396209759132.\n",
      "iter: 1.0, RMSE on training set: 0.9906396209759132.\n",
      "RMSE on test data: 0.9997942390726388.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903077736887939.\n",
      "iter: 1.0, RMSE on training set: 0.9903077736887939.\n",
      "RMSE on test data: 1.0027082065772994.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901569506454073.\n",
      "iter: 1.0, RMSE on training set: 0.9901569506454073.\n",
      "RMSE on test data: 1.0038539688350838.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9912656205083487.\n",
      "iter: 1.0, RMSE on training set: 0.9912656205083487.\n",
      "RMSE on test data: 0.9940247900194299.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903658668750077.\n",
      "iter: 1.0, RMSE on training set: 0.9903658668750077.\n",
      "RMSE on test data: 1.002076761580741.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902890961771723.\n",
      "iter: 1.0, RMSE on training set: 0.9902890961771723.\n",
      "RMSE on test data: 1.0027108841003567.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909715054301408.\n",
      "iter: 1.0, RMSE on training set: 0.9909715054301408.\n",
      "RMSE on test data: 0.9967517999687288.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905282781820027.\n",
      "iter: 1.0, RMSE on training set: 0.9905282781820027.\n",
      "RMSE on test data: 1.0006851983767115.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904254723793929.\n",
      "iter: 1.0, RMSE on training set: 0.9904254723793929.\n",
      "RMSE on test data: 1.0015496969500484.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904940079285858.\n",
      "iter: 1.0, RMSE on training set: 0.9904940079285858.\n",
      "RMSE on test data: 1.0009843795854074.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906500609991347.\n",
      "iter: 1.0, RMSE on training set: 0.9906500609991347.\n",
      "RMSE on test data: 0.9997988498374581.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903183274812254.\n",
      "iter: 1.0, RMSE on training set: 0.9903183274812254.\n",
      "RMSE on test data: 1.002708604177134.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901674127158822.\n",
      "iter: 1.0, RMSE on training set: 0.9901674127158822.\n",
      "RMSE on test data: 1.0038609801563834.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.99127607618982.\n",
      "iter: 1.0, RMSE on training set: 0.99127607618982.\n",
      "RMSE on test data: 0.9940385121056141.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990376344076472.\n",
      "iter: 1.0, RMSE on training set: 0.990376344076472.\n",
      "RMSE on test data: 1.0021033478692372.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902995592396094.\n",
      "iter: 1.0, RMSE on training set: 0.9902995592396094.\n",
      "RMSE on test data: 1.0027163897267914.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909819902623261.\n",
      "iter: 1.0, RMSE on training set: 0.9909819902623261.\n",
      "RMSE on test data: 0.996765781094067.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109f78a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cross_validation import *\n",
    "## !!! Takes long time to run\n",
    "\n",
    "method = 3     # 0-SGD 1-ALS\n",
    "K = 10       ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 1   # K in the lecture notes\n",
    "lambda_user_arr = [0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
    "lambda_item = 0.75\n",
    "min_num_ratings=1\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "for i, lambda_user in enumerate(lambda_user_arr):\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation_minimalist(ratings, method, K, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b685f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('ccdplus_CV_lambda_user.png');\n",
    "import pandas as pd\n",
    "tab = pd.DataFrame()\n",
    "tab['lambda user'] = lambda_user_arr;\n",
    "tab['train RMSE mean'] = train_rmse_mean\n",
    "tab['train RMSE std'] = train_rmse_std\n",
    "tab['test RMSE mean'] = validation_rmse_mean\n",
    "tab['test RMSE std'] = validation_rmse_std\n",
    "tab.to_csv('ccdplus_CV_lambda_user.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903460936707355.\n",
      "iter: 1.0, RMSE on training set: 0.9903460936707355.\n",
      "RMSE on test data: 1.0006444836431792.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902435834073589.\n",
      "iter: 1.0, RMSE on training set: 0.9902435834073589.\n",
      "RMSE on test data: 1.001530368698387.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903120665219117.\n",
      "iter: 1.0, RMSE on training set: 0.9903120665219117.\n",
      "RMSE on test data: 1.0008828779363521.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904680360622357.\n",
      "iter: 1.0, RMSE on training set: 0.9904680360622357.\n",
      "RMSE on test data: 0.9996035584468832.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901368633465064.\n",
      "iter: 1.0, RMSE on training set: 0.9901368633465064.\n",
      "RMSE on test data: 1.0025373606618362.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9899860322558249.\n",
      "iter: 1.0, RMSE on training set: 0.9899860322558249.\n",
      "RMSE on test data: 1.0038120696241075.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.991094600154201.\n",
      "iter: 1.0, RMSE on training set: 0.991094600154201.\n",
      "RMSE on test data: 0.9939002093307102.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901944424871881.\n",
      "iter: 1.0, RMSE on training set: 0.9901944424871881.\n",
      "RMSE on test data: 1.0019673023870777.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901179415554942.\n",
      "iter: 1.0, RMSE on training set: 0.9901179415554942.\n",
      "RMSE on test data: 1.00254568872452.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9908001299243049.\n",
      "iter: 1.0, RMSE on training set: 0.9908001299243049.\n",
      "RMSE on test data: 0.9965602713330679.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990347162616726.\n",
      "iter: 1.0, RMSE on training set: 0.990347162616726.\n",
      "RMSE on test data: 1.0006372382079303.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902446493017473.\n",
      "iter: 1.0, RMSE on training set: 0.9902446493017473.\n",
      "RMSE on test data: 1.0015219427897253.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903131345871349.\n",
      "iter: 1.0, RMSE on training set: 0.9903131345871349.\n",
      "RMSE on test data: 1.0008791542628355.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904691028139256.\n",
      "iter: 1.0, RMSE on training set: 0.9904691028139256.\n",
      "RMSE on test data: 0.9996047585370386.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901379278100911.\n",
      "iter: 1.0, RMSE on training set: 0.9901379278100911.\n",
      "RMSE on test data: 1.0025376502823544.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9899870963355504.\n",
      "iter: 1.0, RMSE on training set: 0.9899870963355504.\n",
      "RMSE on test data: 1.0038054985657534.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9910956637679015.\n",
      "iter: 1.0, RMSE on training set: 0.9910956637679015.\n",
      "RMSE on test data: 0.9938985294469055.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901955087234721.\n",
      "iter: 1.0, RMSE on training set: 0.9901955087234721.\n",
      "RMSE on test data: 1.001965450366236.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901190068772123.\n",
      "iter: 1.0, RMSE on training set: 0.9901190068772123.\n",
      "RMSE on test data: 1.0025459588473649.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9908011966972924.\n",
      "iter: 1.0, RMSE on training set: 0.9908011966972924.\n",
      "RMSE on test data: 0.9965622258455776.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903499753492309.\n",
      "iter: 1.0, RMSE on training set: 0.9903499753492309.\n",
      "RMSE on test data: 1.0006297725431177.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902474562540743.\n",
      "iter: 1.0, RMSE on training set: 0.9902474562540743.\n",
      "RMSE on test data: 1.0015130094387352.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903159440425747.\n",
      "iter: 1.0, RMSE on training set: 0.9903159440425747.\n",
      "RMSE on test data: 1.0008760507242365.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904719121177245.\n",
      "iter: 1.0, RMSE on training set: 0.9904719121177245.\n",
      "RMSE on test data: 0.9996078397320834.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901407286698914.\n",
      "iter: 1.0, RMSE on training set: 0.9901407286698914.\n",
      "RMSE on test data: 1.0025395204602978.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9899898962706251.\n",
      "iter: 1.0, RMSE on training set: 0.9899898962706251.\n",
      "RMSE on test data: 1.0037988360305221.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9910984642804131.\n",
      "iter: 1.0, RMSE on training set: 0.9910984642804131.\n",
      "RMSE on test data: 0.9938979726324272.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901983157643901.\n",
      "iter: 1.0, RMSE on training set: 0.9901983157643901.\n",
      "RMSE on test data: 1.0019646854761484.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901218099819673.\n",
      "iter: 1.0, RMSE on training set: 0.9901218099819673.\n",
      "RMSE on test data: 1.0025478021810736.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9908040049465858.\n",
      "iter: 1.0, RMSE on training set: 0.9908040049465858.\n",
      "RMSE on test data: 0.9965662221423123.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904295293281908.\n",
      "iter: 1.0, RMSE on training set: 0.9904295293281908.\n",
      "RMSE on test data: 1.000631455949057.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903268811484419.\n",
      "iter: 1.0, RMSE on training set: 0.9903268811484419.\n",
      "RMSE on test data: 1.0015031368575227.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903953912582463.\n",
      "iter: 1.0, RMSE on training set: 0.9903953912582463.\n",
      "RMSE on test data: 1.0009110953447458.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905513987518497.\n",
      "iter: 1.0, RMSE on training set: 0.9905513987518497.\n",
      "RMSE on test data: 0.999693348391727.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902199598724662.\n",
      "iter: 1.0, RMSE on training set: 0.9902199598724662.\n",
      "RMSE on test data: 1.002612756450083.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9900690950950936.\n",
      "iter: 1.0, RMSE on training set: 0.9900690950950936.\n",
      "RMSE on test data: 1.0038054334833313.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9911777052724804.\n",
      "iter: 1.0, RMSE on training set: 0.9911777052724804.\n",
      "RMSE on test data: 0.9939530414998016.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902777409516008.\n",
      "iter: 1.0, RMSE on training set: 0.9902777409516008.\n",
      "RMSE on test data: 1.002018409326926.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902011069512259.\n",
      "iter: 1.0, RMSE on training set: 0.9902011069512259.\n",
      "RMSE on test data: 1.0026207529357845.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9908834259007623.\n",
      "iter: 1.0, RMSE on training set: 0.9908834259007623.\n",
      "RMSE on test data: 0.9966577581533331.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905282781820027.\n",
      "iter: 1.0, RMSE on training set: 0.9905282781820027.\n",
      "RMSE on test data: 1.0006851983767115.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904254723793929.\n",
      "iter: 1.0, RMSE on training set: 0.9904254723793929.\n",
      "RMSE on test data: 1.0015496969500484.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904940079285858.\n",
      "iter: 1.0, RMSE on training set: 0.9904940079285858.\n",
      "RMSE on test data: 1.0009843795854074.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906500609991347.\n",
      "iter: 1.0, RMSE on training set: 0.9906500609991347.\n",
      "RMSE on test data: 0.9997988498374581.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903183274812254.\n",
      "iter: 1.0, RMSE on training set: 0.9903183274812254.\n",
      "RMSE on test data: 1.002708604177134.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9901674127158822.\n",
      "iter: 1.0, RMSE on training set: 0.9901674127158822.\n",
      "RMSE on test data: 1.0038609801563834.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.99127607618982.\n",
      "iter: 1.0, RMSE on training set: 0.99127607618982.\n",
      "RMSE on test data: 0.9940385121056141.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990376344076472.\n",
      "iter: 1.0, RMSE on training set: 0.990376344076472.\n",
      "RMSE on test data: 1.0021033478692372.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9902995592396094.\n",
      "iter: 1.0, RMSE on training set: 0.9902995592396094.\n",
      "RMSE on test data: 1.0027163897267914.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9909819902623261.\n",
      "iter: 1.0, RMSE on training set: 0.9909819902623261.\n",
      "RMSE on test data: 0.996765781094067.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906624249225906.\n",
      "iter: 1.0, RMSE on training set: 0.9906624249225906.\n",
      "RMSE on test data: 1.0007769136994433.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905594030989507.\n",
      "iter: 1.0, RMSE on training set: 0.9905594030989507.\n",
      "RMSE on test data: 1.0016341746749107.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9906279781979498.\n",
      "iter: 1.0, RMSE on training set: 0.9906279781979498.\n",
      "RMSE on test data: 1.0010946971408892.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9907840844456849.\n",
      "iter: 1.0, RMSE on training set: 0.9907840844456849.\n",
      "RMSE on test data: 0.9999417523907047.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.990451976971243.\n",
      "iter: 1.0, RMSE on training set: 0.990451976971243.\n",
      "RMSE on test data: 1.002840599144725.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9903009843579793.\n",
      "iter: 1.0, RMSE on training set: 0.9903009843579793.\n",
      "RMSE on test data: 1.0039536108784712.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9914097183878791.\n",
      "iter: 1.0, RMSE on training set: 0.9914097183878791.\n",
      "RMSE on test data: 0.9941607695651464.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9905103031128515.\n",
      "iter: 1.0, RMSE on training set: 0.9905103031128515.\n",
      "RMSE on test data: 1.0022253089223727.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9904333265989589.\n",
      "iter: 1.0, RMSE on training set: 0.9904333265989589.\n",
      "RMSE on test data: 1.002848173231903.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9911158753055199.\n",
      "iter: 1.0, RMSE on training set: 0.9911158753055199.\n",
      "RMSE on test data: 0.9969102735254493.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9924253089193293.\n",
      "iter: 1.0, RMSE on training set: 0.9924253089193293.\n",
      "RMSE on test data: 1.0023185899185312.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9923208907548208.\n",
      "iter: 1.0, RMSE on training set: 0.9923208907548208.\n",
      "RMSE on test data: 1.003107922133257.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9923891756417598.\n",
      "iter: 1.0, RMSE on training set: 0.9923891756417598.\n",
      "RMSE on test data: 1.002713252652573.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9925458083556382.\n",
      "iter: 1.0, RMSE on training set: 0.9925458083556382.\n",
      "RMSE on test data: 1.001793028616732.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9922110557317946.\n",
      "iter: 1.0, RMSE on training set: 0.9922110557317946.\n",
      "RMSE on test data: 1.0045901853947397.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9920608206155175.\n",
      "iter: 1.0, RMSE on training set: 0.9920608206155175.\n",
      "RMSE on test data: 1.005468900392551.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9931671542062115.\n",
      "iter: 1.0, RMSE on training set: 0.9931671542062115.\n",
      "RMSE on test data: 0.9958790086684062.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9922718329586745.\n",
      "iter: 1.0, RMSE on training set: 0.9922718329586745.\n",
      "RMSE on test data: 1.0039698354719129.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9921935407382074.\n",
      "iter: 1.0, RMSE on training set: 0.9921935407382074.\n",
      "RMSE on test data: 1.0046049025179093.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9928752658589002.\n",
      "iter: 1.0, RMSE on training set: 0.9928752658589002.\n",
      "RMSE on test data: 0.998771137628146.\n",
      "Running lambda_user=10\n",
      "Running 1th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9927546750269677.\n",
      "iter: 1.0, RMSE on training set: 0.9927546750269677.\n",
      "RMSE on test data: 1.0026510484761395.\n",
      "Running 2th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9926507689235553.\n",
      "iter: 1.0, RMSE on training set: 0.9926507689235553.\n",
      "RMSE on test data: 1.0034063111942788.\n",
      "Running 3th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9927187188004509.\n",
      "iter: 1.0, RMSE on training set: 0.9927187188004509.\n",
      "RMSE on test data: 1.0030021641539733.\n",
      "Running 4th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9928751919615896.\n",
      "iter: 1.0, RMSE on training set: 0.9928751919615896.\n",
      "RMSE on test data: 1.0021116395387075.\n",
      "Running 5th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9925414861410753.\n",
      "iter: 1.0, RMSE on training set: 0.9925414861410753.\n",
      "RMSE on test data: 1.0048948918184724.\n",
      "Running 6th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9923917533207082.\n",
      "iter: 1.0, RMSE on training set: 0.9923917533207082.\n",
      "RMSE on test data: 1.0057719985770845.\n",
      "Running 7th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9934961574784493.\n",
      "iter: 1.0, RMSE on training set: 0.9934961574784493.\n",
      "RMSE on test data: 0.9962197799933942.\n",
      "Running 8th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9926013628791669.\n",
      "iter: 1.0, RMSE on training set: 0.9926013628791669.\n",
      "RMSE on test data: 1.0043541147467265.\n",
      "Running 9th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.992523663797008.\n",
      "iter: 1.0, RMSE on training set: 0.992523663797008.\n",
      "RMSE on test data: 1.0049214982566277.\n",
      "Running 10th fold in 10 folds\n",
      "learn the matrix factorization using CCD++...\n",
      "iter: 0.0, RMSE on training set: 0.9932045824489303.\n",
      "iter: 1.0, RMSE on training set: 0.9932045824489303.\n",
      "RMSE on test data: 0.9991259867386107.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-abd615fa13a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m## Plotting results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n\u001b[0;32m---> 29\u001b[0;31m                      train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n\u001b[0m\u001b[1;32m     30\u001b[0m plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n\u001b[1;32m     31\u001b[0m                      validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mfill_between\u001b[0;34m(x, y1, y2, where, interpolate, step, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   2887\u001b[0m         ret = ax.fill_between(x, y1, y2=y2, where=where,\n\u001b[1;32m   2888\u001b[0m                               \u001b[0minterpolate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2889\u001b[0;31m                               **kwargs)\n\u001b[0m\u001b[1;32m   2890\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1810\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1811\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1812\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1813\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mfill_between\u001b[0;34m(self, x, y1, y2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   4614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4615\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4616\u001b[0;31m             \u001b[0mwhere\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4618\u001b[0m             \u001b[0mwhere\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqbH/BTlbs8dE2Xm9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c59b5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cross_validation import *\n",
    "## !!! Takes long time to run\n",
    "\n",
    "method = 3     # 0-SGD 1-ALS\n",
    "K = 10       ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 1   # K in the lecture notes\n",
    "lambda_user_arr = 0.1\n",
    "lambda_item_arr = [0.01, 0.05, 0.1, 0.5, 0.75, 1, 5, 10]\n",
    "min_num_ratings=1\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "\n",
    "for i, lambda_item in enumerate(lambda_item_arr):\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation_minimalist(ratings, method, K, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_item_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_item_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJ4EACbtAZA1LqyKGxVrkutRYi4JLab3a\ngmC19Wdte1Gr9dbllgeh1FZaawXtfSj3ckUFl65qr1rBq+Gqt6wCBgRBCAgEcJdF1uTz++OcSSaT\nyTqZTJb38/GYR2a+Z5nvjHLe8z3f8/0ec3dEREQSkZbqCoiISPOnMBERkYQpTEREJGEKExERSZjC\nREREEqYwERGRhCU1TMxsnpntNbO3qllnjpltNrM1ZjYyqnycmW00s01mdnuc7X5iZqVm1j1Z9RcR\nkdpJdsvkEeCiqhaa2XhgiLt/EbgBeCgsTwMeDLcdBkwys1OitusHjAW2J6/qIiJSW0kNE3d/Hfik\nmlUmAI+F6y4DuphZNjAa2Ozu2939GPBUuG7E74B/TU6tRUSkrlLdZ9IX2BH1emdYVlU5ZvZ1YIe7\nFzZWJUVEpHptUl2BGFbtQrMOwF0Ep7hqtY2IiCRfqsNkF9A/6nW/sCwDGBCnfAgwEFhrZhaWrzKz\n0e7+fuzOzUwTj4mI1IO71+mHemOc5jKqbj08B3wHwMzGAJ+6+15gBfAFM8sxswxgIvCcu69z9xPd\nfbC7DyI4/TUqXpBEuLse7kyfPj3ldWgqD30X+i70XVT/qI+ktkzM7AkgDzjBzN4DphO0Otzd57r7\nC2Z2sZm9CxwEvkuwsMTMpgKLCAJvnrtviPMWjk5ziYikXFLDxN2vqsU6U6so/ztwcg3bDq5n1URE\npAGl+mouaSR5eXmprkKToe+inL6LcvouEmP1PT/WHJiZt+TPJyKSDGaG17EDPtVXc4lICzZw4EC2\nb9dEFU1VTk4O27Zta5B9qWUiIkkT/sJNdTWkClX996lPy0R9JiIikjCFiYiIJExhIiIiCVOYiIgk\nqLS0lE6dOrFz585UVyVlFCYi0up06tSJzp0707lzZ9LT08nMzCwre/LJJ+u8v7S0NPbv30+/fv2S\nUNvmQVdziUjSVHW10PaiIuZPm0bprl2k9e3LtTNnkjNoUJ323RD7ABg8eDDz5s3j/PPPr3KdkpIS\n0tPT67zvpq4hr+bSOBMRaVTbi4p4YOxYZmzZQhbBpHzTly7lxsWLax0GDbGPiHiTG06bNo3NmzeT\nlpbG888/zwMPPMBJJ53ELbfcwsaNG8nMzOSKK67gvvvuIz09nZKSEtq2bcu2bdsYMGAAV199Nd27\nd2fz5s28/vrr5Obm8sQTT5CTk1OnujUnOs0lIo1q/rRpZSEAkAXM2LKF+dOmNeo+avLMM88wZcoU\nPvvsM7797W/Ttm1b5syZw8cff8wbb7zBSy+9xMMPP1y2fnBXjHJPPvkkd999N5988gn9+/dnWgPW\nrSlSmIhIoyrdtassBCKygNKFC8GsVo/ShQvj76O4uMHqec4553DxxRcD0K5dO770pS/x5S9/GTNj\n4MCBXH/99SxZsqRs/djWzRVXXMGoUaNIT09n8uTJrFmzpsHq1hQpTESkUaX17cvBmLKDQNrkyeBe\nq0fa5Mnx99GnT4PVs3///hVev/POO1x66aX07t2bLl26MH36dD788MMqtz/xxBPLnmdmZnLgwIEG\nq1tTpDARkUZ17cyZTB8ypCwMDgLThwzh2pkzG3UfNYk9bXXDDTeQm5vL1q1b+eyzz5gxY0aLmyqm\naFsRU26aUq9t1QEvIo0qZ9Agbly8mHunTaO0uJi0Pn24sY5XYjXEPupq//79dOnShQ4dOrBhwwYe\nfvjhFnUpcNG2IsZOHcuWEVvqtb3CREQaXc6gQUxfsCDl+4DKLZCq/Pa3v+UHP/gBv/zlLzn99NOZ\nOHEir7/+etz91HafTcHaPWs5fPwwP5v5syBIMuq3H40zEZGk0azBTZuZMfTBobRv0553//wu+8/e\nHyzIR7MGi4hI7b3xvTd45ZpXGH/SeDha//0oTEQaUaSD8/xrz2fKTVMo2laU6ipJK9etQze6tu/K\nPbfdw5C1Q+odKDrNJdJIKnRwZgBHYcjaISx+cDGDBiav4xigpLSEw8cPV/s4UnKk2uWHjh0K/h4/\nxJHjRzh0/FD8/Rw/wuGS4PmHP/1Qp7masNjTkEXbiph23zQWPrCwzqe5FCYijWTKTVNY2GlhxQ7O\no/C1PV/jp3f9tMaDffTBu6oDednBPAyHI8ePcKTkCKVeSrv0drRr04726e1p1yZ4HimLXZaRnhG3\nvGz98Hn7Nu2DdeOUt0tvx8jeIxUmTZjm5hJpJo6VHOPtD95mZfFKXi16Fc6IWSEDlu1cxi9e+0XF\nA3vMQb5dm3Z0zOhIj8welZZlpGeUHbwrBEBUWZu0NqRZGoYFf80qvI4uq2q95nSFkjS+pIaJmc0D\nLgX2uvvwKtaZA4wnGHd0rbuvCcvHAfcT9OvMc/dZYfmvgcuAI8AW4Lvuvi+Zn0OkNiLBsWr3KlYW\nr2RF8Qre/uBt+nbqS252LidknUDx0eJKLZPxXxzPf076z0oH9dgDvUhTltTTXGZ2DnAAeCxemJjZ\neGCqu19iZmcCs919jJmlAZuAC4BiYAUw0d03mtnXgFfcvdTM7gHc3e+s4v11mkuS4njp8bIWR+Sx\n/oP19OnUh+G9hjP8xOEM7zWcYb2G0aVdF7Iystizcw/jbhqXkj6TVNGlwU1bQ57mSnqfiZnlAH+r\nIkweAl5196fD1xuAPGAQMN3dx4fldxCExqyY7b8B/LO7X13FeytMJGGR4FhVvIqVu1eyYteKCsGR\nm53L8OzhnNbrtLLg6NCmA+lple9/EengLN5XTJ/OfZh568wWGySgMGnqWlKfSV9gR9TrnWFZvPLR\ncbb/HvBU0monrU5scKwsXsm699fRu2NvhmcPZ3j2cO44546y4Mhsm0lm28y4wRHPoIGDWDAn8VHb\nkjrbt29n0KBBHD9+nLS0NC6++GImTZrE1VdX/k0bu25d/epXv6KoqIi5c+c2RNWTKtVhEqvWSWhm\n/wYcc/cnqlsvPz+/7HleXh55eXn1rZu0MMdLj7Phgw2s2r2KFcUrKgVHbnYut599e72DQ5qu8ePH\nc+aZZ1Y4PgA8++yz/OAHP2DXrl3VHvyj+7BeeOGFat+rtv1dS5YsYcqUKezYUf47+s47457Bb3AF\nBQUUFBQktI9Uh8kuIHqe535hWQYwIE45AGZ2LXAx8NWa3iD2fxZpnaKDI9I5vv799WR3zC7r4/jp\nWT/ltF6n0bV9VwVHkkVO9+3at4u+nfvW63RfIvu45ppr+NnPflbp+LBgwQKuvvrqerUiEuXuKbvQ\nIvaH9owZM+q+k8gtK5P1AAYChVUsuxh4Pnw+BlgaPk8H3gVyCIJlDTA0XDYOWA+cUIv3dml9jpUc\n88K9hT5/9Xz/l+f/xc/8jzM96+4sHzx7sH/jyW/4tFem+Z/W/8k3frDR9+zf4/sO7/PjJcdTXe0W\nKd6/wa1FW33IJUOcu3Dyce7Ch1wyxLcWba31fhPdx6FDh7xr167+2muvlZV98skn3r59e3/rrbf8\n+eef91GjRnnnzp19wIABnp+fX7betm3bPC0tzUtKStzdPS8vz+fNm+fu7iUlJf6Tn/zEe/To4UOG\nDPHf//73FdZ95JFHfOjQod6pUycfMmSIP/zww+7ufvDgQe/QoYOnp6d7x44dvVOnTr57927Pz8/3\nKVOmlL33s88+68OGDfNu3br5+eef7xs2bChbNnDgQL/33nt9+PDh3rVrV584caIfOXKk2u+hqmNk\nWF6nY32yLw1+gqBD/QQzew+YHoaDu/tcd3/BzC42s3cJLg3+bpgAJWY2FVhE+aXBG8LdPhDuY3GY\n4kvd/UfJ/BzSdB0vPc7GDzdW6Bxf9/46emX1KuvjuO2s2zit12l0a99NLY4mYNp90yrOTpsBW0Zs\nYdp902rdn5ToPtq3b8+VV17JY489xjnnnAPA008/zdChQ8nNzeXjjz/m8ccfZ9iwYaxbt46xY8cy\natQovv71r1e737lz5/LCCy+wdu1aMjMzufzyyyssz87O5oUXXmDgwIG89tprjBs3jtGjRzNy5Ehe\nfPFFrr76at57770K20RaK5s2beKqq67iueee47zzzuO+++7jsssuY8OGDbRpExzK//jHP7Jo0SLa\ntWvHWWedxfz58/n+979fm680YUkNE3e/qhbrTK2i/O/AyXHKv9gAVZNmqKS0JAiOqD6Owr2FCo5m\nZte+XXBCTGEGLHxrIQtnLKzdTt4Czq+8j+J9tb9t7zXXXMOll17Kgw8+SEZGBo8//jjXXHMNAOed\nd17ZeqeddhoTJ05kyZIlNYbJH//4R3784x/TJ7zj45133lnh1r7jx48ve37uuedy4YUX8tprrzFy\n5Mga6/uHP/yBSy+9lK9+NTi7f9tttzF79mz+7//+j6985SsA3HzzzWRnZwNw2WWXNeqtglPdZyIS\nV3RwRPo4CvcW0jOrJyOyRzA8ezi3jrmV3OxcurbvSlbbLAVHM9G3c99gMsGYwZuTh09mwfTatUym\nfDSFhUcrT03Tp3Ptb9t79tln07NnT5555hnOOOMMVqxYwV//+lcAli1bxp133sm6des4evQoR48e\n5corr6xxn8XFxRVu95uTk1Nh+YsvvsjPf/5zNm3aRGlpKYcOHWL48LjjuePuO3p/Zkb//v3Ztaus\nO7ksSCC4VfDu3btrte+GoDCRlCspLeGdj96pMADwrb1v0TOrZ9Di6BUEx2m9TqNbh24KjmZu5q0z\nWTp1aaXBmzMfrP0tdxtiHwBXX301jz76KBs3buSiiy6iZ8+eAEyePJmbbrqJl156ibZt23LLLbfw\n0Ucf1bi/3r17V7gaa/v27WXPjx49yhVXXMGCBQuYMGECaWlpfPOb3ywb51FT53ufPn1Yt25dhbId\nO3Y0mbs9KkykUUWCI7qP4629b9Ezs2fZqPEfj/kxub1yFRwt1KCBg1j84OKKgzcfrNvVXA2xD4Dv\nfOc7/OIXv6CwsJDf/e53ZeUHDhygW7dutG3bluXLl/PEE09w0UUXlS2PBECsb33rW8yZM4dLLrmE\nzMxMZs0qH2cdaeH06NGDtLQ0XnzxRRYtWkRubi4QtCo++ugj9u3bR+fOnePue9asWbz66quce+65\n3H///bRv355/+qd/qtNnThaFiSRNSWkJmz7aFLQ2wgGAa/espWdmz7JR4wqO1qkhBm82xD5ycnI4\n66yzKCwsrNAf8u///u/ceuutTJ06lfPOO49vf/vbfPrpp2XLq7pF7/XXX8/mzZsZMWIEXbp04bbb\nbuPVV18FoGPHjsyZM4crr7ySo0ePctlllzFhwoSybU8++WQmTZrE4MGDKS0t5e23365Q15NOOokF\nCxYwdepUiouLGTlyJH/729/KOt9TPX+bpqCXBhEJjug+jrV71tIjs0dZ53hudi65vXLp3qE7mW0z\nyWqbpeBo4TSdStPWrObmSiWFSXLEBsfK4pWs2bOGHpk9ghZHOAhQwSEKk6ZNYVJLCpPElXppEBzF\nUS2OvWvp3qF7Wed4pMVxQuYJCg6pQGHStClMaklhUjfRwREZy7Fmzxq6d+hObq9cRmSPqBQcmW0z\naZOmrjeJT2HStClMaklhUrVSL2XzR5sr9HGs2bOGbu27lfVxRKZV75HZQ8Eh9aIwadoUJrWkMAlE\nB8eq4lUsL15eFhy52bllgwAVHNLQFCZNm8KkllpjmJR6Ke9+/G6FPo41e9bQpX2X8hZH2M9xQocT\nyMrIUnBI0ihMmraWdHMsSUB0cKzavYrlu5aXB0d4RdUPz/ihgkNSJicnJ+XjH6RqsdO9JEItk2ai\n1EvZ8vEWVhavLOscX717dVlwRAYBDs8eruAQkYToNFeM5homkeCI7ePolNGpQue4gkNEkkFhEiOV\nYVLbu8C5O1s+CVscxWGLY8/qsuAoGwSYPZwemT0UHCKSdAqTGKkKk6JtRYydOrbSjKaLHlhEaZfS\nCn0cq/espmNGx7I+jujgyGybSVZGloJDRBqVwiRGMsOkupbHlJumsLBT5XsttFnahl6X9Crr44gM\nAuyZ2VPBISJNhq7makDVhUWFlscJwFH4x7/8g1//26/ZxjZeevclODNmhxkwKnsUz17/rIJDRFqc\ntFRXINn6fakfPU7vQfbZ2Uy4bgJF24pq3CYSFgs7LaRgUAELOy1k7NSxZdvece8dle4/vXXkVq77\n+XUUvl9ITrec4E5y0Y7CST1Oonen3nRp30VBIiItSos/zcUY4KuU9V0MWDmAgocLqryJjrsz6cZJ\nPN3l6UqnqXqu6Una+Wm8//z7eF7l7+3sLWezaN4i9uzcw4U3Xlipz2Txg4vrfPMeEZHGptNc8USC\nhODve2e8x4W3XsiXrvoSnx35jM8Of8a+I/vYf3Q/+4/sZ//R/ZSsK4HzY/aTAT0zezL/qvn8cssv\neeboM5XCZmDXgWS2zWTwoMENchc4EZHmouWHSUbl16VeyrkDzqVju450yuhEx4zgb6d2wfNbi2/l\nr0f/WiksRvUexZf7fJn7br+PwqmF1d5/uiHuAici0ly0/NNcd1EpFCbvn1ztgb6qS3ujT1NFOujL\nWh5VjCMREWlumtylwWY2D7gU2Ovuw6tYZw4wHjgIXOvua8LyccD9BBcJzHP3WWF5N+BpIAfYBnzL\n3T+rYt917jOJUFiISGvVFMPkHOAA8Fi8MDGz8cBUd7/EzM4EZrv7GDNLAzYBFwDFwApgortvNLNZ\nwEfu/mszux3o5u53VPH+3vf0vhz2w7Tp0IYzTzmT+6fdr1AQEalGk+uAd/fXzay6aSknAI+F6y4z\nsy5mlg0MAja7+3YAM3sqXHdj+Pe8cPtHgQIgbpgA7Fy1M9GPISIiNUj1OJO+wI6o1zvDsqrKAbLd\nfS+Au+8BejVCPUVEpBpN7Wqu+tz4oNrzdPn5+WXP8/LyyMvLq8dbiIi0XAUFBRQUFCS0j6RfzRWe\n5vpbFX0mDwGvuvvT4euNBKewBgH57j4uLL8DcHefZWYbgDx332tmJ4bbD63ivZvlFPQiIqlUnz6T\nxjjNZVTd4ngO+A6AmY0BPg1PYa0AvmBmOWaWAUwM141sc234/Brg2STVW0REainZV3M9AeQRTIe4\nF5hOcJGuu/vccJ0HgXEElwZ/193fDMvHAbMpvzT4nrC8O/AHoD+wneDS4E+reH+1TERE6qjJXRqc\nagoTEZG6a6qnuUREpIVTmIiISMIUJiIikjCFiYiIJExhIiIiCVOYiIhIwhQmIiKSMIWJiIgkTGEi\nIiIJU5iIiEjCFCYiIpIwhYmIiCRMYSIiIglTmIiISMIUJiIikjCFiYiIJExhIiIiCVOYiIhIwhQm\nIiKSMIWJiIgkTGEiIiIJU5iIiEjCFCYiIpKwpIeJmY0zs41mtsnMbo+zvKuZ/cXM1prZUjM7NWrZ\nzWZWGD5uiiofYWb/MLPVZrbczM5I9ucQEZGqJTVMzCwNeBC4CBgGTDKzU2JWuwtY7e4jgGuAOeG2\nw4DrgDOAkcBlZjY43ObXwHR3HwVMB36TzM8hIiLVS3bLZDSw2d23u/sx4ClgQsw6pwKvALj7O8BA\nM+sJDAWWufsRdy8BlgCXh9uUAl3C512BXcn9GCIiUp1kh0lfYEfU651hWbS1hCFhZqOBAUA/YB1w\nrpl1M7NM4GKgf7jNLcC9ZvYeQSvlzqR9AhERqVGbVFcAuAeYbWZvAoXAaqDE3Tea2SxgMXAgUh5u\n80PgZnd/xsyuAP4LGBtv5/n5+WXP8/LyyMvLS9LHEBFpngoKCigoKEhoH+buDVObeDs3GwPku/u4\n8PUdgLv7rGq2KQJy3f1ATPndwA53f8jMPnX3rlHLPnP3LnH25cn8fCIiLZGZ4e5Wl22SfZprBfAF\nM8sxswxgIvBc9Apm1sXM2obPrweWRIIk7DvBzAYA3wQWhpvtMrPzwmUXAJuS/DlERKQaST3N5e4l\nZjYVWEQQXPPcfYOZ3RAs9rkEHe2PmlkpsJ7gCq6IP5tZd+AY8CN33x+WXw/MMbN04DDw/WR+DhER\nqV5ST3Olmk5ziYjUXYOf5jKzr0Y9HxSz7PLKW4iISGtUU5/JvVHP/xyz7GcNXBcREWmmagoTq+J5\nvNciItJK1RQmXsXzeK9FRKSVqulqrsFm9hxBKyTynPD1oKo3ExGR1qTaq7kiYzmq4u5LGrxGDUhX\nc4mI1F19ruaq06XB4eDC04Bd7v5+HevX6BQmIiJ1l4xLgx8Kp4LHzLoQTMr4GLDazCbVu6YiItKi\n1NQBf667rw+ffxfY5O65wJeAnya1ZiIi0mzUFCZHo56PBZ4BcPc9SauRiIg0OzWFyadmdqmZjQLO\nBv4OYGZtgA7JrpyIiDQPNV0afAPBbXRPBH4c1SK5AHg+mRUTEZHmQxM9iohIBfW5mqvalomZzalu\nubvfVJc3ExGRlqmm01w/ILgX+x+AYjQfl4iIxFFTmPQGrgS+DRwHngb+5O6fJrtiIiLSfFR7NZe7\nf+TuD7n7+QTjTLoCb5vZ1Y1SOxERaRZqddteMzsdmEQw1uRFYFUyKyUiIs1LTRM9/hy4BNgAPAX8\n3d2PN1LdEqaruURE6q7BJ3o0s1KgCPg8LIqsbIC7+/D6VLSxKExEROquwS8NRvcsERGRWqg2TNx9\ne7xyM0sj6EOJu1xERFqXmqag72xmd5rZg2Z2oQVuBLYC32qcKoqISFNX00SPjwMnA4XA/wNeBa4A\nvuHuE2rzBmY2zsw2mtkmM7s9zvKuZvYXM1trZkvN7NSoZTebWWH4uClmuxvNbEO47J7a1EVERJKj\npg74wvD+JZhZOrAbGODuh2u18+B02CaCiSGLgRXARHffGLXOr4H97j7TzE4Gfu/uXwtvyvUk8GWC\nAZN/B25w961mlgfcBVzs7sfNrIe7fxjn/dUBLyJSRw1+p0XgWOSJu5cAO2sbJKHRwGZ33+7uxwgu\nL45t0ZwKvBK+xzvAQDPrCQwFlrn7kfC9lwCXh9v8ELgncplyvCAREZHGU1OYjDCzfeFjPzA88tzM\n9tVi/32BHVGvd4Zl0dYShoSZjQYGAP0I5gQ718y6mVkmcDHQP9zmJOAr4WmxV83sjFrURUREkqSm\nq7nSG6EO9wCzzexNgr6Z1UCJu280s1nAYuBApDzcpg3Qzd3HmNmXCSaiHBxv5/n5+WXP8/LyyMvL\nS9LHEBFpngoKCigoKEhoH0m9n4mZjQHy3X1c+PoOgsGOs6rZpgjIdfcDMeV3Azvc/SEze5HgNNeS\ncNm7wJnu/lHMNuozERGpo2T0mSRqBfAFM8sxswxgIvBc9Apm1sXM2obPrweWRIIk7DvBzAYA3wSe\nCDf7K/DVcNlJQNvYIBERkcZTq4ke68vdS8xsKrCIILjmufsGM7shWOxzCTraHw2nblkPXBe1iz+b\nWXeCCwF+5O6RfppHgP8ys0LgCPCdZH4OERGpnm7bKyIiFTTF01wiItIKKExERCRhChMREUmYwkRE\nRBKmMBERkYQpTEREJGEKExERSZjCREREEqYwERGRhClMREQkYQoTERFJmMJEREQSpjAREZGEKUxE\nRCRhChMREUmYwkRERBKmMBERkYQpTEREJGEKExERSZjCREREEqYwERGRhClMREQkYQoTERFJWNLD\nxMzGmdlGM9tkZrfHWd7VzP5iZmvNbKmZnRq17GYzKwwfN8XZ9idmVmpm3ZP9OUREpGpJDRMzSwMe\nBC4ChgGTzOyUmNXuAla7+wjgGmBOuO0w4DrgDGAkcKmZDY7adz9gLLA9mZ9BRERqluyWyWhgs7tv\nd/djwFPAhJh1TgVeAXD3d4CBZtYTGAosc/cj7l4C/C9wedR2vwP+Ncn1FxGRWkh2mPQFdkS93hmW\nRVtLGBJmNhoYAPQD1gHnmlk3M8sELgb6h+t9Hdjh7oXJrb6IiNRGm1RXALgHmG1mbwKFwGqgxN03\nmtksYDFwIFJuZh0ITo2NjdqHVbXz/Pz8sud5eXnk5eU1dP1FRJq1goICCgoKEtqHuXvD1Cbezs3G\nAPnuPi58fQfg7j6rmm2KgFx3PxBTfjdBK+d14GXgc4IQ6QfsAka7+/sx23gyP5+ISEtkZrh7lT/S\n40l2y2QF8AUzywF2AxOBSdErmFkX4HN3P2Zm1wNLIkFiZj3d/QMzGwB8Exjj7vuAE6O2LwJOd/dP\nkvxZRESkCkkNE3cvMbOpwCKC/pl57r7BzG4IFvtcgo72R82sFFhPcAVXxJ/Dy36PAT8Kg6TS21DN\naS4REUm+pJ7mSjWd5hIRqbv6nObSCHgREUmYwkRERBKmMBERkYQpTEREJGEKExERSZjCREREANhe\nVMSMKVPqta0uDRYREbYXFfHA2LHM2LKFjtDkRsCLiEhDcYcjR+Dw4fo/Dh2q+Dd8Pn/lSmbs2UNW\nPaumMBERqS13OHq0YQ/mhw5VPLBHHtGhceRI8Dh6FDIyoF27+I/27Wu3rHv3SuWlW7aQtWdPvb8a\nhYmINB/ucOxYYgfzzz+v+hd6VQf0yMH8yBFo2zaxA3m7dtCtG5x4YvXbxSvPyID0dDCDtLTgEe95\ndcst/tmrtFde4eCGDfVumajPRKQRbS8qYv60aZTu2kVa375cO3MmOYMGpbpatecOx4/X/9d49Ov6\nHMwPH4Y2bWp34I0uq8sBu6ZgiD6Y1+egXsXBPNUS7TNRmIg0kuh/rFnAQWD6kCHcuHhx3QKlNgfz\n6g7QtTnFEu9gHvmbllb5wFuXX+X1PZi3b1/+y7wuv8SbycG8KYj82MlfuFBhEk1hIk3JjKuu4rYn\nn6xwGuEgcG9ODtNHjar9wdwseQfsmsozMoKWQV0P4DqYNytN8X4mIq2TO7z3HixfDitWwNKllL7x\nRqXz0VlAaUYGjB9f86/yuh7Mq1su0sAUJiIN4cMPg9BYsQKWLQv+AowcGTx++EPSOnfm4PPPV2qZ\npI0eDd//fipqLdJgdJpLpK4OHoTVq4NWx7Jlwd+PPoLcXBg1KgiPESOgTx/o2BEyM6Ft24brMxFJ\nsvqc5lIfa+xqAAAOXUlEQVSYiFTn2DFYvz4IjEh4bNkCJ59c3uoYORKGDAlCIysrOC1VhbKruYqL\nSevTp/ldzSWtgsIkhsJE6sQ9CIro4CgsDFoY0cExdCh06hQER/v26oOQFkdhEkNhItXas6dCBzkr\nVwati+jgGD4cunYNgiMzM+jAFmnhFCYxFCZSZt++ICyiO8gPHCjv34iER3Z2eXC00fUp0jopTGIo\nTFqpI0fgrbcqdpDv2AHDhgXBMWpU8HfgwPJ+joyMVNdapMlQmMRQmLQCpaXwzjsVg+Ptt4OgiL6y\n6uSTgyurIh3k6ucQqZLCJIbCpIVxh507K/ZzvPlmMANqdD/HaadB587lp6sUHCJ10iTDxMzGAfcT\n3NVxnrvPilneFfgvYAhwCPieu78dLrsZ+H/hqv/p7rPD8l8DlwFHgC3Ad919X5z3Vpg0Zx9/XD4Q\ncOnS4G9pacXgGDECevQoD4709FTXWqTZa3JhYmZpwCbgAqAYWAFMdPeNUev8Gtjv7jPN7GTg9+7+\nNTMbBjwJfBk4DvwduMHdt5rZ14BX3L3UzO4B3N3vjPP+CpPm4vPPg4GAkQ7y5cvh/feDgYCR4Bg1\nKrhMNysreLRtm+pai7RITXFurtHAZnffDmBmTwETgI1R65wK/ArA3d8xs4Fm1hMYCixz9yPhtkuA\ny4F73f3lqO2XAv+c5M8hDen48aBfI7qfY/NmOOmkoKUxejTccEMwEDASHNUMBBSR1Et2mPQFdkS9\n3kkQMNHWEoTEG2Y2GhgA9APWAb8ws24Ep7MuJmjZxPoe8FQD11saijsUFVXs51i7Fnr3Lr+yasIE\nOPXU8oGAHTqon0OkmWkKF9LfA8w2szeBQmA1UOLuG81sFrAYOBApj97QzP4NOObuT1S18/z8/LLn\neXl55OXlNXT9JdrevRX7OVauDC67jVxZddNNQYh066aBgCJNREFBAQUFBQntI9l9JmOAfHcfF76+\ng6B/Y1Y12xQBue5+IKb8bmCHuz8Uvr4WuB74auRUWJx9qc8kmfbvh1WrKg4E/OyzICyix3OceGL5\n6SoNBBRp8ppiB3w68A5BB/xuYDkwyd03RK3TBfjc3Y+Z2fXA2e5+bbisp7t/YGYDCDrgx7j7vvAK\nsd8CX3H3j6p5f4VJQzl6NBgIGN1Bvm1bcHoq+uqqgQPLg0MDAUWapSYXJlB2afBsyi8NvsfMbiBo\nocwNWy+PAqXAeuA6d/8s3PZ/ge7AMeAWdy8IyzcDGUAkSJa6+4/ivLfCpD5KS4MO8egO8vXrISen\nfOqRUaOCgYCR4NCEhyItRpMMk1RSmNTSrl0VZ8pdtSqY3DA6OE47Dbp0Ke8gVz+HSIulMImhMInj\nk08qTni4fHlwCisSGpEAiQwEzMrSQECRVkZhEqPVh8mhQ7BmTXlwLFsWTLseGQgY6STv108DAUWk\njMIkRqsKk5IS2LChYj/HO+/AF75QsYP8i18snym3fftU11pEmiCFSYwWGybusH17xX6O1auDe3FE\nB8epp5ZPeKiBgCJSSwqTGC0mTD74oHwgYKTV0aZN5TsCdu9efrpKHeQiUk8KkxjNMkwOHAimVY/u\n5/j00yAsosOjd28NBBSRpFCYxEhlmGwvKmL+tGmU7tpFWt++XDtzJjmDBlVc6dgxKCysGBxFRXDK\nKRWvrBo8uLyfQxMeikiSKUxipCpMthcV8cDYsczYsoUs4CAwffBgbpw7l5w9e8pPVRUWQv/+Fa+s\nOuWU8jsCaiCgiKSAwiRGMsOkupbHjClTuG3hQrKi1j8I3NuhA9MvuKD8VFVubvlAQE14KCJNRFO8\nn0mLFLfl8Y9/cOPMmeRs3Ejp3/5WIUgAsoDSUaNg4UINBBSRFkc/hauwvaiIGVOmMP3885kxZQrb\ni4rKls2fNq0sSCAIihlbtzJ/6lT45BPScnM5GLO/g0DaoEHBpboKEhFpYVp8y2R8v35kHD5Ml/R0\nsseMYer991fuCI8Rt+Xx8svceOWV5GzdSunLL8dveZx6KvzmN1y7ezfTY7cfMoQbZ85MymcUEUm1\nFh8mQ3ftYibBwf7gc89x15o13FpQEATKsWPBzZz27Akeu3fDnj3Mf/zxyi2PvXu5d/Fipt92G2nH\nj3Nw0aJKfSJpAwdC+/bkDBrEjYsXc++0aZQWF5PWp09wCqyGEBMRaa5afAf8AajcEd65M9PbtoV9\n++CEE6Bnz+DRqxf07Mn0//5vZkSd1oqYft55zCgoiN9yGTKEGxcvVmCISLOnDvg44p6OGjQIHnss\nuHVsRkYwuWHUI+3jjzlYVFS55dGvH4BaHiIiMVpny2TyZKYvWFDldmp5iEhrpnEmMczMb4HyPhPg\nrgEDyvtMqlE2jiRsecQdwS4i0gIpTGKYmY/r2ze4mqtNG7LPPLNWV3OJiLRmCpMYzXKiRxGRFKtP\nmGjQooiIJExhIiIiCVOYiIhIwpIeJmY2zsw2mtkmM7s9zvKuZvYXM1trZkvN7NSoZTebWWH4uCmq\nvJuZLTKzd8zsJTPrkuzPISIiVUtqmJhZGvAgcBEwDJhkZqfErHYXsNrdRwDXAHPCbYcB1wFnACOB\ny8xscLjNHcDL7n4y8ApwZzI/R0tQUFCQ6io0Gfouyum7KKfvIjHJbpmMBja7+3Z3PwY8BUyIWedU\ngkDA3d8BBppZT2AosMzdj7h7CbAEuDzcZgLwaPj8UeAbyf0YzZ/+oZTTd1FO30U5fReJSXaY9AV2\nRL3eGZZFW0sYEmY2GhgA9APWAeeGp7QygYuB/uE22e6+F8Dd9wC9kvYJRESkRk1hbq57gNlm9iZQ\nCKwGStx9o5nNAhYDByLlVexDg0lERFIoqYMWzWwMkO/u48LXdwDu7rOq2aYIyHX3AzHldwM73P0h\nM9sA5Ln7XjM7EXjV3YfG2ZdCRkSkHprarMErgC+YWQ6wG5gITIpeIbwS63N3P2Zm1wNLIkFiZj3d\n/QMzGwB8ExgTbvYccC0wi6DT/tl4b17XL0NEROon6dOpmNk4YDZB/8w8d7/HzG4gaKHMDVsvjwKl\nwHrgOnf/LNz2f4HuwDHgFncvCMu7A38g6EPZDnzL3T9N6gcREZEqtei5uUREpHG0yBHwNQ2UbC3M\nrJ+ZvWJm62MHfrZWZpZmZm+a2XOprksqmVkXM/ujmW0I//84M9V1ShUzu8XM1pnZW2a20MwyUl2n\nxmRm88xsr5m9FVVW54HhLS5MajlQsrU4Dtzq7sOAfwL+pRV/FxE3A2+nuhJNwGzghfDClRHAhhTX\nJyXMrA9wI3C6uw8n6EeemNpaNbpHCI6X0eo8MLzFhQm1GyjZKrj7HndfEz4/QHDAiB3n02qYWT+C\n8Ur/meq6pJKZdQbOdfdHANz9uLvvS3G1UikdyDKzNkAmUJzi+jQqd38d+CSmuM4Dw1timNRmoGSr\nY2YDCaalWZbamqTU74B/ReOSBgEfmtkj4Sm/uWbWIdWVSgV3LwZ+C7wH7AI+dfeXU1urJqFXXQeG\nt8QwkRhm1hH4E3Bz7Pid1sLMLgH2hi01Cx+tVRvgdOD37n468DnBaY1Wx8y6EvwKzwH6AB3N7KrU\n1qpJqvEHWEsMk10EU7JE9AvLWqWw6f4n4HF3jzsep5U4G/i6mW0FngTON7PHUlynVNlJMAB4Zfj6\nTwTh0hp9Ddjq7h+HcwD+BTgrxXVqCvaaWTZAODD8/Zo2aIlhUjZQMrwqYyLBIMfW6r+At919dqor\nkkrufpe7D3D3wQT/T7zi7t9Jdb1SITx9scPMTgqLLqD1XpTwHjDGzNqbmRF8F63xYoTY1npkYDhU\nMzA8WlOYm6tBuXuJmU0FFlE+ULI1/s+BmZ0NTAYKzWw1QVP1Lnf/e2prJk3ATcBCM2sLbAW+m+L6\npIS7LzezPxHM/Xcs/Ds3tbVqXGb2BJAHnGBm7wHTCeZM/KOZfY9wYHiN+9GgRRERSVRLPM0lIiKN\nTGEiIiIJU5iIiEjCFCYiIpIwhYmIiCRMYSIiIglTmEirZWb7k7DPovDmbQ363mb232bWOZw6/of1\nr6FIcihMpDVLxiCr2u6zTu/t7peGM/t2A35U51qJJJnCRCSKmV1qZkvNbFV4c6CeYfl0M5tvZv8b\ntj6+aWazwhsqvWBm6ZFdALeH5UvNbHC4/UAz+z8zW2tmM6PeL8vMXjazleGyr1dRr0iL51fA4HC2\n31nhstvMbLmZrTGz6WFZTnjjq0fCGxwtMLMLzOz18PUZyfsWpTVSmIhU9Jq7j3H3LwFPAz+NWjaY\nYNqJCcAC4H/CGyodBi6JWu+TsPz3BDehIvz7e3cfAeyOWvcw8A13PwP4KsF06PFEWjJ3AFvc/XR3\nv93MxgJfdPfRwCjgDDM7J1x3CPCb8AZHpwCT3P0cgmn4/632X4lIzVrc3FwiCepvZn8AegNtgaKo\nZS+6e6mZFQJp7r4oLC8EBkat91T490ngvvD52cDl4fPHCeY+gqAl8ysz+wpQCvQxs17uHjtLa1VT\n5l8IjDWzN8N1soAvEtzTp8jdIxM4rgf+J6q+OVXsT6ReFCYiFT0A3Ovuz5vZeQST3kUcAXB3N7Nj\nUeWlVPy35DU8jw6GyUAPYFQYVEVA+zrU14Bfuft/VCg0y4nUN6qOR6Ke69++NCid5pLWLN6v/c6U\n37b1mjpuG/Ht8O9E4B/h89eBSeHzyVHrdgHeD4PkfGpuMewHOkW9fgn4npllQXBP80g/Tw11bM03\nB5Mk0K8Tac06hFNuG0Gr4T4gH/iTmX0MvELF01fRqroay4FuZraWoD8kEiA/Bp4ws59S8d4QC4G/\nheuvpOp7aTiAu39sZm+Y2VsEp91uN7OhwD+C23GwH5hC0PqoqoVUXf1F6kVT0IuISMJ0mktERBKm\nMBERkYQpTEREJGEKExERSZjCREREEqYwERGRhClMREQkYQoTERFJ2P8HD4Jyj5qASYYAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1254cde10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.fill_between(lambda_item_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_item_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1254cd940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('ccdplus_CV_lambda_item.png');\n",
    "import pandas as pd\n",
    "tab = pd.DataFrame()\n",
    "tab['lambda item'] = lambda_item_arr;\n",
    "tab['train RMSE mean'] = train_rmse_mean\n",
    "tab['train RMSE std'] = train_rmse_std\n",
    "tab['test RMSE mean'] = validation_rmse_mean\n",
    "tab['test RMSE std'] = validation_rmse_std\n",
    "tab.to_csv('ccdplus_CV_lambda_item.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running num_features=1\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.9964586489194359.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.9964583524323956.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.9964522566776196.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.996441726997925.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 0.9964502496543466.\n",
      "iter: 1.0, RMSE on training set: 0.9964502489575263.\n",
      "RMSE on test data: 0.9964463174935322.\n",
      "Running num_features=3\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.9965275894151306.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.9965273966035967.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.9965211996115378.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.9965105969189195.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 1.716113666877524.\n",
      "iter: 1.0, RMSE on training set: 1.647782025460337.\n",
      "iter: 2.0, RMSE on training set: 1.5868036731220516.\n",
      "iter: 3.0, RMSE on training set: 1.5323532363355687.\n",
      "iter: 4.0, RMSE on training set: 1.4836837691477447.\n",
      "iter: 5.0, RMSE on training set: 1.4401241626813621.\n",
      "iter: 6.0, RMSE on training set: 1.4010748544817841.\n",
      "iter: 7.0, RMSE on training set: 1.3660031053481585.\n",
      "iter: 8.0, RMSE on training set: 1.33443794563244.\n",
      "iter: 9.0, RMSE on training set: 1.305964926040176.\n",
      "iter: 10.0, RMSE on training set: 1.280220809809396.\n",
      "iter: 11.0, RMSE on training set: 1.2568883343529058.\n",
      "iter: 12.0, RMSE on training set: 1.2356911532491217.\n",
      "iter: 13.0, RMSE on training set: 1.2163890469458598.\n",
      "iter: 14.0, RMSE on training set: 1.1987734660277194.\n",
      "iter: 15.0, RMSE on training set: 1.182663447219877.\n",
      "iter: 16.0, RMSE on training set: 1.1679019213840203.\n",
      "iter: 17.0, RMSE on training set: 1.1543524155966423.\n",
      "iter: 18.0, RMSE on training set: 1.141896138237886.\n",
      "iter: 19.0, RMSE on training set: 1.1304294266268733.\n",
      "iter: 20.0, RMSE on training set: 1.1198615306206408.\n",
      "iter: 21.0, RMSE on training set: 1.110112702149067.\n",
      "iter: 22.0, RMSE on training set: 1.1011125592873907.\n",
      "iter: 23.0, RMSE on training set: 1.092798693627544.\n",
      "iter: 24.0, RMSE on training set: 1.0851154909378304.\n",
      "iter: 25.0, RMSE on training set: 1.0780131370251869.\n",
      "iter: 26.0, RMSE on training set: 1.071446783046535.\n",
      "iter: 27.0, RMSE on training set: 1.0653758470401857.\n",
      "iter: 28.0, RMSE on training set: 1.0597634310099893.\n",
      "iter: 29.0, RMSE on training set: 1.0545758353876882.\n",
      "iter: 30.0, RMSE on training set: 1.0497821550530722.\n",
      "iter: 31.0, RMSE on training set: 1.0453539432658676.\n",
      "iter: 32.0, RMSE on training set: 1.0412649318362732.\n",
      "iter: 33.0, RMSE on training set: 1.0374907976260714.\n",
      "iter: 34.0, RMSE on training set: 1.0340089670317976.\n",
      "iter: 35.0, RMSE on training set: 1.0307984514652178.\n",
      "iter: 36.0, RMSE on training set: 1.0278397080272481.\n",
      "iter: 37.0, RMSE on training set: 1.0251145205852918.\n",
      "iter: 38.0, RMSE on training set: 1.0226058973271193.\n",
      "iter: 39.0, RMSE on training set: 1.0202979815937143.\n",
      "iter: 40.0, RMSE on training set: 1.0181759734054407.\n",
      "iter: 41.0, RMSE on training set: 1.0162260596056814.\n",
      "iter: 42.0, RMSE on training set: 1.0144353509681998.\n",
      "iter: 43.0, RMSE on training set: 1.0127918249613324.\n",
      "iter: 44.0, RMSE on training set: 1.0112842731449332.\n",
      "iter: 45.0, RMSE on training set: 1.0099022524044265.\n",
      "iter: 46.0, RMSE on training set: 1.0086360394087037.\n",
      "iter: 47.0, RMSE on training set: 1.007476587822807.\n",
      "iter: 48.0, RMSE on training set: 1.0064154879189875.\n",
      "iter: 49.0, RMSE on training set: 1.0054449283171192.\n",
      "iter: 50.0, RMSE on training set: 1.00455765965264.\n",
      "iter: 51.0, RMSE on training set: 1.0037469600215323.\n",
      "iter: 52.0, RMSE on training set: 1.003006602090279.\n",
      "iter: 53.0, RMSE on training set: 1.0023308217869948.\n",
      "iter: 54.0, RMSE on training set: 1.0017142885095984.\n",
      "iter: 55.0, RMSE on training set: 1.001152076799989.\n",
      "iter: 56.0, RMSE on training set: 1.000639639441149.\n",
      "iter: 57.0, RMSE on training set: 1.0001727819383135.\n",
      "iter: 58.0, RMSE on training set: 0.9997476383472129.\n",
      "iter: 59.0, RMSE on training set: 0.9993606484126062.\n",
      "iter: 60.0, RMSE on training set: 0.9990085359796412.\n",
      "iter: 61.0, RMSE on training set: 0.998688288639427.\n",
      "iter: 62.0, RMSE on training set: 0.9983971385687644.\n",
      "iter: 63.0, RMSE on training set: 0.998132544522559.\n",
      "iter: 64.0, RMSE on training set: 0.9978921749361016.\n",
      "iter: 65.0, RMSE on training set: 0.9976738920932516.\n",
      "iter: 66.0, RMSE on training set: 0.997475737315665.\n",
      "iter: 67.0, RMSE on training set: 0.9972959171276025.\n",
      "iter: 68.0, RMSE on training set: 0.9971327903505353.\n",
      "iter: 69.0, RMSE on training set: 0.9969848560817273.\n",
      "iter: 70.0, RMSE on training set: 0.996850742511194.\n",
      "iter: 71.0, RMSE on training set: 0.9967291965319698.\n",
      "iter: 72.0, RMSE on training set: 0.9966190740993115.\n",
      "iter: 73.0, RMSE on training set: 0.996519331295434.\n",
      "RMSE on test data: 0.996514383877923.\n",
      "Running num_features=5\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.9958706995672243.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.995871082996432.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.9958647252676435.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.9958544761771045.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.1600205843670244.\n",
      "iter: 1.0, RMSE on training set: 2.0598356504043043.\n",
      "iter: 2.0, RMSE on training set: 1.9717957871952057.\n",
      "iter: 3.0, RMSE on training set: 1.894252250697865.\n",
      "iter: 4.0, RMSE on training set: 1.8258173210720257.\n",
      "iter: 5.0, RMSE on training set: 1.7652724130860196.\n",
      "iter: 6.0, RMSE on training set: 1.7115524801975162.\n",
      "iter: 7.0, RMSE on training set: 1.6637307974508455.\n",
      "iter: 8.0, RMSE on training set: 1.621004238471915.\n",
      "iter: 9.0, RMSE on training set: 1.5826792484943546.\n",
      "iter: 10.0, RMSE on training set: 1.5481586891747232.\n",
      "iter: 11.0, RMSE on training set: 1.5169296889633186.\n",
      "iter: 12.0, RMSE on training set: 1.4885525824858359.\n",
      "iter: 13.0, RMSE on training set: 1.4626509722251047.\n",
      "iter: 14.0, RMSE on training set: 1.4389029018330177.\n",
      "iter: 15.0, RMSE on training set: 1.4170330957604265.\n",
      "iter: 16.0, RMSE on training set: 1.3968061953438482.\n",
      "iter: 17.0, RMSE on training set: 1.3780209063017776.\n",
      "iter: 18.0, RMSE on training set: 1.3605049652506715.\n",
      "iter: 19.0, RMSE on training set: 1.3441108315630907.\n",
      "iter: 20.0, RMSE on training set: 1.32871201392852.\n",
      "iter: 21.0, RMSE on training set: 1.3141999468433947.\n",
      "iter: 22.0, RMSE on training set: 1.300481339751247.\n",
      "iter: 23.0, RMSE on training set: 1.287475929775547.\n",
      "iter: 24.0, RMSE on training set: 1.2751145772997095.\n",
      "iter: 25.0, RMSE on training set: 1.2633376516303272.\n",
      "iter: 26.0, RMSE on training set: 1.2520936613766405.\n",
      "iter: 27.0, RMSE on training set: 1.2413380908597038.\n",
      "iter: 28.0, RMSE on training set: 1.2310324097815457.\n",
      "iter: 29.0, RMSE on training set: 1.2211432285472186.\n",
      "iter: 30.0, RMSE on training set: 1.2116415760839012.\n",
      "iter: 31.0, RMSE on training set: 1.2025022808029087.\n",
      "iter: 32.0, RMSE on training set: 1.1937034385710725.\n",
      "iter: 33.0, RMSE on training set: 1.1852259542670567.\n",
      "iter: 34.0, RMSE on training set: 1.1770531457621025.\n",
      "iter: 35.0, RMSE on training set: 1.1691704010454989.\n",
      "iter: 36.0, RMSE on training set: 1.1615648807702472.\n",
      "iter: 37.0, RMSE on training set: 1.1542252597760339.\n",
      "iter: 38.0, RMSE on training set: 1.1471415022017322.\n",
      "iter: 39.0, RMSE on training set: 1.1403046656685967.\n",
      "iter: 40.0, RMSE on training set: 1.1337067307319078.\n",
      "iter: 41.0, RMSE on training set: 1.1273404523907742.\n",
      "iter: 42.0, RMSE on training set: 1.1211992309350858.\n",
      "iter: 43.0, RMSE on training set: 1.115276999813074.\n",
      "iter: 44.0, RMSE on training set: 1.109568128537214.\n",
      "iter: 45.0, RMSE on training set: 1.1040673389219697.\n",
      "iter: 46.0, RMSE on training set: 1.0987696331739412.\n",
      "iter: 47.0, RMSE on training set: 1.0936702325413261.\n",
      "iter: 48.0, RMSE on training set: 1.0887645253818619.\n",
      "iter: 49.0, RMSE on training set: 1.0840480236327072.\n",
      "iter: 50.0, RMSE on training set: 1.079516326767724.\n",
      "iter: 51.0, RMSE on training set: 1.0751650924126857.\n",
      "iter: 52.0, RMSE on training set: 1.0709900128619387.\n",
      "iter: 53.0, RMSE on training set: 1.066986796804601.\n",
      "iter: 54.0, RMSE on training set: 1.0631511556271875.\n",
      "iter: 55.0, RMSE on training set: 1.0594787937136139.\n",
      "iter: 56.0, RMSE on training set: 1.0559654022134621.\n",
      "iter: 57.0, RMSE on training set: 1.0526066557953775.\n",
      "iter: 58.0, RMSE on training set: 1.0493982119445833.\n",
      "iter: 59.0, RMSE on training set: 1.0463357124024493.\n",
      "iter: 60.0, RMSE on training set: 1.0434147863821737.\n",
      "iter: 61.0, RMSE on training set: 1.0406310552285054.\n",
      "iter: 62.0, RMSE on training set: 1.0379801382215041.\n",
      "iter: 63.0, RMSE on training set: 1.0354576592545754.\n",
      "iter: 64.0, RMSE on training set: 1.0330592541457864.\n",
      "iter: 65.0, RMSE on training set: 1.0307805783685255.\n",
      "iter: 66.0, RMSE on training set: 1.0286173150131304.\n",
      "iter: 67.0, RMSE on training set: 1.0265651828149966.\n",
      "iter: 68.0, RMSE on training set: 1.024619944107085.\n",
      "iter: 69.0, RMSE on training set: 1.0227774125755895.\n",
      "iter: 70.0, RMSE on training set: 1.0210334607169018.\n",
      "iter: 71.0, RMSE on training set: 1.0193840269119827.\n",
      "iter: 72.0, RMSE on training set: 1.0178251220507575.\n",
      "iter: 73.0, RMSE on training set: 1.0163528356543137.\n",
      "iter: 74.0, RMSE on training set: 1.0149633414563826.\n",
      "iter: 75.0, RMSE on training set: 1.0136529024179453.\n",
      "iter: 76.0, RMSE on training set: 1.0124178751597381.\n",
      "iter: 77.0, RMSE on training set: 1.0112547138070653.\n",
      "iter: 78.0, RMSE on training set: 1.0101599732496616.\n",
      "iter: 79.0, RMSE on training set: 1.0091303118264712.\n",
      "iter: 80.0, RMSE on training set: 1.0081624934512334.\n",
      "iter: 81.0, RMSE on training set: 1.007253389199705.\n",
      "iter: 82.0, RMSE on training set: 1.006399978383392.\n",
      "iter: 83.0, RMSE on training set: 1.0055993491377657.\n",
      "iter: 84.0, RMSE on training set: 1.00484869855532.\n",
      "iter: 85.0, RMSE on training set: 1.0041453323954448.\n",
      "iter: 86.0, RMSE on training set: 1.003486664404101.\n",
      "iter: 87.0, RMSE on training set: 1.0028702152767501.\n",
      "iter: 88.0, RMSE on training set: 1.002293611297944.\n",
      "iter: 89.0, RMSE on training set: 1.0017545826905836.\n",
      "iter: 90.0, RMSE on training set: 1.00125096170705.\n",
      "iter: 91.0, RMSE on training set: 1.0007806804934123.\n",
      "iter: 92.0, RMSE on training set: 1.0003417687566172.\n",
      "iter: 93.0, RMSE on training set: 0.9999323512631757.\n",
      "iter: 94.0, RMSE on training set: 0.9995506451962932.\n",
      "iter: 95.0, RMSE on training set: 0.9991949573967989.\n",
      "iter: 96.0, RMSE on training set: 0.9988636815115274.\n",
      "iter: 97.0, RMSE on training set: 0.9985552950711665.\n",
      "iter: 98.0, RMSE on training set: 0.9982683565178736.\n",
      "iter: 99.0, RMSE on training set: 0.9980015022013266.\n",
      "iter: 100.0, RMSE on training set: 0.9977534433602718.\n",
      "iter: 101.0, RMSE on training set: 0.9975229631050362.\n",
      "iter: 102.0, RMSE on training set: 0.9973089134149817.\n",
      "iter: 103.0, RMSE on training set: 0.9971102121634219.\n",
      "iter: 104.0, RMSE on training set: 0.9969258401811228.\n",
      "iter: 105.0, RMSE on training set: 0.9967548383682024.\n",
      "iter: 106.0, RMSE on training set: 0.9965963048629665.\n",
      "iter: 107.0, RMSE on training set: 0.9964493922750587.\n",
      "iter: 108.0, RMSE on training set: 0.9963133049891546.\n",
      "iter: 109.0, RMSE on training set: 0.9961872965444178.\n",
      "iter: 110.0, RMSE on training set: 0.9960706670939253.\n",
      "iter: 111.0, RMSE on training set: 0.9959627609474004.\n",
      "iter: 112.0, RMSE on training set: 0.9958629641997404.\n",
      "RMSE on test data: 0.9958592801982589.\n",
      "Running num_features=7\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951689138004891.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951682211479115.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951619977785641.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951513392775841.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.5355607053060893.\n",
      "iter: 1.0, RMSE on training set: 2.407322873378896.\n",
      "iter: 2.0, RMSE on training set: 2.294814119354758.\n",
      "iter: 3.0, RMSE on training set: 2.196019196935565.\n",
      "iter: 4.0, RMSE on training set: 2.109150787456919.\n",
      "iter: 5.0, RMSE on training set: 2.0326295913764403.\n",
      "iter: 6.0, RMSE on training set: 1.9650670960121286.\n",
      "iter: 7.0, RMSE on training set: 1.9052485986578125.\n",
      "iter: 8.0, RMSE on training set: 1.852116466429474.\n",
      "iter: 9.0, RMSE on training set: 1.804753829691588.\n",
      "iter: 10.0, RMSE on training set: 1.762368923173226.\n",
      "iter: 11.0, RMSE on training set: 1.72428028332877.\n",
      "iter: 12.0, RMSE on training set: 1.6899029768956482.\n",
      "iter: 13.0, RMSE on training set: 1.658735983676349.\n",
      "iter: 14.0, RMSE on training set: 1.6303507990736228.\n",
      "iter: 15.0, RMSE on training set: 1.6043812683169083.\n",
      "iter: 16.0, RMSE on training set: 1.5805146200168216.\n",
      "iter: 17.0, RMSE on training set: 1.5584836337355201.\n",
      "iter: 18.0, RMSE on training set: 1.5380598544861528.\n",
      "iter: 19.0, RMSE on training set: 1.51904775502774.\n",
      "iter: 20.0, RMSE on training set: 1.5012797425177165.\n",
      "iter: 21.0, RMSE on training set: 1.4846119074795654.\n",
      "iter: 22.0, RMSE on training set: 1.4689204183034594.\n",
      "iter: 23.0, RMSE on training set: 1.4540984721180255.\n",
      "iter: 24.0, RMSE on training set: 1.440053721696145.\n",
      "iter: 25.0, RMSE on training set: 1.4267061072532745.\n",
      "iter: 26.0, RMSE on training set: 1.4139860309981431.\n",
      "iter: 27.0, RMSE on training set: 1.4018328207482922.\n",
      "iter: 28.0, RMSE on training set: 1.390193436629831.\n",
      "iter: 29.0, RMSE on training set: 1.3790213817567671.\n",
      "iter: 30.0, RMSE on training set: 1.3682757838202506.\n",
      "iter: 31.0, RMSE on training set: 1.357920619749037.\n",
      "iter: 32.0, RMSE on training set: 1.347924060091986.\n",
      "iter: 33.0, RMSE on training set: 1.3382579135953292.\n",
      "iter: 34.0, RMSE on training set: 1.3288971556774123.\n",
      "iter: 35.0, RMSE on training set: 1.319819527215569.\n",
      "iter: 36.0, RMSE on training set: 1.3110051923235102.\n",
      "iter: 37.0, RMSE on training set: 1.3024364456793318.\n",
      "iter: 38.0, RMSE on training set: 1.2940974615250644.\n",
      "iter: 39.0, RMSE on training set: 1.2859740777536317.\n",
      "iter: 40.0, RMSE on training set: 1.2780536095756023.\n",
      "iter: 41.0, RMSE on training set: 1.2703246881552293.\n",
      "iter: 42.0, RMSE on training set: 1.2627771203542288.\n",
      "iter: 43.0, RMSE on training set: 1.2554017663469932.\n",
      "iter: 44.0, RMSE on training set: 1.2481904323921926.\n",
      "iter: 45.0, RMSE on training set: 1.241135776479194.\n",
      "iter: 46.0, RMSE on training set: 1.2342312249274647.\n",
      "iter: 47.0, RMSE on training set: 1.2274708983163887.\n",
      "iter: 48.0, RMSE on training set: 1.2208495453730157.\n",
      "iter: 49.0, RMSE on training set: 1.2143624836565232.\n",
      "iter: 50.0, RMSE on training set: 1.2080055460583867.\n",
      "iter: 51.0, RMSE on training set: 1.20177503229189.\n",
      "iter: 52.0, RMSE on training set: 1.195667664677534.\n",
      "iter: 53.0, RMSE on training set: 1.189680547644323.\n",
      "iter: 54.0, RMSE on training set: 1.183811130463441.\n",
      "iter: 55.0, RMSE on training set: 1.1780571728121374.\n",
      "iter: 56.0, RMSE on training set: 1.172416712834451.\n",
      "iter: 57.0, RMSE on training set: 1.1668880374233368.\n",
      "iter: 58.0, RMSE on training set: 1.1614696544977294.\n",
      "iter: 59.0, RMSE on training set: 1.1561602670892521.\n",
      "iter: 60.0, RMSE on training set: 1.1509587490877262.\n",
      "iter: 61.0, RMSE on training set: 1.1458641225230568.\n",
      "iter: 62.0, RMSE on training set: 1.1408755362842797.\n",
      "iter: 63.0, RMSE on training set: 1.135992246195107.\n",
      "iter: 64.0, RMSE on training set: 1.131213596380054.\n",
      "iter: 65.0, RMSE on training set: 1.126539001866783.\n",
      "iter: 66.0, RMSE on training set: 1.1219679323794542.\n",
      "iter: 67.0, RMSE on training set: 1.117499897285156.\n",
      "iter: 68.0, RMSE on training set: 1.1131344316613647.\n",
      "iter: 69.0, RMSE on training set: 1.1088710834570452.\n",
      "iter: 70.0, RMSE on training set: 1.1047094017232841.\n",
      "iter: 71.0, RMSE on training set: 1.1006489258914434.\n",
      "iter: 72.0, RMSE on training set: 1.096689176077395.\n",
      "iter: 73.0, RMSE on training set: 1.092829644389903.\n",
      "iter: 74.0, RMSE on training set: 1.0890697872196673.\n",
      "iter: 75.0, RMSE on training set: 1.0854090184834269.\n",
      "iter: 76.0, RMSE on training set: 1.081846703795069.\n",
      "iter: 77.0, RMSE on training set: 1.0783821555330793.\n",
      "iter: 78.0, RMSE on training set: 1.0750146287711462.\n",
      "iter: 79.0, RMSE on training set: 1.0717433180362492.\n",
      "iter: 80.0, RMSE on training set: 1.0685673548563148.\n",
      "iter: 81.0, RMSE on training set: 1.0654858060573809.\n",
      "iter: 82.0, RMSE on training set: 1.0624976727684392.\n",
      "iter: 83.0, RMSE on training set: 1.0596018900904896.\n",
      "iter: 84.0, RMSE on training set: 1.0567973273851017.\n",
      "iter: 85.0, RMSE on training set: 1.0540827891367854.\n",
      "iter: 86.0, RMSE on training set: 1.051457016342823.\n",
      "iter: 87.0, RMSE on training set: 1.0489186883838493.\n",
      "iter: 88.0, RMSE on training set: 1.0464664253283844.\n",
      "iter: 89.0, RMSE on training set: 1.0440987906247445.\n",
      "iter: 90.0, RMSE on training set: 1.041814294134194.\n",
      "iter: 91.0, RMSE on training set: 1.0396113954598893.\n",
      "iter: 92.0, RMSE on training set: 1.0374885075271836.\n",
      "iter: 93.0, RMSE on training set: 1.035444000371995.\n",
      "iter: 94.0, RMSE on training set: 1.0334762050954962.\n",
      "iter: 95.0, RMSE on training set: 1.0315834179451033.\n",
      "iter: 96.0, RMSE on training set: 1.0297639044837557.\n",
      "iter: 97.0, RMSE on training set: 1.0280159038117374.\n",
      "iter: 98.0, RMSE on training set: 1.0263376328076814.\n",
      "iter: 99.0, RMSE on training set: 1.0247272903579359.\n",
      "iter: 100.0, RMSE on training set: 1.023183061546045.\n",
      "iter: 101.0, RMSE on training set: 1.0217031217766697.\n",
      "iter: 102.0, RMSE on training set: 1.0202856408107381.\n",
      "iter: 103.0, RMSE on training set: 1.0189287866910737.\n",
      "iter: 104.0, RMSE on training set: 1.017630729539961.\n",
      "iter: 105.0, RMSE on training set: 1.0163896452124226.\n",
      "iter: 106.0, RMSE on training set: 1.0152037187909841.\n",
      "iter: 107.0, RMSE on training set: 1.0140711479098532.\n",
      "iter: 108.0, RMSE on training set: 1.0129901458984503.\n",
      "iter: 109.0, RMSE on training set: 1.0119589447362358.\n",
      "iter: 110.0, RMSE on training set: 1.0109757978127731.\n",
      "iter: 111.0, RMSE on training set: 1.0100389824889062.\n",
      "iter: 112.0, RMSE on training set: 1.0091468024567662.\n",
      "iter: 113.0, RMSE on training set: 1.0082975898980764.\n",
      "iter: 114.0, RMSE on training set: 1.0074897074417792.\n",
      "iter: 115.0, RMSE on training set: 1.006721549923432.\n",
      "iter: 116.0, RMSE on training set: 1.0059915459500386.\n",
      "iter: 117.0, RMSE on training set: 1.0052981592749999.\n",
      "iter: 118.0, RMSE on training set: 1.0046398899887972.\n",
      "iter: 119.0, RMSE on training set: 1.0040152755316845.\n",
      "iter: 120.0, RMSE on training set: 1.0034228915353403.\n",
      "iter: 121.0, RMSE on training set: 1.0028613525008552.\n",
      "iter: 122.0, RMSE on training set: 1.002329312320892.\n",
      "iter: 123.0, RMSE on training set: 1.0018254646541074.\n",
      "iter: 124.0, RMSE on training set: 1.0013485431602123.\n",
      "iter: 125.0, RMSE on training set: 1.0008973216041313.\n",
      "iter: 126.0, RMSE on training set: 1.0004706138378792.\n",
      "iter: 127.0, RMSE on training set: 1.00006727366871.\n",
      "iter: 128.0, RMSE on training set: 0.9996861946221225.\n",
      "iter: 129.0, RMSE on training set: 0.9993263096081154.\n",
      "iter: 130.0, RMSE on training set: 0.998986590499003.\n",
      "iter: 131.0, RMSE on training set: 0.99866604762685.\n",
      "iter: 132.0, RMSE on training set: 0.9983637292083561.\n",
      "iter: 133.0, RMSE on training set: 0.9980787207047518.\n",
      "iter: 134.0, RMSE on training set: 0.997810144123974.\n",
      "iter: 135.0, RMSE on training set: 0.9975571572720474.\n",
      "iter: 136.0, RMSE on training set: 0.9973189529603028.\n",
      "iter: 137.0, RMSE on training set: 0.9970947581746913.\n",
      "iter: 138.0, RMSE on training set: 0.9968838332131442.\n",
      "iter: 139.0, RMSE on training set: 0.996685470796535.\n",
      "iter: 140.0, RMSE on training set: 0.9964989951585009.\n",
      "iter: 141.0, RMSE on training set: 0.9963237611190272.\n",
      "iter: 142.0, RMSE on training set: 0.9961591531463169.\n",
      "iter: 143.0, RMSE on training set: 0.9960045844112105.\n",
      "iter: 144.0, RMSE on training set: 0.995859495838024.\n",
      "iter: 145.0, RMSE on training set: 0.9957233551553841.\n",
      "iter: 146.0, RMSE on training set: 0.9955956559503508.\n",
      "iter: 147.0, RMSE on training set: 0.9954759167287495.\n",
      "iter: 148.0, RMSE on training set: 0.995363679984416.\n",
      "iter: 149.0, RMSE on training set: 0.9952585112797206.\n",
      "iter: 150.0, RMSE on training set: 0.9951599983394995.\n",
      "RMSE on test data: 0.9951561529835574.\n",
      "Running num_features=10\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939761161314021.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939761276075211.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939705430597577.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939586319702657.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9905910886823186.\n",
      "iter: 1.0, RMSE on training set: 2.830295316605344.\n",
      "iter: 2.0, RMSE on training set: 2.689670551732953.\n",
      "iter: 3.0, RMSE on training set: 2.566216802333157.\n",
      "iter: 4.0, RMSE on training set: 2.457715538605971.\n",
      "iter: 5.0, RMSE on training set: 2.3622054013081724.\n",
      "iter: 6.0, RMSE on training set: 2.277959399908402.\n",
      "iter: 7.0, RMSE on training set: 2.2034638171006113.\n",
      "iter: 8.0, RMSE on training set: 2.1373978330691616.\n",
      "iter: 9.0, RMSE on training set: 2.078613861888893.\n",
      "iter: 10.0, RMSE on training set: 2.0261187300235783.\n",
      "iter: 11.0, RMSE on training set: 1.9790558714975137.\n",
      "iter: 12.0, RMSE on training set: 1.9366887092981584.\n",
      "iter: 13.0, RMSE on training set: 1.8983853560962332.\n",
      "iter: 14.0, RMSE on training set: 1.8636047155035074.\n",
      "iter: 15.0, RMSE on training set: 1.8318840104327856.\n",
      "iter: 16.0, RMSE on training set: 1.8028277159827644.\n",
      "iter: 17.0, RMSE on training set: 1.7760978350072467.\n",
      "iter: 18.0, RMSE on training set: 1.7514054265187229.\n",
      "iter: 19.0, RMSE on training set: 1.7285032797508006.\n",
      "iter: 20.0, RMSE on training set: 1.7071796184453012.\n",
      "iter: 21.0, RMSE on training set: 1.6872527187278588.\n",
      "iter: 22.0, RMSE on training set: 1.668566327803812.\n",
      "iter: 23.0, RMSE on training set: 1.6509857779043808.\n",
      "iter: 24.0, RMSE on training set: 1.6343946990393083.\n",
      "iter: 25.0, RMSE on training set: 1.6186922441083678.\n",
      "iter: 26.0, RMSE on training set: 1.6037907500396902.\n",
      "iter: 27.0, RMSE on training set: 1.5896137683599136.\n",
      "iter: 28.0, RMSE on training set: 1.5760944076563028.\n",
      "iter: 29.0, RMSE on training set: 1.5631739386041525.\n",
      "iter: 30.0, RMSE on training set: 1.5508006195431354.\n",
      "iter: 31.0, RMSE on training set: 1.5389287069991733.\n",
      "iter: 32.0, RMSE on training set: 1.5275176211100383.\n",
      "iter: 33.0, RMSE on training set: 1.516531240692456.\n",
      "iter: 34.0, RMSE on training set: 1.5059373067659358.\n",
      "iter: 35.0, RMSE on training set: 1.4957069168070039.\n",
      "iter: 36.0, RMSE on training set: 1.485814094927067.\n",
      "iter: 37.0, RMSE on training set: 1.4762354256232508.\n",
      "iter: 38.0, RMSE on training set: 1.4669497408114218.\n",
      "iter: 39.0, RMSE on training set: 1.4579378515745367.\n",
      "iter: 40.0, RMSE on training set: 1.4491823174988807.\n",
      "iter: 41.0, RMSE on training set: 1.4406672476707139.\n",
      "iter: 42.0, RMSE on training set: 1.432378128404255.\n",
      "iter: 43.0, RMSE on training set: 1.4243016736014378.\n",
      "iter: 44.0, RMSE on training set: 1.416425694332098.\n",
      "iter: 45.0, RMSE on training set: 1.4087389847932807.\n",
      "iter: 46.0, RMSE on training set: 1.401231222277954.\n",
      "iter: 47.0, RMSE on training set: 1.3938928791729575.\n",
      "iter: 48.0, RMSE on training set: 1.3867151453276136.\n",
      "iter: 49.0, RMSE on training set: 1.3796898594000249.\n",
      "iter: 50.0, RMSE on training set: 1.3728094480076172.\n",
      "iter: 51.0, RMSE on training set: 1.3660668716902284.\n",
      "iter: 52.0, RMSE on training set: 1.3594555768446543.\n",
      "iter: 53.0, RMSE on training set: 1.3529694529144314.\n",
      "iter: 54.0, RMSE on training set: 1.3466027942221237.\n",
      "iter: 55.0, RMSE on training set: 1.3403502659172166.\n",
      "iter: 56.0, RMSE on training set: 1.3342068735838761.\n",
      "iter: 57.0, RMSE on training set: 1.3281679361118663.\n",
      "iter: 58.0, RMSE on training set: 1.3222290614830494.\n",
      "iter: 59.0, RMSE on training set: 1.3163861251667461.\n",
      "iter: 60.0, RMSE on training set: 1.3106352508513022.\n",
      "iter: 61.0, RMSE on training set: 1.3049727932678687.\n",
      "iter: 62.0, RMSE on training set: 1.2993953228865236.\n",
      "iter: 63.0, RMSE on training set: 1.2938996122855164.\n",
      "iter: 64.0, RMSE on training set: 1.288482624012353.\n",
      "iter: 65.0, RMSE on training set: 1.2831414997713377.\n",
      "iter: 66.0, RMSE on training set: 1.2778735507863925.\n",
      "iter: 67.0, RMSE on training set: 1.2726762492010788.\n",
      "iter: 68.0, RMSE on training set: 1.2675472203896758.\n",
      "iter: 69.0, RMSE on training set: 1.262484236064204.\n",
      "iter: 70.0, RMSE on training set: 1.2574852080724772.\n",
      "iter: 71.0, RMSE on training set: 1.2525481827915304.\n",
      "iter: 72.0, RMSE on training set: 1.2476713360294538.\n",
      "iter: 73.0, RMSE on training set: 1.2428529683564746.\n",
      "iter: 74.0, RMSE on training set: 1.2380915007935418.\n",
      "iter: 75.0, RMSE on training set: 1.2333854707935081.\n",
      "iter: 76.0, RMSE on training set: 1.2287335284564938.\n",
      "iter: 77.0, RMSE on training set: 1.2241344329271535.\n",
      "iter: 78.0, RMSE on training set: 1.219587048927436.\n",
      "iter: 79.0, RMSE on training set: 1.215090343384026.\n",
      "iter: 80.0, RMSE on training set: 1.2106433821149498.\n",
      "iter: 81.0, RMSE on training set: 1.2062453265448683.\n",
      "iter: 82.0, RMSE on training set: 1.2018954304232734.\n",
      "iter: 83.0, RMSE on training set: 1.1975930365241323.\n",
      "iter: 84.0, RMSE on training set: 1.1933375733095433.\n",
      "iter: 85.0, RMSE on training set: 1.1891285515435401.\n",
      "iter: 86.0, RMSE on training set: 1.1849655608454877.\n",
      "iter: 87.0, RMSE on training set: 1.1808482661754514.\n",
      "iter: 88.0, RMSE on training set: 1.1767764042466136.\n",
      "iter: 89.0, RMSE on training set: 1.1727497798623443.\n",
      "iter: 90.0, RMSE on training set: 1.1687682621778783.\n",
      "iter: 91.0, RMSE on training set: 1.1648317808887192.\n",
      "iter: 92.0, RMSE on training set: 1.1609403223501642.\n",
      "iter: 93.0, RMSE on training set: 1.1570939256341786.\n",
      "iter: 94.0, RMSE on training set: 1.153292678531885.\n",
      "iter: 95.0, RMSE on training set: 1.1495367135114747.\n",
      "iter: 96.0, RMSE on training set: 1.1458262036428701.\n",
      "iter: 97.0, RMSE on training set: 1.142161358501637.\n",
      "iter: 98.0, RMSE on training set: 1.138542420065552.\n",
      "iter: 99.0, RMSE on training set: 1.1349696586178688.\n",
      "iter: 100.0, RMSE on training set: 1.1314433686717473.\n",
      "iter: 101.0, RMSE on training set: 1.127963864930408.\n",
      "iter: 102.0, RMSE on training set: 1.124531478297533.\n",
      "iter: 103.0, RMSE on training set: 1.1211465519522044.\n",
      "iter: 104.0, RMSE on training set: 1.1178094375022365.\n",
      "iter: 105.0, RMSE on training set: 1.114520491229264.\n",
      "iter: 106.0, RMSE on training set: 1.1112800704382941.\n",
      "iter: 107.0, RMSE on training set: 1.1080885299237835.\n",
      "iter: 108.0, RMSE on training set: 1.1049462185635004.\n",
      "iter: 109.0, RMSE on training set: 1.1018534760507024.\n",
      "iter: 110.0, RMSE on training set: 1.0988106297743034.\n",
      "iter: 111.0, RMSE on training set: 1.0958179918558855.\n",
      "iter: 112.0, RMSE on training set: 1.092875856351502.\n",
      "iter: 113.0, RMSE on training set: 1.0899844966252976.\n",
      "iter: 114.0, RMSE on training set: 1.0871441629009984.\n",
      "iter: 115.0, RMSE on training set: 1.0843550799962929.\n",
      "iter: 116.0, RMSE on training set: 1.0816174452440908.\n",
      "iter: 117.0, RMSE on training set: 1.078931426603555.\n",
      "iter: 118.0, RMSE on training set: 1.076297160962779.\n",
      "iter: 119.0, RMSE on training set: 1.0737147526338908.\n",
      "iter: 120.0, RMSE on training set: 1.0711842720404285.\n",
      "iter: 121.0, RMSE on training set: 1.0687057545958232.\n",
      "iter: 122.0, RMSE on training set: 1.0662791997710066.\n",
      "iter: 123.0, RMSE on training set: 1.0639045703483074.\n",
      "iter: 124.0, RMSE on training set: 1.0615817918580448.\n",
      "iter: 125.0, RMSE on training set: 1.0593107521935514.\n",
      "iter: 126.0, RMSE on training set: 1.0570913013996586.\n",
      "iter: 127.0, RMSE on training set: 1.0549232516290985.\n",
      "iter: 128.0, RMSE on training set: 1.052806377260696.\n",
      "iter: 129.0, RMSE on training set: 1.050740415172653.\n",
      "iter: 130.0, RMSE on training set: 1.0487250651637785.\n",
      "iter: 131.0, RMSE on training set: 1.0467599905150486.\n",
      "iter: 132.0, RMSE on training set: 1.0448448186834693.\n",
      "iter: 133.0, RMSE on training set: 1.0429791421199106.\n",
      "iter: 134.0, RMSE on training set: 1.0411625192022547.\n",
      "iter: 135.0, RMSE on training set: 1.0393944752750144.\n",
      "iter: 136.0, RMSE on training set: 1.0376745037863744.\n",
      "iter: 137.0, RMSE on training set: 1.0360020675135184.\n",
      "iter: 138.0, RMSE on training set: 1.0343765998670555.\n",
      "iter: 139.0, RMSE on training set: 1.0327975062653236.\n",
      "iter: 140.0, RMSE on training set: 1.0312641655694457.\n",
      "iter: 141.0, RMSE on training set: 1.029775931570102.\n",
      "iter: 142.0, RMSE on training set: 1.0283321345171335.\n",
      "iter: 143.0, RMSE on training set: 1.0269320826833483.\n",
      "iter: 144.0, RMSE on training set: 1.025575063954127.\n",
      "iter: 145.0, RMSE on training set: 1.0242603474347665.\n",
      "iter: 146.0, RMSE on training set: 1.022987185067839.\n",
      "iter: 147.0, RMSE on training set: 1.0217548132532654.\n",
      "iter: 148.0, RMSE on training set: 1.0205624544641592.\n",
      "iter: 149.0, RMSE on training set: 1.0194093188519964.\n",
      "iter: 150.0, RMSE on training set: 1.0182946058350741.\n",
      "iter: 151.0, RMSE on training set: 1.0172175056646708.\n",
      "iter: 152.0, RMSE on training set: 1.0161772009637873.\n",
      "iter: 153.0, RMSE on training set: 1.0151728682337724.\n",
      "iter: 154.0, RMSE on training set: 1.0142036793245583.\n",
      "iter: 155.0, RMSE on training set: 1.013268802864671.\n",
      "iter: 156.0, RMSE on training set: 1.0123674056475558.\n",
      "iter: 157.0, RMSE on training set: 1.011498653971173.\n",
      "iter: 158.0, RMSE on training set: 1.010661714928167.\n",
      "iter: 159.0, RMSE on training set: 1.0098557576443012.\n",
      "iter: 160.0, RMSE on training set: 1.0090799544631832.\n",
      "iter: 161.0, RMSE on training set: 1.008333482075651.\n",
      "iter: 162.0, RMSE on training set: 1.0076155225925338.\n",
      "iter: 163.0, RMSE on training set: 1.0069252645597835.\n",
      "iter: 164.0, RMSE on training set: 1.0062619039153244.\n",
      "iter: 165.0, RMSE on training set: 1.0056246448872372.\n",
      "iter: 166.0, RMSE on training set: 1.0050127008331768.\n",
      "iter: 167.0, RMSE on training set: 1.0044252950212078.\n",
      "iter: 168.0, RMSE on training set: 1.003861661352501.\n",
      "iter: 169.0, RMSE on training set: 1.003321045026543.\n",
      "iter: 170.0, RMSE on training set: 1.0028027031497724.\n",
      "iter: 171.0, RMSE on training set: 1.0023059052887369.\n",
      "iter: 172.0, RMSE on training set: 1.001829933969029.\n",
      "iter: 173.0, RMSE on training set: 1.001374085121459.\n",
      "iter: 174.0, RMSE on training set: 1.0009376684770215.\n",
      "iter: 175.0, RMSE on training set: 1.0005200079123142.\n",
      "iter: 176.0, RMSE on training set: 1.000120441747228.\n",
      "iter: 177.0, RMSE on training set: 0.99973832299671.\n",
      "iter: 178.0, RMSE on training set: 0.9993730195785283.\n",
      "iter: 179.0, RMSE on training set: 0.9990239144789911.\n",
      "iter: 180.0, RMSE on training set: 0.998690405878573.\n",
      "iter: 181.0, RMSE on training set: 0.998371907239482.\n",
      "iter: 182.0, RMSE on training set: 0.9980678473571349.\n",
      "iter: 183.0, RMSE on training set: 0.9977776703775707.\n",
      "iter: 184.0, RMSE on training set: 0.99750083578278.\n",
      "iter: 185.0, RMSE on training set: 0.9972368183459269.\n",
      "iter: 186.0, RMSE on training set: 0.9969851080584006.\n",
      "iter: 187.0, RMSE on training set: 0.9967452100306167.\n",
      "iter: 188.0, RMSE on training set: 0.9965166443684372.\n",
      "iter: 189.0, RMSE on training set: 0.996298946027029.\n",
      "iter: 190.0, RMSE on training set: 0.996091664643948.\n",
      "iter: 191.0, RMSE on training set: 0.9958943643531747.\n",
      "iter: 192.0, RMSE on training set: 0.9957066235817714.\n",
      "iter: 193.0, RMSE on training set: 0.9955280348307788.\n",
      "iter: 194.0, RMSE on training set: 0.9953582044419194.\n",
      "iter: 195.0, RMSE on training set: 0.9951967523515867.\n",
      "iter: 196.0, RMSE on training set: 0.9950433118335844.\n",
      "iter: 197.0, RMSE on training set: 0.9948975292319752.\n",
      "iter: 198.0, RMSE on training set: 0.9947590636853618.\n",
      "iter: 199.0, RMSE on training set: 0.9946275868438499.\n",
      "iter: 200.0, RMSE on training set: 0.9945027825798937.\n",
      "iter: 201.0, RMSE on training set: 0.9943843466941372.\n",
      "iter: 202.0, RMSE on training set: 0.9942719866173263.\n",
      "iter: 203.0, RMSE on training set: 0.9941654211092912.\n",
      "iter: 204.0, RMSE on training set: 0.9940643799559309.\n",
      "iter: 205.0, RMSE on training set: 0.9939686036650984.\n",
      "RMSE on test data: 0.9939652233061246.\n",
      "Running num_features=13\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9927995702343569.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9928006881240632.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9927934022741228.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9927813610462145.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.3864787383699797.\n",
      "iter: 1.0, RMSE on training set: 3.2030617635284124.\n",
      "iter: 2.0, RMSE on training set: 3.041903661422984.\n",
      "iter: 3.0, RMSE on training set: 2.900168692112112.\n",
      "iter: 4.0, RMSE on training set: 2.775384164000644.\n",
      "iter: 5.0, RMSE on training set: 2.66536008484133.\n",
      "iter: 6.0, RMSE on training set: 2.5681632251028432.\n",
      "iter: 7.0, RMSE on training set: 2.482094578955638.\n",
      "iter: 8.0, RMSE on training set: 2.4056675988743015.\n",
      "iter: 9.0, RMSE on training set: 2.337587141548265.\n",
      "iter: 10.0, RMSE on training set: 2.2767292431954966.\n",
      "iter: 11.0, RMSE on training set: 2.222121901576184.\n",
      "iter: 12.0, RMSE on training set: 2.172927051259277.\n",
      "iter: 13.0, RMSE on training set: 2.1284238875664636.\n",
      "iter: 14.0, RMSE on training set: 2.087993643072368.\n",
      "iter: 15.0, RMSE on training set: 2.051105862945598.\n",
      "iter: 16.0, RMSE on training set: 2.017306171485263.\n",
      "iter: 17.0, RMSE on training set: 1.9862054775393987.\n",
      "iter: 18.0, RMSE on training set: 1.9574705333291131.\n",
      "iter: 19.0, RMSE on training set: 1.9308157394856815.\n",
      "iter: 20.0, RMSE on training set: 1.9059960774445663.\n",
      "iter: 21.0, RMSE on training set: 1.8828010467436362.\n",
      "iter: 22.0, RMSE on training set: 1.861049487155831.\n",
      "iter: 23.0, RMSE on training set: 1.8405851720486825.\n",
      "iter: 24.0, RMSE on training set: 1.821273068325389.\n",
      "iter: 25.0, RMSE on training set: 1.802996168540571.\n",
      "iter: 26.0, RMSE on training set: 1.785652811406124.\n",
      "iter: 27.0, RMSE on training set: 1.7691544172977443.\n",
      "iter: 28.0, RMSE on training set: 1.7534235751527374.\n",
      "iter: 29.0, RMSE on training set: 1.738392426093348.\n",
      "iter: 30.0, RMSE on training set: 1.724001297116244.\n",
      "iter: 31.0, RMSE on training set: 1.7101975452382157.\n",
      "iter: 32.0, RMSE on training set: 1.6969345786150172.\n",
      "iter: 33.0, RMSE on training set: 1.6841710264207541.\n",
      "iter: 34.0, RMSE on training set: 1.6718700337736851.\n",
      "iter: 35.0, RMSE on training set: 1.659998661811549.\n",
      "iter: 36.0, RMSE on training set: 1.6485273762460693.\n",
      "iter: 37.0, RMSE on training set: 1.6374296104456445.\n",
      "iter: 38.0, RMSE on training set: 1.6266813913822646.\n",
      "iter: 39.0, RMSE on training set: 1.6162610186965805.\n",
      "iter: 40.0, RMSE on training set: 1.6061487887387402.\n",
      "iter: 41.0, RMSE on training set: 1.596326756777683.\n",
      "iter: 42.0, RMSE on training set: 1.5867785316786736.\n",
      "iter: 43.0, RMSE on training set: 1.5774890982633738.\n",
      "iter: 44.0, RMSE on training set: 1.5684446633209583.\n",
      "iter: 45.0, RMSE on training set: 1.5596325218616893.\n",
      "iter: 46.0, RMSE on training set: 1.5510409407218804.\n",
      "iter: 47.0, RMSE on training set: 1.5426590570627214.\n",
      "iter: 48.0, RMSE on training set: 1.5344767896717801.\n",
      "iter: 49.0, RMSE on training set: 1.5264847612870078.\n",
      "iter: 50.0, RMSE on training set: 1.518674230427295.\n",
      "iter: 51.0, RMSE on training set: 1.5110370314372552.\n",
      "iter: 52.0, RMSE on training set: 1.5035655216426542.\n",
      "iter: 53.0, RMSE on training set: 1.4962525346717162.\n",
      "iter: 54.0, RMSE on training set: 1.489091339131588.\n",
      "iter: 55.0, RMSE on training set: 1.4820756019426744.\n",
      "iter: 56.0, RMSE on training set: 1.4751993557300813.\n",
      "iter: 57.0, RMSE on training set: 1.468456969753792.\n",
      "iter: 58.0, RMSE on training set: 1.461843123929467.\n",
      "iter: 59.0, RMSE on training set: 1.4553527855516726.\n",
      "iter: 60.0, RMSE on training set: 1.4489811883821508.\n",
      "iter: 61.0, RMSE on training set: 1.442723813809221.\n",
      "iter: 62.0, RMSE on training set: 1.436576373821642.\n",
      "iter: 63.0, RMSE on training set: 1.4305347955729788.\n",
      "iter: 64.0, RMSE on training set: 1.4245952073414343.\n",
      "iter: 65.0, RMSE on training set: 1.4187539257160682.\n",
      "iter: 66.0, RMSE on training set: 1.4130074438636333.\n",
      "iter: 67.0, RMSE on training set: 1.407352420750772.\n",
      "iter: 68.0, RMSE on training set: 1.4017856712144126.\n",
      "iter: 69.0, RMSE on training set: 1.3963041567886059.\n",
      "iter: 70.0, RMSE on training set: 1.390904977209278.\n",
      "iter: 71.0, RMSE on training set: 1.3855853625294836.\n",
      "iter: 72.0, RMSE on training set: 1.3803426657870996.\n",
      "iter: 73.0, RMSE on training set: 1.3751743561748746.\n",
      "iter: 74.0, RMSE on training set: 1.3700780126693246.\n",
      "iter: 75.0, RMSE on training set: 1.3650513180806847.\n",
      "iter: 76.0, RMSE on training set: 1.3600920534907992.\n",
      "iter: 77.0, RMSE on training set: 1.3551980930498604.\n",
      "iter: 78.0, RMSE on training set: 1.3503673991063605.\n",
      "iter: 79.0, RMSE on training set: 1.3455980176474887.\n",
      "iter: 80.0, RMSE on training set: 1.3408880740298599.\n",
      "iter: 81.0, RMSE on training set: 1.336235768982722.\n",
      "iter: 82.0, RMSE on training set: 1.3316393748679252.\n",
      "iter: 83.0, RMSE on training set: 1.3270972321828272.\n",
      "iter: 84.0, RMSE on training set: 1.3226077462940897.\n",
      "iter: 85.0, RMSE on training set: 1.318169384391784.\n",
      "iter: 86.0, RMSE on training set: 1.3137806726545354.\n",
      "iter: 87.0, RMSE on training set: 1.3094401936173197.\n",
      "iter: 88.0, RMSE on training set: 1.3051465837342184.\n",
      "iter: 89.0, RMSE on training set: 1.3008985311286634.\n",
      "iter: 90.0, RMSE on training set: 1.2966947735238072.\n",
      "iter: 91.0, RMSE on training set: 1.29253409634542.\n",
      "iter: 92.0, RMSE on training set: 1.288415330989483.\n",
      "iter: 93.0, RMSE on training set: 1.284337353246252.\n",
      "iter: 94.0, RMSE on training set: 1.2802990818723157.\n",
      "iter: 95.0, RMSE on training set: 1.2762994773019207.\n",
      "iter: 96.0, RMSE on training set: 1.2723375404887363.\n",
      "iter: 97.0, RMSE on training set: 1.2684123118691932.\n",
      "iter: 98.0, RMSE on training set: 1.2645228704386435.\n",
      "iter: 99.0, RMSE on training set: 1.2606683329316708.\n",
      "iter: 100.0, RMSE on training set: 1.2568478530981393.\n",
      "iter: 101.0, RMSE on training set: 1.2530606210666662.\n",
      "iter: 102.0, RMSE on training set: 1.2493058627874263.\n",
      "iter: 103.0, RMSE on training set: 1.2455828395462445.\n",
      "iter: 104.0, RMSE on training set: 1.241890847541984.\n",
      "iter: 105.0, RMSE on training set: 1.2382292175192386.\n",
      "iter: 106.0, RMSE on training set: 1.2345973144482105.\n",
      "iter: 107.0, RMSE on training set: 1.2309945372436668.\n",
      "iter: 108.0, RMSE on training set: 1.2274203185148265.\n",
      "iter: 109.0, RMSE on training set: 1.2238741243380435.\n",
      "iter: 110.0, RMSE on training set: 1.220355454044407.\n",
      "iter: 111.0, RMSE on training set: 1.2168638400145566.\n",
      "iter: 112.0, RMSE on training set: 1.2133988474734336.\n",
      "iter: 113.0, RMSE on training set: 1.209960074278089.\n",
      "iter: 114.0, RMSE on training set: 1.206547150692197.\n",
      "iter: 115.0, RMSE on training set: 1.203159739141397.\n",
      "iter: 116.0, RMSE on training set: 1.1997975339441376.\n",
      "iter: 117.0, RMSE on training set: 1.1964602610132105.\n",
      "iter: 118.0, RMSE on training set: 1.1931476775236478.\n",
      "iter: 119.0, RMSE on training set: 1.1898595715431584.\n",
      "iter: 120.0, RMSE on training set: 1.1865957616217448.\n",
      "iter: 121.0, RMSE on training set: 1.1833560963376122.\n",
      "iter: 122.0, RMSE on training set: 1.1801404537968863.\n",
      "iter: 123.0, RMSE on training set: 1.1769487410851451.\n",
      "iter: 124.0, RMSE on training set: 1.1737808936691347.\n",
      "iter: 125.0, RMSE on training set: 1.1706368747474853.\n",
      "iter: 126.0, RMSE on training set: 1.167516674549635.\n",
      "iter: 127.0, RMSE on training set: 1.1644203095825754.\n",
      "iter: 128.0, RMSE on training set: 1.1613478218253979.\n",
      "iter: 129.0, RMSE on training set: 1.158299277872028.\n",
      "iter: 130.0, RMSE on training set: 1.1552747680229396.\n",
      "iter: 131.0, RMSE on training set: 1.152274405326944.\n",
      "iter: 132.0, RMSE on training set: 1.1492983245746218.\n",
      "iter: 133.0, RMSE on training set: 1.1463466812452467.\n",
      "iter: 134.0, RMSE on training set: 1.1434196504094487.\n",
      "iter: 135.0, RMSE on training set: 1.1405174255901724.\n",
      "iter: 136.0, RMSE on training set: 1.1376402175848055.\n",
      "iter: 137.0, RMSE on training set: 1.1347882532516258.\n",
      "iter: 138.0, RMSE on training set: 1.1319617742639552.\n",
      "iter: 139.0, RMSE on training set: 1.129161035835614.\n",
      "iter: 140.0, RMSE on training set: 1.126386305421456.\n",
      "iter: 141.0, RMSE on training set: 1.123637861396876.\n",
      "iter: 142.0, RMSE on training set: 1.120915991720295.\n",
      "iter: 143.0, RMSE on training set: 1.1182209925827313.\n",
      "iter: 144.0, RMSE on training set: 1.1155531670485366.\n",
      "iter: 145.0, RMSE on training set: 1.112912823691503.\n",
      "iter: 146.0, RMSE on training set: 1.1103002752304845.\n",
      "iter: 147.0, RMSE on training set: 1.107715837168689.\n",
      "iter: 148.0, RMSE on training set: 1.1051598264407951.\n",
      "iter: 149.0, RMSE on training set: 1.102632560071988.\n",
      "iter: 150.0, RMSE on training set: 1.1001343538529782.\n",
      "iter: 151.0, RMSE on training set: 1.097665521035006.\n",
      "iter: 152.0, RMSE on training set: 1.0952263710487289.\n",
      "iter: 153.0, RMSE on training set: 1.0928172082508631.\n",
      "iter: 154.0, RMSE on training set: 1.0904383307022436.\n",
      "iter: 155.0, RMSE on training set: 1.0880900289809317.\n",
      "iter: 156.0, RMSE on training set: 1.0857725850337674.\n",
      "iter: 157.0, RMSE on training set: 1.0834862710696658.\n",
      "iter: 158.0, RMSE on training set: 1.0812313484976792.\n",
      "iter: 159.0, RMSE on training set: 1.079008066912717.\n",
      "iter: 160.0, RMSE on training set: 1.0768166631315252.\n",
      "iter: 161.0, RMSE on training set: 1.0746573602813063.\n",
      "iter: 162.0, RMSE on training set: 1.072530366943096.\n",
      "iter: 163.0, RMSE on training set: 1.0704358763517476.\n",
      "iter: 164.0, RMSE on training set: 1.0683740656541094.\n",
      "iter: 165.0, RMSE on training set: 1.0663450952266642.\n",
      "iter: 166.0, RMSE on training set: 1.064349108053708.\n",
      "iter: 167.0, RMSE on training set: 1.0623862291667872.\n",
      "iter: 168.0, RMSE on training set: 1.0604565651459157.\n",
      "iter: 169.0, RMSE on training set: 1.0585602036828017.\n",
      "iter: 170.0, RMSE on training set: 1.0566972132061188.\n",
      "iter: 171.0, RMSE on training set: 1.054867642568566.\n",
      "iter: 172.0, RMSE on training set: 1.05307152079531.\n",
      "iter: 173.0, RMSE on training set: 1.0513088568931577.\n",
      "iter: 174.0, RMSE on training set: 1.0495796397196178.\n",
      "iter: 175.0, RMSE on training set: 1.0478838379108752.\n",
      "iter: 176.0, RMSE on training set: 1.0462213998674417.\n",
      "iter: 177.0, RMSE on training set: 1.044592253796169.\n",
      "iter: 178.0, RMSE on training set: 1.0429963078070736.\n",
      "iter: 179.0, RMSE on training set: 1.0414334500633005.\n",
      "iter: 180.0, RMSE on training set: 1.0399035489823922.\n",
      "iter: 181.0, RMSE on training set: 1.0384064534868478.\n",
      "iter: 182.0, RMSE on training set: 1.0369419933018558.\n",
      "iter: 183.0, RMSE on training set: 1.035509979297922.\n",
      "iter: 184.0, RMSE on training set: 1.0341102038760002.\n",
      "iter: 185.0, RMSE on training set: 1.0327424413926458.\n",
      "iter: 186.0, RMSE on training set: 1.0314064486226209.\n",
      "iter: 187.0, RMSE on training set: 1.0301019652563197.\n",
      "iter: 188.0, RMSE on training set: 1.0288287144293444.\n",
      "iter: 189.0, RMSE on training set: 1.0275864032815618.\n",
      "iter: 190.0, RMSE on training set: 1.0263747235429261.\n",
      "iter: 191.0, RMSE on training set: 1.025193352143429.\n",
      "iter: 192.0, RMSE on training set: 1.0240419518445065.\n",
      "iter: 193.0, RMSE on training set: 1.0229201718893077.\n",
      "iter: 194.0, RMSE on training set: 1.021827648669256.\n",
      "iter: 195.0, RMSE on training set: 1.0207640064043948.\n",
      "iter: 196.0, RMSE on training set: 1.0197288578350492.\n",
      "iter: 197.0, RMSE on training set: 1.018721804922396.\n",
      "iter: 198.0, RMSE on training set: 1.017742439555599.\n",
      "iter: 199.0, RMSE on training set: 1.0167903442632245.\n",
      "iter: 200.0, RMSE on training set: 1.0158650929267283.\n",
      "iter: 201.0, RMSE on training set: 1.014966251493857.\n",
      "iter: 202.0, RMSE on training set: 1.0140933786899287.\n",
      "iter: 203.0, RMSE on training set: 1.0132460267250123.\n",
      "iter: 204.0, RMSE on training set: 1.0124237419951232.\n",
      "iter: 205.0, RMSE on training set: 1.0116260657756655.\n",
      "iter: 206.0, RMSE on training set: 1.010852534905441.\n",
      "iter: 207.0, RMSE on training set: 1.0101026824596464.\n",
      "iter: 208.0, RMSE on training set: 1.0093760384103885.\n",
      "iter: 209.0, RMSE on training set: 1.0086721302733554.\n",
      "iter: 210.0, RMSE on training set: 1.0079904837393712.\n",
      "iter: 211.0, RMSE on training set: 1.0073306232896857.\n",
      "iter: 212.0, RMSE on training set: 1.0066920727939292.\n",
      "iter: 213.0, RMSE on training set: 1.0060743560898042.\n",
      "iter: 214.0, RMSE on training set: 1.0054769975436608.\n",
      "iter: 215.0, RMSE on training set: 1.0048995225912096.\n",
      "iter: 216.0, RMSE on training set: 1.0043414582577643.\n",
      "iter: 217.0, RMSE on training set: 1.003802333657472.\n",
      "iter: 218.0, RMSE on training set: 1.003281680471093.\n",
      "iter: 219.0, RMSE on training set: 1.0027790334020181.\n",
      "iter: 220.0, RMSE on training set: 1.0022939306102647.\n",
      "iter: 221.0, RMSE on training set: 1.0018259141243004.\n",
      "iter: 222.0, RMSE on training set: 1.001374530230621.\n",
      "iter: 223.0, RMSE on training set: 1.0009393298410674.\n",
      "iter: 224.0, RMSE on training set: 1.0005198688379775.\n",
      "iter: 225.0, RMSE on training set: 1.0001157083972867.\n",
      "iter: 226.0, RMSE on training set: 0.9997264152897893.\n",
      "iter: 227.0, RMSE on training set: 0.9993515621608148.\n",
      "iter: 228.0, RMSE on training set: 0.9989907277886354.\n",
      "iter: 229.0, RMSE on training set: 0.9986434973219699.\n",
      "iter: 230.0, RMSE on training set: 0.9983094624969788.\n",
      "iter: 231.0, RMSE on training set: 0.997988221834229.\n",
      "iter: 232.0, RMSE on training set: 0.9976793808160781.\n",
      "iter: 233.0, RMSE on training set: 0.9973825520450266.\n",
      "iter: 234.0, RMSE on training set: 0.9970973553835808.\n",
      "iter: 235.0, RMSE on training set: 0.9968234180761786.\n",
      "iter: 236.0, RMSE on training set: 0.9965603748538034.\n",
      "iter: 237.0, RMSE on training set: 0.9963078680218546.\n",
      "iter: 238.0, RMSE on training set: 0.9960655475319415.\n",
      "iter: 239.0, RMSE on training set: 0.9958330710381986.\n",
      "iter: 240.0, RMSE on training set: 0.9956101039387877.\n",
      "iter: 241.0, RMSE on training set: 0.9953963194032174.\n",
      "iter: 242.0, RMSE on training set: 0.9951913983861478.\n",
      "iter: 243.0, RMSE on training set: 0.994995029628323.\n",
      "iter: 244.0, RMSE on training set: 0.9948069096452816.\n",
      "iter: 245.0, RMSE on training set: 0.9946267427044961.\n",
      "iter: 246.0, RMSE on training set: 0.9944542407915902.\n",
      "iter: 247.0, RMSE on training set: 0.9942891235662591.\n",
      "iter: 248.0, RMSE on training set: 0.9941311183085377.\n",
      "iter: 249.0, RMSE on training set: 0.9939799598560156.\n",
      "iter: 250.0, RMSE on training set: 0.9938353905326349.\n",
      "iter: 251.0, RMSE on training set: 0.9936971600696495.\n",
      "iter: 252.0, RMSE on training set: 0.9935650255193346.\n",
      "iter: 253.0, RMSE on training set: 0.9934387511620218.\n",
      "iter: 254.0, RMSE on training set: 0.9933181084070098.\n",
      "iter: 255.0, RMSE on training set: 0.9932028756878987.\n",
      "iter: 256.0, RMSE on training set: 0.993092838352848.\n",
      "iter: 257.0, RMSE on training set: 0.9929877885502915.\n",
      "iter: 258.0, RMSE on training set: 0.9928875251105712.\n",
      "iter: 259.0, RMSE on training set: 0.9927918534239532.\n",
      "RMSE on test data: 0.9927884867072101.\n",
      "Running num_features=15\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922762654420818.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922758515554405.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922704974042525.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922604371501577.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 3.6492217513937417.\n",
      "iter: 1.0, RMSE on training set: 3.449263425450958.\n",
      "iter: 2.0, RMSE on training set: 3.2736780370109564.\n",
      "iter: 3.0, RMSE on training set: 3.1192662886475184.\n",
      "iter: 4.0, RMSE on training set: 2.9833087632793878.\n",
      "iter: 5.0, RMSE on training set: 2.863406034676639.\n",
      "iter: 6.0, RMSE on training set: 2.7574466366223804.\n",
      "iter: 7.0, RMSE on training set: 2.6635799544461887.\n",
      "iter: 8.0, RMSE on training set: 2.580190611437612.\n",
      "iter: 9.0, RMSE on training set: 2.505874046654794.\n",
      "iter: 10.0, RMSE on training set: 2.4394133405651233.\n",
      "iter: 11.0, RMSE on training set: 2.3797574370961323.\n",
      "iter: 12.0, RMSE on training set: 2.3260009171230895.\n",
      "iter: 13.0, RMSE on training set: 2.2773654437609046.\n",
      "iter: 14.0, RMSE on training set: 2.2331829472271383.\n",
      "iter: 15.0, RMSE on training set: 2.1928805598017687.\n",
      "iter: 16.0, RMSE on training set: 2.1559672591686474.\n",
      "iter: 17.0, RMSE on training set: 2.122022136420961.\n",
      "iter: 18.0, RMSE on training set: 2.0906841750401934.\n",
      "iter: 19.0, RMSE on training set: 2.061643408815069.\n",
      "iter: 20.0, RMSE on training set: 2.034633318319586.\n",
      "iter: 21.0, RMSE on training set: 2.0094243250737787.\n",
      "iter: 22.0, RMSE on training set: 1.9858182476938255.\n",
      "iter: 23.0, RMSE on training set: 1.963643593265853.\n",
      "iter: 24.0, RMSE on training set: 1.9427515682707466.\n",
      "iter: 25.0, RMSE on training set: 1.923012705441786.\n",
      "iter: 26.0, RMSE on training set: 1.9043140150920086.\n",
      "iter: 27.0, RMSE on training set: 1.8865565811283505.\n",
      "iter: 28.0, RMSE on training set: 1.8696535328227906.\n",
      "iter: 29.0, RMSE on training set: 1.8535283332503185.\n",
      "iter: 30.0, RMSE on training set: 1.8381133340589084.\n",
      "iter: 31.0, RMSE on training set: 1.8233485539150258.\n",
      "iter: 32.0, RMSE on training set: 1.8091806446244725.\n",
      "iter: 33.0, RMSE on training set: 1.7955620146455724.\n",
      "iter: 34.0, RMSE on training set: 1.7824500845854687.\n",
      "iter: 35.0, RMSE on training set: 1.769806653400244.\n",
      "iter: 36.0, RMSE on training set: 1.757597357502676.\n",
      "iter: 37.0, RMSE on training set: 1.7457912079082893.\n",
      "iter: 38.0, RMSE on training set: 1.734360193003432.\n",
      "iter: 39.0, RMSE on training set: 1.7232789365709738.\n",
      "iter: 40.0, RMSE on training set: 1.7125244024233928.\n",
      "iter: 41.0, RMSE on training set: 1.7020756384234728.\n",
      "iter: 42.0, RMSE on training set: 1.6919135538650603.\n",
      "iter: 43.0, RMSE on training set: 1.6820207251783754.\n",
      "iter: 44.0, RMSE on training set: 1.6723812257484874.\n",
      "iter: 45.0, RMSE on training set: 1.6629804763192904.\n",
      "iter: 46.0, RMSE on training set: 1.653805113022775.\n",
      "iter: 47.0, RMSE on training set: 1.644842870545128.\n",
      "iter: 48.0, RMSE on training set: 1.6360824783342756.\n",
      "iter: 49.0, RMSE on training set: 1.6275135680820683.\n",
      "iter: 50.0, RMSE on training set: 1.6191265909890078.\n",
      "iter: 51.0, RMSE on training set: 1.6109127435495234.\n",
      "iter: 52.0, RMSE on training set: 1.6028639007882604.\n",
      "iter: 53.0, RMSE on training set: 1.5949725560387844.\n",
      "iter: 54.0, RMSE on training set: 1.5872317664905966.\n",
      "iter: 55.0, RMSE on training set: 1.5796351038427028.\n",
      "iter: 56.0, RMSE on training set: 1.5721766094959586.\n",
      "iter: 57.0, RMSE on training set: 1.564850753794997.\n",
      "iter: 58.0, RMSE on training set: 1.5576523988966071.\n",
      "iter: 59.0, RMSE on training set: 1.5505767648970847.\n",
      "iter: 60.0, RMSE on training set: 1.543619398898127.\n",
      "iter: 61.0, RMSE on training set: 1.5367761467311478.\n",
      "iter: 62.0, RMSE on training set: 1.5300431270941852.\n",
      "iter: 63.0, RMSE on training set: 1.523416707885245.\n",
      "iter: 64.0, RMSE on training set: 1.5168934845413835.\n",
      "iter: 65.0, RMSE on training set: 1.5104702602145286.\n",
      "iter: 66.0, RMSE on training set: 1.5041440276334583.\n",
      "iter: 67.0, RMSE on training set: 1.4979119525167028.\n",
      "iter: 68.0, RMSE on training set: 1.4917713584137076.\n",
      "iter: 69.0, RMSE on training set: 1.4857197128621693.\n",
      "iter: 70.0, RMSE on training set: 1.4797546147580996.\n",
      "iter: 71.0, RMSE on training set: 1.4738737828429271.\n",
      "iter: 72.0, RMSE on training set: 1.4680750452187572.\n",
      "iter: 73.0, RMSE on training set: 1.46235632980948.\n",
      "iter: 74.0, RMSE on training set: 1.456715655691554.\n",
      "iter: 75.0, RMSE on training set: 1.4511511252242435.\n",
      "iter: 76.0, RMSE on training set: 1.445660916914569.\n",
      "iter: 77.0, RMSE on training set: 1.4402432789574355.\n",
      "iter: 78.0, RMSE on training set: 1.434896523395999.\n",
      "iter: 79.0, RMSE on training set: 1.4296190208515793.\n",
      "iter: 80.0, RMSE on training set: 1.4244091957761846.\n",
      "iter: 81.0, RMSE on training set: 1.4192655221841204.\n",
      "iter: 82.0, RMSE on training set: 1.4141865198222314.\n",
      "iter: 83.0, RMSE on training set: 1.4091707507412.\n",
      "iter: 84.0, RMSE on training set: 1.4042168162328692.\n",
      "iter: 85.0, RMSE on training set: 1.3993233541011134.\n",
      "iter: 86.0, RMSE on training set: 1.3944890362359608.\n",
      "iter: 87.0, RMSE on training set: 1.3897125664629215.\n",
      "iter: 88.0, RMSE on training set: 1.3849926786414517.\n",
      "iter: 89.0, RMSE on training set: 1.3803281349883987.\n",
      "iter: 90.0, RMSE on training set: 1.375717724604113.\n",
      "iter: 91.0, RMSE on training set: 1.3711602621806063.\n",
      "iter: 92.0, RMSE on training set: 1.3666545868727846.\n",
      "iter: 93.0, RMSE on training set: 1.3621995613154196.\n",
      "iter: 94.0, RMSE on training set: 1.357794070770011.\n",
      "iter: 95.0, RMSE on training set: 1.3534370223871903.\n",
      "iter: 96.0, RMSE on training set: 1.3491273445717729.\n",
      "iter: 97.0, RMSE on training set: 1.3448639864388157.\n",
      "iter: 98.0, RMSE on training set: 1.340645917350419.\n",
      "iter: 99.0, RMSE on training set: 1.3364721265240593.\n",
      "iter: 100.0, RMSE on training set: 1.332341622704444.\n",
      "iter: 101.0, RMSE on training set: 1.3282534338917777.\n",
      "iter: 102.0, RMSE on training set: 1.3242066071203153.\n",
      "iter: 103.0, RMSE on training set: 1.320200208281829.\n",
      "iter: 104.0, RMSE on training set: 1.316233321989386.\n",
      "iter: 105.0, RMSE on training set: 1.3123050514774282.\n",
      "iter: 106.0, RMSE on training set: 1.3084145185346658.\n",
      "iter: 107.0, RMSE on training set: 1.304560863466741.\n",
      "iter: 108.0, RMSE on training set: 1.3007432450859417.\n",
      "iter: 109.0, RMSE on training set: 1.2969608407255087.\n",
      "iter: 110.0, RMSE on training set: 1.2932128462762604.\n",
      "iter: 111.0, RMSE on training set: 1.2894984762434332.\n",
      "iter: 112.0, RMSE on training set: 1.2858169638217374.\n",
      "iter: 113.0, RMSE on training set: 1.2821675609867256.\n",
      "iter: 114.0, RMSE on training set: 1.2785495386006749.\n",
      "iter: 115.0, RMSE on training set: 1.2749621865312541.\n",
      "iter: 116.0, RMSE on training set: 1.27140481378131.\n",
      "iter: 117.0, RMSE on training set: 1.2678767486281965.\n",
      "iter: 118.0, RMSE on training set: 1.2643773387710657.\n",
      "iter: 119.0, RMSE on training set: 1.260905951484622.\n",
      "iter: 120.0, RMSE on training set: 1.257461973777741.\n",
      "iter: 121.0, RMSE on training set: 1.2540448125554586.\n",
      "iter: 122.0, RMSE on training set: 1.2506538947826729.\n",
      "iter: 123.0, RMSE on training set: 1.2472886676479495.\n",
      "iter: 124.0, RMSE on training set: 1.2439485987257173.\n",
      "iter: 125.0, RMSE on training set: 1.2406331761351137.\n",
      "iter: 126.0, RMSE on training set: 1.237341908693727.\n",
      "iter: 127.0, RMSE on training set: 1.2340743260644014.\n",
      "iter: 128.0, RMSE on training set: 1.230829978893323.\n",
      "iter: 129.0, RMSE on training set: 1.2276084389375552.\n",
      "iter: 130.0, RMSE on training set: 1.2244092991802258.\n",
      "iter: 131.0, RMSE on training set: 1.2212321739315808.\n",
      "iter: 132.0, RMSE on training set: 1.2180766989141794.\n",
      "iter: 133.0, RMSE on training set: 1.214942531330518.\n",
      "iter: 134.0, RMSE on training set: 1.2118293499114712.\n",
      "iter: 135.0, RMSE on training set: 1.20873685494399.\n",
      "iter: 136.0, RMSE on training set: 1.2056647682765962.\n",
      "iter: 137.0, RMSE on training set: 1.202612833301296.\n",
      "iter: 138.0, RMSE on training set: 1.1995808149106377.\n",
      "iter: 139.0, RMSE on training set: 1.1965684994287227.\n",
      "iter: 140.0, RMSE on training set: 1.193575694515067.\n",
      "iter: 141.0, RMSE on training set: 1.1906022290402887.\n",
      "iter: 142.0, RMSE on training set: 1.1876479529326653.\n",
      "iter: 143.0, RMSE on training set: 1.1847127369946784.\n",
      "iter: 144.0, RMSE on training set: 1.1817964726887462.\n",
      "iter: 145.0, RMSE on training set: 1.178899071891402.\n",
      "iter: 146.0, RMSE on training set: 1.1760204666153211.\n",
      "iter: 147.0, RMSE on training set: 1.173160608698697.\n",
      "iter: 148.0, RMSE on training set: 1.1703194694616057.\n",
      "iter: 149.0, RMSE on training set: 1.1674970393291861.\n",
      "iter: 150.0, RMSE on training set: 1.1646933274215936.\n",
      "iter: 151.0, RMSE on training set: 1.1619083611108998.\n",
      "iter: 152.0, RMSE on training set: 1.159142185545235.\n",
      "iter: 153.0, RMSE on training set: 1.1563948631406624.\n",
      "iter: 154.0, RMSE on training set: 1.1536664730413948.\n",
      "iter: 155.0, RMSE on training set: 1.150957110549092.\n",
      "iter: 156.0, RMSE on training set: 1.1482668865221373.\n",
      "iter: 157.0, RMSE on training set: 1.1455959267458249.\n",
      "iter: 158.0, RMSE on training set: 1.142944371274596.\n",
      "iter: 159.0, RMSE on training set: 1.140312373747478.\n",
      "iter: 160.0, RMSE on training set: 1.1377001006780616.\n",
      "iter: 161.0, RMSE on training set: 1.1351077307204107.\n",
      "iter: 162.0, RMSE on training set: 1.1325354539124477.\n",
      "iter: 163.0, RMSE on training set: 1.1299834708984322.\n",
      "iter: 164.0, RMSE on training set: 1.1274519921322936.\n",
      "iter: 165.0, RMSE on training set: 1.1249412370636378.\n",
      "iter: 166.0, RMSE on training set: 1.1224514333083657.\n",
      "iter: 167.0, RMSE on training set: 1.1199828158059149.\n",
      "iter: 168.0, RMSE on training set: 1.1175356259652058.\n",
      "iter: 169.0, RMSE on training set: 1.1151101108014265.\n",
      "iter: 170.0, RMSE on training set: 1.1127065220658636.\n",
      "iter: 171.0, RMSE on training set: 1.1103251153710145.\n",
      "iter: 172.0, RMSE on training set: 1.1079661493132489.\n",
      "iter: 173.0, RMSE on training set: 1.1056298845953334.\n",
      "iter: 174.0, RMSE on training set: 1.1033165831511322.\n",
      "iter: 175.0, RMSE on training set: 1.1010265072748109.\n",
      "iter: 176.0, RMSE on training set: 1.098759918756882.\n",
      "iter: 177.0, RMSE on training set: 1.0965170780293865.\n",
      "iter: 178.0, RMSE on training set: 1.0942982433225321.\n",
      "iter: 179.0, RMSE on training set: 1.0921036698350066.\n",
      "iter: 180.0, RMSE on training set: 1.089933608920199.\n",
      "iter: 181.0, RMSE on training set: 1.0877883072904366.\n",
      "iter: 182.0, RMSE on training set: 1.0856680062413302.\n",
      "iter: 183.0, RMSE on training set: 1.0835729408981614.\n",
      "iter: 184.0, RMSE on training set: 1.0815033394862044.\n",
      "iter: 185.0, RMSE on training set: 1.079459422626734.\n",
      "iter: 186.0, RMSE on training set: 1.0774414026603503.\n",
      "iter: 187.0, RMSE on training set: 1.0754494829991477.\n",
      "iter: 188.0, RMSE on training set: 1.0734838575090946.\n",
      "iter: 189.0, RMSE on training set: 1.0715447099238935.\n",
      "iter: 190.0, RMSE on training set: 1.0696322132914442.\n",
      "iter: 191.0, RMSE on training set: 1.0677465294538993.\n",
      "iter: 192.0, RMSE on training set: 1.0658878085621941.\n",
      "iter: 193.0, RMSE on training set: 1.0640561886257882.\n",
      "iter: 194.0, RMSE on training set: 1.0622517950982509.\n",
      "iter: 195.0, RMSE on training set: 1.0604747404992063.\n",
      "iter: 196.0, RMSE on training set: 1.0587251240730255.\n",
      "iter: 197.0, RMSE on training set: 1.0570030314845344.\n",
      "iter: 198.0, RMSE on training set: 1.0553085345519182.\n",
      "iter: 199.0, RMSE on training set: 1.053641691016849.\n",
      "iter: 200.0, RMSE on training set: 1.0520025443517698.\n",
      "iter: 201.0, RMSE on training set: 1.0503911236041397.\n",
      "iter: 202.0, RMSE on training set: 1.0488074432773316.\n",
      "iter: 203.0, RMSE on training set: 1.0472515032477614.\n",
      "iter: 204.0, RMSE on training set: 1.0457232887177126.\n",
      "iter: 205.0, RMSE on training set: 1.0442227702032014.\n",
      "iter: 206.0, RMSE on training set: 1.0427499035561594.\n",
      "iter: 207.0, RMSE on training set: 1.041304630020059.\n",
      "iter: 208.0, RMSE on training set: 1.0398868763180904.\n",
      "iter: 209.0, RMSE on training set: 1.0384965547728413.\n",
      "iter: 210.0, RMSE on training set: 1.0371335634564245.\n",
      "iter: 211.0, RMSE on training set: 1.0357977863698886.\n",
      "iter: 212.0, RMSE on training set: 1.0344890936507087.\n",
      "iter: 213.0, RMSE on training set: 1.0332073418070862.\n",
      "iter: 214.0, RMSE on training set: 1.031952373977785.\n",
      "iter: 215.0, RMSE on training set: 1.0307240202161185.\n",
      "iter: 216.0, RMSE on training set: 1.029522097796756.\n",
      "iter: 217.0, RMSE on training set: 1.028346411543923.\n",
      "iter: 218.0, RMSE on training set: 1.0271967541795735.\n",
      "iter: 219.0, RMSE on training set: 1.0260729066901155.\n",
      "iter: 220.0, RMSE on training set: 1.024974638710222.\n",
      "iter: 221.0, RMSE on training set: 1.0239017089222753.\n",
      "iter: 222.0, RMSE on training set: 1.022853865469987.\n",
      "iter: 223.0, RMSE on training set: 1.0218308463847348.\n",
      "iter: 224.0, RMSE on training set: 1.0208323800231622.\n",
      "iter: 225.0, RMSE on training set: 1.0198581855145965.\n",
      "iter: 226.0, RMSE on training set: 1.0189079732168547.\n",
      "iter: 227.0, RMSE on training set: 1.017981445179038.\n",
      "iter: 228.0, RMSE on training set: 1.0170782956099123.\n",
      "iter: 229.0, RMSE on training set: 1.0161982113505295.\n",
      "iter: 230.0, RMSE on training set: 1.0153408723497515.\n",
      "iter: 231.0, RMSE on training set: 1.0145059521414064.\n",
      "iter: 232.0, RMSE on training set: 1.0136931183218116.\n",
      "iter: 233.0, RMSE on training set: 1.0129020330264606.\n",
      "iter: 234.0, RMSE on training set: 1.0121323534047408.\n",
      "iter: 235.0, RMSE on training set: 1.0113837320915389.\n",
      "iter: 236.0, RMSE on training set: 1.0106558176747227.\n",
      "iter: 237.0, RMSE on training set: 1.0099482551574648.\n",
      "iter: 238.0, RMSE on training set: 1.009260686414488.\n",
      "iter: 239.0, RMSE on training set: 1.008592750641329.\n",
      "iter: 240.0, RMSE on training set: 1.007944084795796.\n",
      "iter: 241.0, RMSE on training set: 1.0073143240308395.\n",
      "iter: 242.0, RMSE on training set: 1.0067031021181205.\n",
      "iter: 243.0, RMSE on training set: 1.0061100518616102.\n",
      "iter: 244.0, RMSE on training set: 1.0055348055006064.\n",
      "iter: 245.0, RMSE on training set: 1.0049769951016299.\n",
      "iter: 246.0, RMSE on training set: 1.0044362529386897.\n",
      "iter: 247.0, RMSE on training set: 1.0039122118614747.\n",
      "iter: 248.0, RMSE on training set: 1.0034045056510972.\n",
      "iter: 249.0, RMSE on training set: 1.0029127693630326.\n",
      "iter: 250.0, RMSE on training set: 1.0024366396569773.\n",
      "iter: 251.0, RMSE on training set: 1.0019757551133956.\n",
      "iter: 252.0, RMSE on training set: 1.0015297565365564.\n",
      "iter: 253.0, RMSE on training set: 1.0010982872439236.\n",
      "iter: 254.0, RMSE on training set: 1.000680993341809.\n",
      "iter: 255.0, RMSE on training set: 1.000277523987224.\n",
      "iter: 256.0, RMSE on training set: 0.9998875316359274.\n",
      "iter: 257.0, RMSE on training set: 0.999510672276689.\n",
      "iter: 258.0, RMSE on training set: 0.9991466056518378.\n",
      "iter: 259.0, RMSE on training set: 0.9987949954641805.\n",
      "iter: 260.0, RMSE on training set: 0.9984555095704305.\n",
      "iter: 261.0, RMSE on training set: 0.9981278201613087.\n",
      "iter: 262.0, RMSE on training set: 0.9978116039284872.\n",
      "iter: 263.0, RMSE on training set: 0.9975065422185961.\n",
      "iter: 264.0, RMSE on training set: 0.9972123211745436.\n",
      "iter: 265.0, RMSE on training set: 0.9969286318643755.\n",
      "iter: 266.0, RMSE on training set: 0.9966551703979812.\n",
      "iter: 267.0, RMSE on training set: 0.996391638031923.\n",
      "iter: 268.0, RMSE on training set: 0.9961377412627053.\n",
      "iter: 269.0, RMSE on training set: 0.9958931919088039.\n",
      "iter: 270.0, RMSE on training set: 0.995657707181806.\n",
      "iter: 271.0, RMSE on training set: 0.9954310097469922.\n",
      "iter: 272.0, RMSE on training set: 0.9952128277737232.\n",
      "iter: 273.0, RMSE on training set: 0.9950028949760066.\n",
      "iter: 274.0, RMSE on training set: 0.9948009506436066.\n",
      "iter: 275.0, RMSE on training set: 0.9946067396640723.\n",
      "iter: 276.0, RMSE on training set: 0.9944200125360705.\n",
      "iter: 277.0, RMSE on training set: 0.9942405253744091.\n",
      "iter: 278.0, RMSE on training set: 0.9940680399071231.\n",
      "iter: 279.0, RMSE on training set: 0.9939023234650272.\n",
      "iter: 280.0, RMSE on training set: 0.9937431489640958.\n",
      "iter: 281.0, RMSE on training set: 0.9935902948810736.\n",
      "iter: 282.0, RMSE on training set: 0.9934435452226781.\n",
      "iter: 283.0, RMSE on training set: 0.993302689488782.\n",
      "iter: 284.0, RMSE on training set: 0.9931675226299275.\n",
      "iter: 285.0, RMSE on training set: 0.9930378449995496.\n",
      "iter: 286.0, RMSE on training set: 0.9929134623012547.\n",
      "iter: 287.0, RMSE on training set: 0.9927941855315016.\n",
      "iter: 288.0, RMSE on training set: 0.9926798309180315.\n",
      "iter: 289.0, RMSE on training set: 0.992570219854357.\n",
      "iter: 290.0, RMSE on training set: 0.9924651788306473.\n",
      "iter: 291.0, RMSE on training set: 0.9923645393613064.\n",
      "iter: 292.0, RMSE on training set: 0.9922681379095389.\n",
      "RMSE on test data: 0.9922650837952336.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrlkyMcYYk2uWTIwxxuTa/wM64EJ/c1SAfAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b6a2828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5         ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features_arr = [1, 3, 5, 7, 10, 13, 15]   # K in the lecture notes\n",
    "lambda_user = 0.9\n",
    "lambda_item = 0.1\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(num_features_arr))\n",
    "train_rmse_std = np.zeros(len(num_features_arr))\n",
    "validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "for i, num_features in enumerate(num_features_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running num_features={n}'.format(n=num_features))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation_minimalist(ratings, method, K, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    ## Calculate mean and standard deviation    \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(num_features_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(num_features_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Number of features (K)'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 4   # K in the lecture notes\n",
    "lambda_user_arr = [0.5, 0.75, 1, 1.25, 1.5, 2]\n",
    "lambda_item = 0.7\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "for i, lambda_user in enumerate(lambda_user_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings,  num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 5   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item_arr = [0.01, 0.1, 0.5, 1]\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "\n",
    "for i, lambda_item in enumerate(lambda_item_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_item={n}'.format(n=lambda_item))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings,  num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_item_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_item_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma_arr = [0.01, 0.1, 1]\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.5\n",
    "\n",
    "train_rmse_mean = np.zeros(len(gamma_arr))\n",
    "train_rmse_std = np.zeros(len(gamma_arr))\n",
    "validation_rmse_mean = np.zeros(len(gamma_arr))\n",
    "validation_rmse_std = np.zeros(len(gamma_arr))\n",
    "\n",
    "for i, gamma in enumerate(gamma_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running gamma={n}'.format(n=gamma))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(gamma_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(gamma_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(gamma_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(gamma_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "#### 1. Compare SGD, ALS with the best set of parameters (based on above results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
