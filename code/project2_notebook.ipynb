{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from helpers import *\n",
    "from plots import *\n",
    "from plots import *\n",
    "from split_data import *\n",
    "from recommender import *\n",
    "from cross_validation import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\u001c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "path_dataset = \"../data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAakAAAEbCAYAAABgLnslAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncs45l1n/HyRRptq0/AuwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103e50908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min # of items per user = 8, min # of users per item = 3.\n"
     ]
    }
   ],
   "source": [
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)\n",
    "\n",
    "print(\"min # of items per user = {}, min # of users per item = {}.\".format(\n",
    "        min(num_items_per_user), min(num_users_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_ratings, train_validation, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "#plot_train_test_data(train_validation, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Matrix factorisation using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run run.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run run.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods\n",
    "### CCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "number of items: 10000, number of users: 1000\n",
      "Preprocessing data\n",
      "Splitting data into train and test sets\n",
      "Training model\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0.0, RMSE on training set: 2.9950929124619052.\n",
      "iter: 1.0, RMSE on training set: 2.8375133312177385.\n",
      "iter: 2.0, RMSE on training set: 2.698924358850894.\n",
      "iter: 3.0, RMSE on training set: 2.5769580418708635.\n",
      "iter: 4.0, RMSE on training set: 2.4695058435338724.\n",
      "iter: 5.0, RMSE on training set: 2.3747001823458365.\n",
      "iter: 6.0, RMSE on training set: 2.2908928062854743.\n",
      "iter: 7.0, RMSE on training set: 2.2166350497167655.\n",
      "iter: 8.0, RMSE on training set: 2.1506589227177604.\n",
      "iter: 9.0, RMSE on training set: 2.0918589698662795.\n",
      "iter: 10.0, RMSE on training set: 2.0392749650854327.\n",
      "iter: 11.0, RMSE on training set: 1.9920755622374824.\n",
      "iter: 12.0, RMSE on training set: 1.949543027272273.\n",
      "iter: 13.0, RMSE on training set: 1.9110591555029883.\n",
      "iter: 14.0, RMSE on training set: 1.8760924395755691.\n",
      "iter: 15.0, RMSE on training set: 1.8441865107720288.\n",
      "iter: 16.0, RMSE on training set: 1.8149498360746452.\n",
      "iter: 17.0, RMSE on training set: 1.7880466202723584.\n",
      "iter: 18.0, RMSE on training set: 1.763188838006205.\n",
      "iter: 19.0, RMSE on training set: 1.740129304928859.\n",
      "iter: 20.0, RMSE on training set: 1.7186556889934048.\n",
      "iter: 21.0, RMSE on training set: 1.6985853607671593.\n",
      "iter: 22.0, RMSE on training set: 1.6797609840183791.\n",
      "iter: 23.0, RMSE on training set: 1.6620467532315155.\n",
      "iter: 24.0, RMSE on training set: 1.6453251919953749.\n",
      "iter: 25.0, RMSE on training set: 1.6294944344610105.\n",
      "iter: 26.0, RMSE on training set: 1.6144659206067447.\n",
      "iter: 27.0, RMSE on training set: 1.6001624444076923.\n",
      "iter: 28.0, RMSE on training set: 1.5865165018872058.\n",
      "iter: 29.0, RMSE on training set: 1.5734688932574752.\n",
      "iter: 30.0, RMSE on training set: 1.5609675398599663.\n",
      "iter: 31.0, RMSE on training set: 1.5489664823792688.\n",
      "iter: 32.0, RMSE on training set: 1.5374250318510028.\n",
      "iter: 33.0, RMSE on training set: 1.5263070493615978.\n",
      "iter: 34.0, RMSE on training set: 1.515580334102754.\n",
      "iter: 35.0, RMSE on training set: 1.5052161026589794.\n",
      "iter: 36.0, RMSE on training set: 1.4951885451373188.\n",
      "iter: 37.0, RMSE on training set: 1.4854744460575666.\n",
      "iter: 38.0, RMSE on training set: 1.4760528598679963.\n",
      "iter: 39.0, RMSE on training set: 1.4669048325894816.\n",
      "iter: 40.0, RMSE on training set: 1.4580131624664743.\n",
      "iter: 41.0, RMSE on training set: 1.4493621936568162.\n",
      "iter: 42.0, RMSE on training set: 1.440937637958127.\n",
      "iter: 43.0, RMSE on training set: 1.4327264203760235.\n",
      "iter: 44.0, RMSE on training set: 1.4247165450141406.\n",
      "iter: 45.0, RMSE on training set: 1.4168969783293368.\n",
      "iter: 46.0, RMSE on training set: 1.4092575472660198.\n",
      "iter: 47.0, RMSE on training set: 1.4017888501762632.\n",
      "iter: 48.0, RMSE on training set: 1.3944821787603745.\n",
      "iter: 49.0, RMSE on training set: 1.3873294495360375.\n",
      "iter: 50.0, RMSE on training set: 1.3803231435725267.\n",
      "iter: 51.0, RMSE on training set: 1.3734562534168318.\n",
      "iter: 52.0, RMSE on training set: 1.3667222362974638.\n",
      "iter: 53.0, RMSE on training set: 1.3601149728243156.\n",
      "iter: 54.0, RMSE on training set: 1.3536287305136576.\n",
      "iter: 55.0, RMSE on training set: 1.3472581315596857.\n",
      "iter: 56.0, RMSE on training set: 1.3409981243513214.\n",
      "iter: 57.0, RMSE on training set: 1.334843958297346.\n",
      "iter: 58.0, RMSE on training set: 1.3287911615771115.\n",
      "iter: 59.0, RMSE on training set: 1.3228355214794054.\n",
      "iter: 60.0, RMSE on training set: 1.3169730670305941.\n",
      "iter: 61.0, RMSE on training set: 1.3112000536458304.\n",
      "iter: 62.0, RMSE on training set: 1.3055129495654407.\n",
      "iter: 63.0, RMSE on training set: 1.2999084238630858.\n",
      "iter: 64.0, RMSE on training set: 1.29438333583392.\n",
      "iter: 65.0, RMSE on training set: 1.2889347255900612.\n",
      "iter: 66.0, RMSE on training set: 1.2835598057077426.\n",
      "iter: 67.0, RMSE on training set: 1.278255953785729.\n",
      "iter: 68.0, RMSE on training set: 1.2730207057882998.\n",
      "iter: 69.0, RMSE on training set: 1.267851750058399.\n",
      "iter: 70.0, RMSE on training set: 1.2627469218977694.\n",
      "iter: 71.0, RMSE on training set: 1.2577041986210435.\n",
      "iter: 72.0, RMSE on training set: 1.2527216950001052.\n",
      "iter: 73.0, RMSE on training set: 1.2477976590236646.\n",
      "iter: 74.0, RMSE on training set: 1.242930467904914.\n",
      "iter: 75.0, RMSE on training set: 1.238118624277597.\n",
      "iter: 76.0, RMSE on training set: 1.2333607525276744.\n",
      "iter: 77.0, RMSE on training set: 1.2286555952142264.\n",
      "iter: 78.0, RMSE on training set: 1.2240020095392434.\n",
      "iter: 79.0, RMSE on training set: 1.219398963831547.\n",
      "iter: 80.0, RMSE on training set: 1.214845534015374.\n",
      "iter: 81.0, RMSE on training set: 1.2103409000390697.\n",
      "iter: 82.0, RMSE on training set: 1.2058843422439653.\n",
      "iter: 83.0, RMSE on training set: 1.2014752376578703.\n",
      "iter: 84.0, RMSE on training set: 1.1971130562016397.\n",
      "iter: 85.0, RMSE on training set: 1.1927973568010817.\n",
      "iter: 86.0, RMSE on training set: 1.1885277833999905.\n",
      "iter: 87.0, RMSE on training set: 1.184304060873265.\n",
      "iter: 88.0, RMSE on training set: 1.1801259908420894.\n",
      "iter: 89.0, RMSE on training set: 1.1759934473957403.\n",
      "iter: 90.0, RMSE on training set: 1.1719063727270331.\n",
      "iter: 91.0, RMSE on training set: 1.167864772690451.\n",
      "iter: 92.0, RMSE on training set: 1.1638687122939024.\n",
      "iter: 93.0, RMSE on training set: 1.1599183111364988.\n",
      "iter: 94.0, RMSE on training set: 1.1560137388061325.\n",
      "iter: 95.0, RMSE on training set: 1.152155210251541.\n",
      "iter: 96.0, RMSE on training set: 1.1483429811443606.\n",
      "iter: 97.0, RMSE on training set: 1.144577343247124.\n",
      "iter: 98.0, RMSE on training set: 1.1408586198034265.\n",
      "iter: 99.0, RMSE on training set: 1.137187160966526.\n",
      "iter: 100.0, RMSE on training set: 1.133563339282457.\n",
      "iter: 101.0, RMSE on training set: 1.1299875452434098.\n",
      "iter: 102.0, RMSE on training set: 1.1264601829266339.\n",
      "iter: 103.0, RMSE on training set: 1.1229816657334972.\n",
      "iter: 104.0, RMSE on training set: 1.1195524122426386.\n",
      "iter: 105.0, RMSE on training set: 1.1161728421903083.\n",
      "iter: 106.0, RMSE on training set: 1.1128433725901763.\n",
      "iter: 107.0, RMSE on training set: 1.1095644140039398.\n",
      "iter: 108.0, RMSE on training set: 1.1063363669731185.\n",
      "iter: 109.0, RMSE on training set: 1.1031596186214798.\n",
      "iter: 110.0, RMSE on training set: 1.1000345394364839.\n",
      "iter: 111.0, RMSE on training set: 1.0969614802371752.\n",
      "iter: 112.0, RMSE on training set: 1.0939407693348857.\n",
      "iter: 113.0, RMSE on training set: 1.0909727098921183.\n",
      "iter: 114.0, RMSE on training set: 1.088057577483889.\n",
      "iter: 115.0, RMSE on training set: 1.0851956178648468.\n",
      "iter: 116.0, RMSE on training set: 1.0823870449443806.\n",
      "iter: 117.0, RMSE on training set: 1.079632038970979.\n",
      "iter: 118.0, RMSE on training set: 1.076930744926076.\n",
      "iter: 119.0, RMSE on training set: 1.0742832711266361.\n",
      "iter: 120.0, RMSE on training set: 1.0716896880348525.\n",
      "iter: 121.0, RMSE on training set: 1.0691500272723484.\n",
      "iter: 122.0, RMSE on training set: 1.0666642808354825.\n",
      "iter: 123.0, RMSE on training set: 1.064232400507507.\n",
      "iter: 124.0, RMSE on training set: 1.061854297462582.\n",
      "iter: 125.0, RMSE on training set: 1.0595298420559118.\n",
      "iter: 126.0, RMSE on training set: 1.0572588637936327.\n",
      "iter: 127.0, RMSE on training set: 1.0550411514754612.\n",
      "iter: 128.0, RMSE on training set: 1.0528764535025639.\n",
      "iter: 129.0, RMSE on training set: 1.0507644783426207.\n",
      "iter: 130.0, RMSE on training set: 1.0487048951436038.\n",
      "iter: 131.0, RMSE on training set: 1.046697334487469.\n",
      "iter: 132.0, RMSE on training set: 1.044741389274569.\n",
      "iter: 133.0, RMSE on training set: 1.0428366157294342.\n",
      "iter: 134.0, RMSE on training set: 1.0409825345183268.\n",
      "iter: 135.0, RMSE on training set: 1.0391786319688625.\n",
      "iter: 136.0, RMSE on training set: 1.037424361381951.\n",
      "iter: 137.0, RMSE on training set: 1.0357191444262908.\n",
      "iter: 138.0, RMSE on training set: 1.0340623726057043.\n",
      "iter: 139.0, RMSE on training set: 1.032453408789732.\n",
      "iter: 140.0, RMSE on training set: 1.0308915887980297.\n",
      "iter: 141.0, RMSE on training set: 1.0293762230293635.\n",
      "iter: 142.0, RMSE on training set: 1.0279065981261855.\n",
      "iter: 143.0, RMSE on training set: 1.0264819786661297.\n",
      "iter: 144.0, RMSE on training set: 1.0251016088720142.\n",
      "iter: 145.0, RMSE on training set: 1.0237647143323487.\n",
      "iter: 146.0, RMSE on training set: 1.0224705037246753.\n",
      "iter: 147.0, RMSE on training set: 1.0212181705344958.\n",
      "iter: 148.0, RMSE on training set: 1.020006894762938.\n",
      "iter: 149.0, RMSE on training set: 1.0188358446167638.\n",
      "iter: 150.0, RMSE on training set: 1.0177041781747411.\n",
      "iter: 151.0, RMSE on training set: 1.016611045024874.\n",
      "iter: 152.0, RMSE on training set: 1.0155555878674312.\n",
      "iter: 153.0, RMSE on training set: 1.014536944079172.\n",
      "iter: 154.0, RMSE on training set: 1.0135542472346215.\n",
      "iter: 155.0, RMSE on training set: 1.0126066285807058.\n",
      "iter: 156.0, RMSE on training set: 1.011693218461488.\n",
      "iter: 157.0, RMSE on training set: 1.0108131476901934.\n",
      "iter: 158.0, RMSE on training set: 1.0099655488661057.\n",
      "iter: 159.0, RMSE on training set: 1.0091495576343674.\n",
      "iter: 160.0, RMSE on training set: 1.0083643138870448.\n",
      "iter: 161.0, RMSE on training set: 1.007608962904249.\n",
      "iter: 162.0, RMSE on training set: 1.0068826564344022.\n",
      "iter: 163.0, RMSE on training set: 1.006184553713112.\n",
      "iter: 164.0, RMSE on training set: 1.0055138224204048.\n",
      "iter: 165.0, RMSE on training set: 1.0048696395763415.\n",
      "iter: 166.0, RMSE on training set: 1.0042511923753539.\n",
      "iter: 167.0, RMSE on training set: 1.0036576789598155.\n",
      "iter: 168.0, RMSE on training set: 1.0030883091336416.\n",
      "iter: 169.0, RMSE on training set: 1.0025423050168702.\n",
      "iter: 170.0, RMSE on training set: 1.0020189016423795.\n",
      "iter: 171.0, RMSE on training set: 1.0015173474960257.\n",
      "iter: 172.0, RMSE on training set: 1.0010369050016446.\n",
      "iter: 173.0, RMSE on training set: 1.00057685095247.\n",
      "iter: 174.0, RMSE on training set: 1.0001364768906074.\n",
      "iter: 175.0, RMSE on training set: 0.9997150894362942.\n",
      "iter: 176.0, RMSE on training set: 0.9993120105687514.\n",
      "iter: 177.0, RMSE on training set: 0.9989265778604557.\n",
      "iter: 178.0, RMSE on training set: 0.9985581446667368.\n",
      "iter: 179.0, RMSE on training set: 0.9982060802725955.\n",
      "iter: 180.0, RMSE on training set: 0.9978697699986987.\n",
      "iter: 181.0, RMSE on training set: 0.9975486152684686.\n",
      "iter: 182.0, RMSE on training set: 0.997242033638235.\n",
      "iter: 183.0, RMSE on training set: 0.9969494587923851.\n",
      "iter: 184.0, RMSE on training set: 0.9966703405054541.\n",
      "iter: 185.0, RMSE on training set: 0.9964041445730819.\n",
      "iter: 186.0, RMSE on training set: 0.9961503527137577.\n",
      "iter: 187.0, RMSE on training set: 0.995908462443239.\n",
      "iter: 188.0, RMSE on training set: 0.9956779869235384.\n",
      "iter: 189.0, RMSE on training set: 0.9954584547883318.\n",
      "iter: 190.0, RMSE on training set: 0.9952494099466169.\n",
      "iter: 191.0, RMSE on training set: 0.995050411366437.\n",
      "iter: 192.0, RMSE on training set: 0.9948610328404519.\n",
      "iter: 193.0, RMSE on training set: 0.9946808627350946.\n",
      "iter: 194.0, RMSE on training set: 0.994509503725018.\n",
      "iter: 195.0, RMSE on training set: 0.9943465725145186.\n",
      "iter: 196.0, RMSE on training set: 0.9941916995475322.\n",
      "iter: 197.0, RMSE on training set: 0.9940445287077938.\n",
      "iter: 198.0, RMSE on training set: 0.9939047170106623.\n",
      "iter: 199.0, RMSE on training set: 0.9937719342880723.\n",
      "iter: 200.0, RMSE on training set: 0.9936458628679986.\n",
      "iter: 201.0, RMSE on training set: 0.9935261972497534.\n",
      "iter: 202.0, RMSE on training set: 0.993412643776371.\n",
      "iter: 203.0, RMSE on training set: 0.9933049203052366.\n",
      "iter: 204.0, RMSE on training set: 0.9932027558780667.\n",
      "iter: 205.0, RMSE on training set: 0.993105890391254.\n",
      "RMSE on test data: 1.0128703895487268.\n",
      "RMSE on train data: 0.993105890391254.\n",
      "RMSE on test data: 1.0128703895487268.\n"
     ]
    }
   ],
   "source": [
    "%run run.py 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running num_features=1\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964587238475606.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964584304313565.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964523313834746.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964418033298583.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964463963413265.\n",
      "Running num_features=3\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 1.7161553199022836.\n",
      "iter: 1, RMSE on training set: 1.6478830496307406.\n",
      "iter: 2, RMSE on training set: 1.5869549830257481.\n",
      "iter: 3, RMSE on training set: 1.5325465401120755.\n",
      "iter: 4, RMSE on training set: 1.4839120011059341.\n",
      "iter: 5, RMSE on training set: 1.4403812921802261.\n",
      "iter: 6, RMSE on training set: 1.4013557156997305.\n",
      "iter: 7, RMSE on training set: 1.3663032542714906.\n",
      "iter: 8, RMSE on training set: 1.3347535421580987.\n",
      "iter: 9, RMSE on training set: 1.3062926373090427.\n",
      "iter: 10, RMSE on training set: 1.2805577307788125.\n",
      "iter: 11, RMSE on training set: 1.2572319222936001.\n",
      "iter: 12, RMSE on training set: 1.2360391736554153.\n",
      "iter: 13, RMSE on training set: 1.2167395289745746.\n",
      "iter: 14, RMSE on training set: 1.1991246659795536.\n",
      "iter: 15, RMSE on training set: 1.183013818808102.\n",
      "iter: 16, RMSE on training set: 1.168250091676736.\n",
      "iter: 17, RMSE on training set: 1.1546971656262375.\n",
      "iter: 18, RMSE on training set: 1.1422363873733172.\n",
      "iter: 19, RMSE on training set: 1.1307642199110595.\n",
      "iter: 20, RMSE on training set: 1.1201900283860717.\n",
      "iter: 21, RMSE on training set: 1.110434171335779.\n",
      "iter: 22, RMSE on training set: 1.1014263659941002.\n",
      "iter: 23, RMSE on training set: 1.0931042965250943.\n",
      "iter: 24, RMSE on training set: 1.0854124352618721.\n",
      "iter: 25, RMSE on training set: 1.0783010489406102.\n",
      "iter: 26, RMSE on training set: 1.071725364239376.\n",
      "iter: 27, RMSE on training set: 1.0656448694441174.\n",
      "iter: 28, RMSE on training set: 1.0600227316156443.\n",
      "iter: 29, RMSE on training set: 1.0548253111155905.\n",
      "iter: 30, RMSE on training set: 1.0500217576965911.\n",
      "iter: 31, RMSE on training set: 1.0455836745298361.\n",
      "iter: 32, RMSE on training set: 1.0414848385096187.\n",
      "iter: 33, RMSE on training set: 1.0377009669317663.\n",
      "iter: 34, RMSE on training set: 1.034209522194406.\n",
      "iter: 35, RMSE on training set: 1.0309895475258326.\n",
      "iter: 36, RMSE on training set: 1.0280215279204898.\n",
      "iter: 37, RMSE on training set: 1.0252872714770802.\n",
      "iter: 38, RMSE on training set: 1.022769807199309.\n",
      "iter: 39, RMSE on training set: 1.020453296055216.\n",
      "iter: 40, RMSE on training set: 1.0183229527096396.\n",
      "iter: 41, RMSE on training set: 1.0163649758592472.\n",
      "iter: 42, RMSE on training set: 1.0145664855236505.\n",
      "iter: 43, RMSE on training set: 1.0129154659920159.\n",
      "iter: 44, RMSE on training set: 1.0114007134044976.\n",
      "iter: 45, RMSE on training set: 1.0100117871730383.\n",
      "iter: 46, RMSE on training set: 1.008738964626292.\n",
      "iter: 47, RMSE on training set: 1.0075731984068912.\n",
      "iter: 48, RMSE on training set: 1.0065060762625073.\n",
      "iter: 49, RMSE on training set: 1.005529782960579.\n",
      "iter: 50, RMSE on training set: 1.0046370641245996.\n",
      "iter: 51, RMSE on training set: 1.0038211918413371.\n",
      "iter: 52, RMSE on training set: 1.003075931926472.\n",
      "iter: 53, RMSE on training set: 1.0023955127636943.\n",
      "iter: 54, RMSE on training set: 1.0017745956516542.\n",
      "iter: 55, RMSE on training set: 1.0012082466061598.\n",
      "iter: 56, RMSE on training set: 1.000691909573311.\n",
      "iter: 57, RMSE on training set: 1.0002213810140919.\n",
      "iter: 58, RMSE on training set: 0.9997927858232796.\n",
      "iter: 59, RMSE on training set: 0.9994025545462063.\n",
      "iter: 60, RMSE on training set: 0.9990474018565393.\n",
      "iter: 61, RMSE on training set: 0.9987243062571697.\n",
      "iter: 62, RMSE on training set: 0.9984304909650383.\n",
      "iter: 63, RMSE on training set: 0.9981634059392862.\n",
      "iter: 64, RMSE on training set: 0.9979207110108967.\n",
      "iter: 65, RMSE on training set: 0.9977002600708218.\n",
      "iter: 66, RMSE on training set: 0.9975000862727939.\n",
      "iter: 67, RMSE on training set: 0.9973183882064153.\n",
      "iter: 68, RMSE on training set: 0.997153516995811.\n",
      "iter: 69, RMSE on training set: 0.9970039642791223.\n",
      "iter: 70, RMSE on training set: 0.9968683510242837.\n",
      "iter: 71, RMSE on training set: 0.9967454171370183.\n",
      "iter: 72, RMSE on training set: 0.9966340118175466.\n",
      "iter: 73, RMSE on training set: 0.9965330846234047.\n",
      "iter: 74, RMSE on training set: 0.9964416771966422.\n",
      "RMSE on test data: 0.9964499342663414.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 1.7161553199022836.\n",
      "iter: 1, RMSE on training set: 1.6478830496307406.\n",
      "iter: 2, RMSE on training set: 1.5869549830257481.\n",
      "iter: 3, RMSE on training set: 1.5325465401120755.\n",
      "iter: 4, RMSE on training set: 1.4839120011059341.\n",
      "iter: 5, RMSE on training set: 1.4403812921802261.\n",
      "iter: 6, RMSE on training set: 1.4013557156997305.\n",
      "iter: 7, RMSE on training set: 1.3663032542714906.\n",
      "iter: 8, RMSE on training set: 1.3347535421580987.\n",
      "iter: 9, RMSE on training set: 1.3062926373090427.\n",
      "iter: 10, RMSE on training set: 1.2805577307788125.\n",
      "iter: 11, RMSE on training set: 1.2572319222936001.\n",
      "iter: 12, RMSE on training set: 1.2360391736554153.\n",
      "iter: 13, RMSE on training set: 1.2167395289745746.\n",
      "iter: 14, RMSE on training set: 1.1991246659795536.\n",
      "iter: 15, RMSE on training set: 1.183013818808102.\n",
      "iter: 16, RMSE on training set: 1.168250091676736.\n",
      "iter: 17, RMSE on training set: 1.1546971656262375.\n",
      "iter: 18, RMSE on training set: 1.1422363873733172.\n",
      "iter: 19, RMSE on training set: 1.1307642199110595.\n",
      "iter: 20, RMSE on training set: 1.1201900283860717.\n",
      "iter: 21, RMSE on training set: 1.110434171335779.\n",
      "iter: 22, RMSE on training set: 1.1014263659941002.\n",
      "iter: 23, RMSE on training set: 1.0931042965250943.\n",
      "iter: 24, RMSE on training set: 1.0854124352618721.\n",
      "iter: 25, RMSE on training set: 1.0783010489406102.\n",
      "iter: 26, RMSE on training set: 1.071725364239376.\n",
      "iter: 27, RMSE on training set: 1.0656448694441174.\n",
      "iter: 28, RMSE on training set: 1.0600227316156443.\n",
      "iter: 29, RMSE on training set: 1.0548253111155905.\n",
      "iter: 30, RMSE on training set: 1.0500217576965911.\n",
      "iter: 31, RMSE on training set: 1.0455836745298361.\n",
      "iter: 32, RMSE on training set: 1.0414848385096187.\n",
      "iter: 33, RMSE on training set: 1.0377009669317663.\n",
      "iter: 34, RMSE on training set: 1.034209522194406.\n",
      "iter: 35, RMSE on training set: 1.0309895475258326.\n",
      "iter: 36, RMSE on training set: 1.0280215279204898.\n",
      "iter: 37, RMSE on training set: 1.0252872714770802.\n",
      "iter: 38, RMSE on training set: 1.022769807199309.\n",
      "iter: 39, RMSE on training set: 1.020453296055216.\n",
      "iter: 40, RMSE on training set: 1.0183229527096396.\n",
      "iter: 41, RMSE on training set: 1.0163649758592472.\n",
      "iter: 42, RMSE on training set: 1.0145664855236505.\n",
      "iter: 43, RMSE on training set: 1.0129154659920159.\n",
      "iter: 44, RMSE on training set: 1.0114007134044976.\n",
      "iter: 45, RMSE on training set: 1.0100117871730383.\n",
      "iter: 46, RMSE on training set: 1.008738964626292.\n",
      "iter: 47, RMSE on training set: 1.0075731984068912.\n",
      "iter: 48, RMSE on training set: 1.0065060762625073.\n",
      "iter: 49, RMSE on training set: 1.005529782960579.\n",
      "iter: 50, RMSE on training set: 1.0046370641245996.\n",
      "iter: 51, RMSE on training set: 1.0038211918413371.\n",
      "iter: 52, RMSE on training set: 1.003075931926472.\n",
      "iter: 53, RMSE on training set: 1.0023955127636943.\n",
      "iter: 54, RMSE on training set: 1.0017745956516542.\n",
      "iter: 55, RMSE on training set: 1.0012082466061598.\n",
      "iter: 56, RMSE on training set: 1.000691909573311.\n",
      "iter: 57, RMSE on training set: 1.0002213810140919.\n",
      "iter: 58, RMSE on training set: 0.9997927858232796.\n",
      "iter: 59, RMSE on training set: 0.9994025545462063.\n",
      "iter: 60, RMSE on training set: 0.9990474018565393.\n",
      "iter: 61, RMSE on training set: 0.9987243062571697.\n",
      "iter: 62, RMSE on training set: 0.9984304909650383.\n",
      "iter: 63, RMSE on training set: 0.9981634059392862.\n",
      "iter: 64, RMSE on training set: 0.9979207110108967.\n",
      "iter: 65, RMSE on training set: 0.9977002600708218.\n",
      "iter: 66, RMSE on training set: 0.9975000862727939.\n",
      "iter: 67, RMSE on training set: 0.9973183882064153.\n",
      "iter: 68, RMSE on training set: 0.997153516995811.\n",
      "iter: 69, RMSE on training set: 0.9970039642791223.\n",
      "iter: 70, RMSE on training set: 0.9968683510242837.\n",
      "iter: 71, RMSE on training set: 0.9967454171370183.\n",
      "iter: 72, RMSE on training set: 0.9966340118175466.\n",
      "iter: 73, RMSE on training set: 0.9965330846234047.\n",
      "iter: 74, RMSE on training set: 0.9964416771966422.\n",
      "RMSE on test data: 0.9964497236314782.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 1.7161553199022836.\n",
      "iter: 1, RMSE on training set: 1.6478830496307406.\n",
      "iter: 2, RMSE on training set: 1.5869549830257481.\n",
      "iter: 3, RMSE on training set: 1.5325465401120755.\n",
      "iter: 4, RMSE on training set: 1.4839120011059341.\n",
      "iter: 5, RMSE on training set: 1.4403812921802261.\n",
      "iter: 6, RMSE on training set: 1.4013557156997305.\n",
      "iter: 7, RMSE on training set: 1.3663032542714906.\n",
      "iter: 8, RMSE on training set: 1.3347535421580987.\n",
      "iter: 9, RMSE on training set: 1.3062926373090427.\n",
      "iter: 10, RMSE on training set: 1.2805577307788125.\n",
      "iter: 11, RMSE on training set: 1.2572319222936001.\n",
      "iter: 12, RMSE on training set: 1.2360391736554153.\n",
      "iter: 13, RMSE on training set: 1.2167395289745746.\n",
      "iter: 14, RMSE on training set: 1.1991246659795536.\n",
      "iter: 15, RMSE on training set: 1.183013818808102.\n",
      "iter: 16, RMSE on training set: 1.168250091676736.\n",
      "iter: 17, RMSE on training set: 1.1546971656262375.\n",
      "iter: 18, RMSE on training set: 1.1422363873733172.\n",
      "iter: 19, RMSE on training set: 1.1307642199110595.\n",
      "iter: 20, RMSE on training set: 1.1201900283860717.\n",
      "iter: 21, RMSE on training set: 1.110434171335779.\n",
      "iter: 22, RMSE on training set: 1.1014263659941002.\n",
      "iter: 23, RMSE on training set: 1.0931042965250943.\n",
      "iter: 24, RMSE on training set: 1.0854124352618721.\n",
      "iter: 25, RMSE on training set: 1.0783010489406102.\n",
      "iter: 26, RMSE on training set: 1.071725364239376.\n",
      "iter: 27, RMSE on training set: 1.0656448694441174.\n",
      "iter: 28, RMSE on training set: 1.0600227316156443.\n",
      "iter: 29, RMSE on training set: 1.0548253111155905.\n",
      "iter: 30, RMSE on training set: 1.0500217576965911.\n",
      "iter: 31, RMSE on training set: 1.0455836745298361.\n",
      "iter: 32, RMSE on training set: 1.0414848385096187.\n",
      "iter: 33, RMSE on training set: 1.0377009669317663.\n",
      "iter: 34, RMSE on training set: 1.034209522194406.\n",
      "iter: 35, RMSE on training set: 1.0309895475258326.\n",
      "iter: 36, RMSE on training set: 1.0280215279204898.\n",
      "iter: 37, RMSE on training set: 1.0252872714770802.\n",
      "iter: 38, RMSE on training set: 1.022769807199309.\n",
      "iter: 39, RMSE on training set: 1.020453296055216.\n",
      "iter: 40, RMSE on training set: 1.0183229527096396.\n",
      "iter: 41, RMSE on training set: 1.0163649758592472.\n",
      "iter: 42, RMSE on training set: 1.0145664855236505.\n",
      "iter: 43, RMSE on training set: 1.0129154659920159.\n",
      "iter: 44, RMSE on training set: 1.0114007134044976.\n",
      "iter: 45, RMSE on training set: 1.0100117871730383.\n",
      "iter: 46, RMSE on training set: 1.008738964626292.\n",
      "iter: 47, RMSE on training set: 1.0075731984068912.\n",
      "iter: 48, RMSE on training set: 1.0065060762625073.\n",
      "iter: 49, RMSE on training set: 1.005529782960579.\n",
      "iter: 50, RMSE on training set: 1.0046370641245996.\n",
      "iter: 51, RMSE on training set: 1.0038211918413371.\n",
      "iter: 52, RMSE on training set: 1.003075931926472.\n",
      "iter: 53, RMSE on training set: 1.0023955127636943.\n",
      "iter: 54, RMSE on training set: 1.0017745956516542.\n",
      "iter: 55, RMSE on training set: 1.0012082466061598.\n",
      "iter: 56, RMSE on training set: 1.000691909573311.\n",
      "iter: 57, RMSE on training set: 1.0002213810140919.\n",
      "iter: 58, RMSE on training set: 0.9997927858232796.\n",
      "iter: 59, RMSE on training set: 0.9994025545462063.\n",
      "iter: 60, RMSE on training set: 0.9990474018565393.\n",
      "iter: 61, RMSE on training set: 0.9987243062571697.\n",
      "iter: 62, RMSE on training set: 0.9984304909650383.\n",
      "iter: 63, RMSE on training set: 0.9981634059392862.\n",
      "iter: 64, RMSE on training set: 0.9979207110108967.\n",
      "iter: 65, RMSE on training set: 0.9977002600708218.\n",
      "iter: 66, RMSE on training set: 0.9975000862727939.\n",
      "iter: 67, RMSE on training set: 0.9973183882064153.\n",
      "iter: 68, RMSE on training set: 0.997153516995811.\n",
      "iter: 69, RMSE on training set: 0.9970039642791223.\n",
      "iter: 70, RMSE on training set: 0.9968683510242837.\n",
      "iter: 71, RMSE on training set: 0.9967454171370183.\n",
      "iter: 72, RMSE on training set: 0.9966340118175466.\n",
      "iter: 73, RMSE on training set: 0.9965330846234047.\n",
      "iter: 74, RMSE on training set: 0.9964416771966422.\n",
      "RMSE on test data: 0.9964435512070303.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 1.7161553199022836.\n",
      "iter: 1, RMSE on training set: 1.6478830496307406.\n",
      "iter: 2, RMSE on training set: 1.5869549830257481.\n",
      "iter: 3, RMSE on training set: 1.5325465401120755.\n",
      "iter: 4, RMSE on training set: 1.4839120011059341.\n",
      "iter: 5, RMSE on training set: 1.4403812921802261.\n",
      "iter: 6, RMSE on training set: 1.4013557156997305.\n",
      "iter: 7, RMSE on training set: 1.3663032542714906.\n",
      "iter: 8, RMSE on training set: 1.3347535421580987.\n",
      "iter: 9, RMSE on training set: 1.3062926373090427.\n",
      "iter: 10, RMSE on training set: 1.2805577307788125.\n",
      "iter: 11, RMSE on training set: 1.2572319222936001.\n",
      "iter: 12, RMSE on training set: 1.2360391736554153.\n",
      "iter: 13, RMSE on training set: 1.2167395289745746.\n",
      "iter: 14, RMSE on training set: 1.1991246659795536.\n",
      "iter: 15, RMSE on training set: 1.183013818808102.\n",
      "iter: 16, RMSE on training set: 1.168250091676736.\n",
      "iter: 17, RMSE on training set: 1.1546971656262375.\n",
      "iter: 18, RMSE on training set: 1.1422363873733172.\n",
      "iter: 19, RMSE on training set: 1.1307642199110595.\n",
      "iter: 20, RMSE on training set: 1.1201900283860717.\n",
      "iter: 21, RMSE on training set: 1.110434171335779.\n",
      "iter: 22, RMSE on training set: 1.1014263659941002.\n",
      "iter: 23, RMSE on training set: 1.0931042965250943.\n",
      "iter: 24, RMSE on training set: 1.0854124352618721.\n",
      "iter: 25, RMSE on training set: 1.0783010489406102.\n",
      "iter: 26, RMSE on training set: 1.071725364239376.\n",
      "iter: 27, RMSE on training set: 1.0656448694441174.\n",
      "iter: 28, RMSE on training set: 1.0600227316156443.\n",
      "iter: 29, RMSE on training set: 1.0548253111155905.\n",
      "iter: 30, RMSE on training set: 1.0500217576965911.\n",
      "iter: 31, RMSE on training set: 1.0455836745298361.\n",
      "iter: 32, RMSE on training set: 1.0414848385096187.\n",
      "iter: 33, RMSE on training set: 1.0377009669317663.\n",
      "iter: 34, RMSE on training set: 1.034209522194406.\n",
      "iter: 35, RMSE on training set: 1.0309895475258326.\n",
      "iter: 36, RMSE on training set: 1.0280215279204898.\n",
      "iter: 37, RMSE on training set: 1.0252872714770802.\n",
      "iter: 38, RMSE on training set: 1.022769807199309.\n",
      "iter: 39, RMSE on training set: 1.020453296055216.\n",
      "iter: 40, RMSE on training set: 1.0183229527096396.\n",
      "iter: 41, RMSE on training set: 1.0163649758592472.\n",
      "iter: 42, RMSE on training set: 1.0145664855236505.\n",
      "iter: 43, RMSE on training set: 1.0129154659920159.\n",
      "iter: 44, RMSE on training set: 1.0114007134044976.\n",
      "iter: 45, RMSE on training set: 1.0100117871730383.\n",
      "iter: 46, RMSE on training set: 1.008738964626292.\n",
      "iter: 47, RMSE on training set: 1.0075731984068912.\n",
      "iter: 48, RMSE on training set: 1.0065060762625073.\n",
      "iter: 49, RMSE on training set: 1.005529782960579.\n",
      "iter: 50, RMSE on training set: 1.0046370641245996.\n",
      "iter: 51, RMSE on training set: 1.0038211918413371.\n",
      "iter: 52, RMSE on training set: 1.003075931926472.\n",
      "iter: 53, RMSE on training set: 1.0023955127636943.\n",
      "iter: 54, RMSE on training set: 1.0017745956516542.\n",
      "iter: 55, RMSE on training set: 1.0012082466061598.\n",
      "iter: 56, RMSE on training set: 1.000691909573311.\n",
      "iter: 57, RMSE on training set: 1.0002213810140919.\n",
      "iter: 58, RMSE on training set: 0.9997927858232796.\n",
      "iter: 59, RMSE on training set: 0.9994025545462063.\n",
      "iter: 60, RMSE on training set: 0.9990474018565393.\n",
      "iter: 61, RMSE on training set: 0.9987243062571697.\n",
      "iter: 62, RMSE on training set: 0.9984304909650383.\n",
      "iter: 63, RMSE on training set: 0.9981634059392862.\n",
      "iter: 64, RMSE on training set: 0.9979207110108967.\n",
      "iter: 65, RMSE on training set: 0.9977002600708218.\n",
      "iter: 66, RMSE on training set: 0.9975000862727939.\n",
      "iter: 67, RMSE on training set: 0.9973183882064153.\n",
      "iter: 68, RMSE on training set: 0.997153516995811.\n",
      "iter: 69, RMSE on training set: 0.9970039642791223.\n",
      "iter: 70, RMSE on training set: 0.9968683510242837.\n",
      "iter: 71, RMSE on training set: 0.9967454171370183.\n",
      "iter: 72, RMSE on training set: 0.9966340118175466.\n",
      "iter: 73, RMSE on training set: 0.9965330846234047.\n",
      "iter: 74, RMSE on training set: 0.9964416771966422.\n",
      "RMSE on test data: 0.9964329380928149.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 1.7161553199022836.\n",
      "iter: 1, RMSE on training set: 1.6478830496307406.\n",
      "iter: 2, RMSE on training set: 1.5869549830257481.\n",
      "iter: 3, RMSE on training set: 1.5325465401120755.\n",
      "iter: 4, RMSE on training set: 1.4839120011059341.\n",
      "iter: 5, RMSE on training set: 1.4403812921802261.\n",
      "iter: 6, RMSE on training set: 1.4013557156997305.\n",
      "iter: 7, RMSE on training set: 1.3663032542714906.\n",
      "iter: 8, RMSE on training set: 1.3347535421580987.\n",
      "iter: 9, RMSE on training set: 1.3062926373090427.\n",
      "iter: 10, RMSE on training set: 1.2805577307788125.\n",
      "iter: 11, RMSE on training set: 1.2572319222936001.\n",
      "iter: 12, RMSE on training set: 1.2360391736554153.\n",
      "iter: 13, RMSE on training set: 1.2167395289745746.\n",
      "iter: 14, RMSE on training set: 1.1991246659795536.\n",
      "iter: 15, RMSE on training set: 1.183013818808102.\n",
      "iter: 16, RMSE on training set: 1.168250091676736.\n",
      "iter: 17, RMSE on training set: 1.1546971656262375.\n",
      "iter: 18, RMSE on training set: 1.1422363873733172.\n",
      "iter: 19, RMSE on training set: 1.1307642199110595.\n",
      "iter: 20, RMSE on training set: 1.1201900283860717.\n",
      "iter: 21, RMSE on training set: 1.110434171335779.\n",
      "iter: 22, RMSE on training set: 1.1014263659941002.\n",
      "iter: 23, RMSE on training set: 1.0931042965250943.\n",
      "iter: 24, RMSE on training set: 1.0854124352618721.\n",
      "iter: 25, RMSE on training set: 1.0783010489406102.\n",
      "iter: 26, RMSE on training set: 1.071725364239376.\n",
      "iter: 27, RMSE on training set: 1.0656448694441174.\n",
      "iter: 28, RMSE on training set: 1.0600227316156443.\n",
      "iter: 29, RMSE on training set: 1.0548253111155905.\n",
      "iter: 30, RMSE on training set: 1.0500217576965911.\n",
      "iter: 31, RMSE on training set: 1.0455836745298361.\n",
      "iter: 32, RMSE on training set: 1.0414848385096187.\n",
      "iter: 33, RMSE on training set: 1.0377009669317663.\n",
      "iter: 34, RMSE on training set: 1.034209522194406.\n",
      "iter: 35, RMSE on training set: 1.0309895475258326.\n",
      "iter: 36, RMSE on training set: 1.0280215279204898.\n",
      "iter: 37, RMSE on training set: 1.0252872714770802.\n",
      "iter: 38, RMSE on training set: 1.022769807199309.\n",
      "iter: 39, RMSE on training set: 1.020453296055216.\n",
      "iter: 40, RMSE on training set: 1.0183229527096396.\n",
      "iter: 41, RMSE on training set: 1.0163649758592472.\n",
      "iter: 42, RMSE on training set: 1.0145664855236505.\n",
      "iter: 43, RMSE on training set: 1.0129154659920159.\n",
      "iter: 44, RMSE on training set: 1.0114007134044976.\n",
      "iter: 45, RMSE on training set: 1.0100117871730383.\n",
      "iter: 46, RMSE on training set: 1.008738964626292.\n",
      "iter: 47, RMSE on training set: 1.0075731984068912.\n",
      "iter: 48, RMSE on training set: 1.0065060762625073.\n",
      "iter: 49, RMSE on training set: 1.005529782960579.\n",
      "iter: 50, RMSE on training set: 1.0046370641245996.\n",
      "iter: 51, RMSE on training set: 1.0038211918413371.\n",
      "iter: 52, RMSE on training set: 1.003075931926472.\n",
      "iter: 53, RMSE on training set: 1.0023955127636943.\n",
      "iter: 54, RMSE on training set: 1.0017745956516542.\n",
      "iter: 55, RMSE on training set: 1.0012082466061598.\n",
      "iter: 56, RMSE on training set: 1.000691909573311.\n",
      "iter: 57, RMSE on training set: 1.0002213810140919.\n",
      "iter: 58, RMSE on training set: 0.9997927858232796.\n",
      "iter: 59, RMSE on training set: 0.9994025545462063.\n",
      "iter: 60, RMSE on training set: 0.9990474018565393.\n",
      "iter: 61, RMSE on training set: 0.9987243062571697.\n",
      "iter: 62, RMSE on training set: 0.9984304909650383.\n",
      "iter: 63, RMSE on training set: 0.9981634059392862.\n",
      "iter: 64, RMSE on training set: 0.9979207110108967.\n",
      "iter: 65, RMSE on training set: 0.9977002600708218.\n",
      "iter: 66, RMSE on training set: 0.9975000862727939.\n",
      "iter: 67, RMSE on training set: 0.9973183882064153.\n",
      "iter: 68, RMSE on training set: 0.997153516995811.\n",
      "iter: 69, RMSE on training set: 0.9970039642791223.\n",
      "iter: 70, RMSE on training set: 0.9968683510242837.\n",
      "iter: 71, RMSE on training set: 0.9967454171370183.\n",
      "iter: 72, RMSE on training set: 0.9966340118175466.\n",
      "iter: 73, RMSE on training set: 0.9965330846234047.\n",
      "iter: 74, RMSE on training set: 0.9964416771966422.\n",
      "RMSE on test data: 0.9964367669141992.\n",
      "Running num_features=5\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 2.1600882279215265.\n",
      "iter: 1, RMSE on training set: 2.059972462675044.\n",
      "iter: 2, RMSE on training set: 1.9719897481036475.\n",
      "iter: 3, RMSE on training set: 1.8944930037247985.\n",
      "iter: 4, RMSE on training set: 1.8260966239840757.\n",
      "iter: 5, RMSE on training set: 1.7655837031342905.\n",
      "iter: 6, RMSE on training set: 1.71189055807281.\n",
      "iter: 7, RMSE on training set: 1.66409157131686.\n",
      "iter: 8, RMSE on training set: 1.6213845098371853.\n",
      "iter: 9, RMSE on training set: 1.5830765314016504.\n",
      "iter: 10, RMSE on training set: 1.5485710579033944.\n",
      "iter: 11, RMSE on training set: 1.5173556510021333.\n",
      "iter: 12, RMSE on training set: 1.4889909740537692.\n",
      "iter: 13, RMSE on training set: 1.4631008736918283.\n",
      "iter: 14, RMSE on training set: 1.439363570368191.\n",
      "iter: 15, RMSE on training set: 1.4175039125826607.\n",
      "iter: 16, RMSE on training set: 1.3972866250850684.\n",
      "iter: 17, RMSE on training set: 1.3785104662297323.\n",
      "iter: 18, RMSE on training set: 1.3610032023642777.\n",
      "iter: 19, RMSE on training set: 1.344617305861144.\n",
      "iter: 20, RMSE on training set: 1.3292262864325348.\n",
      "iter: 21, RMSE on training set: 1.3147215712201363.\n",
      "iter: 22, RMSE on training set: 1.301009856623912.\n",
      "iter: 23, RMSE on training set: 1.2880108630294984.\n",
      "iter: 24, RMSE on training set: 1.2756554318741005.\n",
      "iter: 25, RMSE on training set: 1.263883912436946.\n",
      "iter: 26, RMSE on training set: 1.2526447931013196.\n",
      "iter: 27, RMSE on training set: 1.2418935384824916.\n",
      "iter: 28, RMSE on training set: 1.2315915997053146.\n",
      "iter: 29, RMSE on training set: 1.221705570256457.\n",
      "iter: 30, RMSE on training set: 1.2122064642715265.\n",
      "iter: 31, RMSE on training set: 1.2030690979062502.\n",
      "iter: 32, RMSE on training set: 1.1942715576515341.\n",
      "iter: 33, RMSE on training set: 1.1857947421543829.\n",
      "iter: 34, RMSE on training set: 1.1776219663681848.\n",
      "iter: 35, RMSE on training set: 1.1697386187400836.\n",
      "iter: 36, RMSE on training set: 1.1621318637070874.\n",
      "iter: 37, RMSE on training set: 1.154790383066395.\n",
      "iter: 38, RMSE on training set: 1.1477041508526598.\n",
      "iter: 39, RMSE on training set: 1.1408642372326088.\n",
      "iter: 40, RMSE on training set: 1.1342626376474603.\n",
      "iter: 41, RMSE on training set: 1.1278921240230908.\n",
      "iter: 42, RMSE on training set: 1.121746115350207.\n",
      "iter: 43, RMSE on training set: 1.1158185653315116.\n",
      "iter: 44, RMSE on training set: 1.1101038651166897.\n",
      "iter: 45, RMSE on training set: 1.1045967594124693.\n",
      "iter: 46, RMSE on training set: 1.0992922744750384.\n",
      "iter: 47, RMSE on training set: 1.0941856566747021.\n",
      "iter: 48, RMSE on training set: 1.0892723204747956.\n",
      "iter: 49, RMSE on training set: 1.084547804794428.\n",
      "iter: 50, RMSE on training set: 1.0800077368322987.\n",
      "iter: 51, RMSE on training set: 1.0756478025205318.\n",
      "iter: 52, RMSE on training set: 1.0714637228563533.\n",
      "iter: 53, RMSE on training set: 1.0674512354281886.\n",
      "iter: 54, RMSE on training set: 1.0636060805131289.\n",
      "iter: 55, RMSE on training set: 1.059923991176586.\n",
      "iter: 56, RMSE on training set: 1.0564006868531879.\n",
      "iter: 57, RMSE on training set: 1.053031869931749.\n",
      "iter: 58, RMSE on training set: 1.049813224907183.\n",
      "iter: 59, RMSE on training set: 1.0467404196991335.\n",
      "iter: 60, RMSE on training set: 1.0438091087716361.\n",
      "iter: 61, RMSE on training set: 1.0410149377206077.\n",
      "iter: 62, RMSE on training set: 1.038353549026906.\n",
      "iter: 63, RMSE on training set: 1.0358205887022476.\n",
      "iter: 64, RMSE on training set: 1.0334117135835805.\n",
      "iter: 65, RMSE on training set: 1.0311225990586024.\n",
      "iter: 66, RMSE on training set: 1.0289489470308477.\n",
      "iter: 67, RMSE on training set: 1.0268864939572147.\n",
      "iter: 68, RMSE on training set: 1.0249310188137062.\n",
      "iter: 69, RMSE on training set: 1.0230783508666283.\n",
      "iter: 70, RMSE on training set: 1.0213243771464138.\n",
      "iter: 71, RMSE on training set: 1.0196650495395803.\n",
      "iter: 72, RMSE on training set: 1.018096391431176.\n",
      "iter: 73, RMSE on training set: 1.0166145038454182.\n",
      "iter: 74, RMSE on training set: 1.0152155710459874.\n",
      "iter: 75, RMSE on training set: 1.0138958655698995.\n",
      "iter: 76, RMSE on training set: 1.012651752679777.\n",
      "iter: 77, RMSE on training set: 1.0114796942290287.\n",
      "iter: 78, RMSE on training set: 1.0103762519427464.\n",
      "iter: 79, RMSE on training set: 1.009338090124304.\n",
      "iter: 80, RMSE on training set: 1.0083619778036403.\n",
      "iter: 81, RMSE on training set: 1.0074447903481816.\n",
      "iter: 82, RMSE on training set: 1.0065835105613663.\n",
      "iter: 83, RMSE on training set: 1.0057752292968773.\n",
      "iter: 84, RMSE on training set: 1.0050171456190262.\n",
      "iter: 85, RMSE on training set: 1.004306566541367.\n",
      "iter: 86, RMSE on training set: 1.0036409063766183.\n",
      "iter: 87, RMSE on training set: 1.0030176857314357.\n",
      "iter: 88, RMSE on training set: 1.002434530179532.\n",
      "iter: 89, RMSE on training set: 1.0018891686461986.\n",
      "iter: 90, RMSE on training set: 1.0013794315364972.\n",
      "iter: 91, RMSE on training set: 1.0009032486383256.\n",
      "iter: 92, RMSE on training set: 1.0004586468302032.\n",
      "iter: 93, RMSE on training set: 1.0000437476222155.\n",
      "iter: 94, RMSE on training set: 0.9996567645568576.\n",
      "iter: 95, RMSE on training set: 0.9992960004948773.\n",
      "iter: 96, RMSE on training set: 0.9989598448094383.\n",
      "iter: 97, RMSE on training set: 0.9986467705101392.\n",
      "iter: 98, RMSE on training set: 0.9983553313167207.\n",
      "iter: 99, RMSE on training set: 0.9980841587005227.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-286ac0b41682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running num_features={n}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n\u001b[0;32m---> 22\u001b[0;31m                                                              num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m## Calculate mean and standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(ratings, K, method, num_items_per_user, num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item, gamma)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             [train_rmse, validation_rmse, user_feature, item_features] = CCD(training, validation, \n\u001b[0;32m---> 75\u001b[0;31m                                                                 num_features, lambda_user, lambda_item)\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Incorrect method, 0-SGD, 1-ALS, 2-CCD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/recommender.py\u001b[0m in \u001b[0;36mCCD\u001b[0;34m(train, test, num_features, lambda_user, lambda_item)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_item_CCD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnz_users_per_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_item_userindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mtrain_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_error_residual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iter: {}, RMSE on training set: {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0merror_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/recommender.py\u001b[0m in \u001b[0;36mcompute_error\u001b[0;34m(data, user_features, item_features, nz)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# ***************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mreal_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcalculate_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/recommender.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m\"\"\"compute the loss (MSE) of the prediction of nonzero elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# ***************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mreal_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muser_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcalculate_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# Scalar fast path first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;31m# Use isinstance checks for common index types; this is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5         ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features_arr = [1, 3, 5, 7, 10, 13, 15]   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.7\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(num_features_arr))\n",
    "train_rmse_std = np.zeros(len(num_features_arr))\n",
    "validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "for i, num_features in enumerate(num_features_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running num_features={n}'.format(n=num_features))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    ## Calculate mean and standard deviation    \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(num_features_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(num_features_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Number of features (K)'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99645032  0.99644168  0.          0.          0.          0.          0.        ]\n",
      "[  1.11022302e-16   1.11022302e-16   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      "[ 0.99645154  0.99644258  0.          0.          0.          0.          0.        ]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_rmse_mean)\n",
    "print(train_rmse_std)\n",
    "print(validation_rmse_mean)\n",
    "print(validation_rmse_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD 1-ALS\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user_arr = [0.01, 0.1, 1, 10]\n",
    "lambda_item = 0.7\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "for i, lambda_user in enumerate(lambda_user_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD 1-ALS\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item_arr = [0.01, 0.1, 0.5, 1]\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "\n",
    "for i, lambda_item in enumerate(lambda_item_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_item={n}'.format(n=lambda_item))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_item_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_item_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma_arr = [0.01, 0.1, 1]\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.5\n",
    "\n",
    "train_rmse_mean = np.zeros(len(gamma_arr))\n",
    "train_rmse_std = np.zeros(len(gamma_arr))\n",
    "validation_rmse_mean = np.zeros(len(gamma_arr))\n",
    "validation_rmse_std = np.zeros(len(gamma_arr))\n",
    "\n",
    "for i, gamma in enumerate(gamma_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running gamma={n}'.format(n=gamma))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(gamma_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(gamma_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(gamma_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(gamma_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "#### 1. Compare SGD, ALS with the best set of parameters (based on above results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
