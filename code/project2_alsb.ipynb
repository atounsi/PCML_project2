{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from plots import *\n",
    "from split_data import *\n",
    "from recommender import *\n",
    "from cross_validation import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\u001c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "path_dataset = \"../data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def split_data(ratings, p_test=0.1, seed=45):\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # generate random indices\n",
    "    row = ratings.shape[0]\n",
    "    col = ratings.shape[1]\n",
    "    num = row*col\n",
    "    \n",
    "    indices = np.random.permutation(num)\n",
    "    index_split = int(np.floor(p_test * num))\n",
    "    \n",
    "    index_tr_n = indices[: index_split]\n",
    "    index_te_n = indices[index_split:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    rat_arr = ratings.toarray()\n",
    "    \n",
    "    # reshaping\n",
    "    valid_array_te = np.copy(rat_arr).reshape((num,1))\n",
    "    valid_array_tr = np.copy(rat_arr).reshape((num,1))  \n",
    "    \n",
    "    # create split\n",
    "    train = valid_array_tr\n",
    "    train[index_tr_n] = 0\n",
    "    train = train.reshape((row,col))\n",
    "    \n",
    "    test = valid_array_te\n",
    "    test[index_te_n] = 0\n",
    "    test = test.reshape((row,col))\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    #print (valid_ratings, train, test)\n",
    "    return rat_arr, train, test\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "rats, train, test = split_data(ratings, p_test=0.1, seed=46)\n",
    "print(\"done\")#2:10 much quicker than that :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def user_mean(data):\n",
    "    nnz_u = np.copy(data)\n",
    "    nnz_u[np.where( data > 0 )] = 1\n",
    "    return train.sum(axis=0) / nnz_u.sum(axis=0)\n",
    "\n",
    "def item_mean(data):\n",
    "    \n",
    "    nnz_i = np.copy(data)\n",
    "    nnz_i[np.where( data >1 )] = 1\n",
    "    return train.sum(axis=1) / nnz_i.sum(axis=1)\n",
    "    \n",
    "print(\"functions 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def init_MF_ALS_biased(train, num_features, factor_features=0.1, factor_biases=1):\n",
    "    num_items, num_users = train.shape\n",
    "    user_features = factor_features*np.ones((num_users,num_features))\n",
    "    item_features = factor_features*np.ones((num_items,num_features))\n",
    "    user_biases = factor_biases*np.ones(num_users)\n",
    "    item_biases = factor_biases*np.ones(num_items)\n",
    "    return user_features, item_features, user_biases, item_biases\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def init_MF_ALS(train, num_features, factor_features=0.1):\n",
    "    num_items, num_users = train.shape\n",
    "    user_features = factor_features*np.ones((num_users,num_features))\n",
    "    item_features = factor_features*np.ones((num_items,num_features))\n",
    "    return user_features, item_features\n",
    "print(\"function 'compiled'\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def data_user_biased(data, user_biases):\n",
    "    data_user_biased = data - user_biases\n",
    "    return data_user_biased\n",
    "def data_item_biased(data, item_biases):\n",
    "    data_item_biased = (data.T - item_biases).T\n",
    "    return  data_item_biased\n",
    "print(\"functions 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def compute_error_prediction(data, prediction, nz):\n",
    "    real_label = np.array([data[d,n] for (d,n) in nz])\n",
    "    prediction_label = np.array([prediction[d,n] for (d,n) in nz])\n",
    "    rmse = np.sqrt((1/len(nz))*calculate_mse(real_label,prediction_label))\n",
    "    return rmse\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def prediction_biased(item_features, item_biases, user_features, user_biases):    \n",
    "    prediction_data =    user_features.dot(item_features.T).T\n",
    "    prediction = ((prediction_data + user_biases).T + item_biases).T       \n",
    "    return prediction\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def update_item_biased_feature(train, user_features, user_biases, lambda_item, nnz_users_per_item, nz_item_userindices):\n",
    "    \n",
    "    num_users, num_features = user_features.shape\n",
    "    num_items = train.shape[0]\n",
    "    ones_biases = np.array([np.ones(num_users)])\n",
    "    item_biases = np.zeros(num_items)\n",
    "    item_features = np.zeros((num_items,num_features))         \n",
    "        \n",
    "    for item in np.arange(num_items): \n",
    "        nnz_users = nnz_users_per_item[item]\n",
    "        nz_userindices = nz_item_userindices[item]\n",
    "        nz_userfeatures = user_features[nz_userindices,:]\n",
    "        nz_onesbiases = ones_biases[:,nz_userindices]\n",
    "        nz_userbiases = user_biases[nz_userindices]\n",
    "    \n",
    "    \n",
    "        Xt = np.concatenate((nz_onesbiases, nz_userfeatures.T), axis=0)\n",
    "        A = Xt.dot(Xt.T) + lambda_item*nnz_users*np.eye(num_features+1)  \n",
    "        train_item = (train[item,nz_userindices])\n",
    "        b = Xt.dot(data_user_biased(train_item, nz_userbiases).T) \n",
    "\n",
    "        Yt = np.linalg.solve(A,b)\n",
    "        \n",
    "        item_features[item,:] = Yt[1:num_features+1]\n",
    "        item_biases[item] = Yt[0]\n",
    "\n",
    "    return item_features, item_biases\n",
    "\n",
    "def update_user_biased_feature(train, item_features, item_biases, lambda_user, nnz_items_per_user, nz_user_itemindices):\n",
    "    \n",
    "    num_users = train.shape[1]\n",
    "    num_items, num_features = item_features.shape\n",
    "    ones_biases = np.array([np.ones(num_items)])\n",
    "    user_biases = np.zeros(num_users)\n",
    "    user_features = np.zeros((num_users,num_features))\n",
    "    \n",
    "    for user in np.arange(num_users):        \n",
    "        nnz_items = nnz_items_per_user[user]\n",
    "        nz_itemindices = nz_user_itemindices[user]\n",
    "        nz_itemfeatures = item_features[nz_itemindices,:]\n",
    "        nz_onesbiases = ones_biases[:,nz_itemindices]\n",
    "        nz_itembiases = item_biases[nz_itemindices]\n",
    "        \n",
    "    \n",
    "        Yt = np.concatenate((nz_onesbiases, nz_itemfeatures.T), axis=0)\n",
    "        A = Yt.dot(Yt.T) + lambda_user*nnz_items*np.eye(num_features+1)  \n",
    "        train_user = train[nz_itemindices,user]\n",
    "        b = Yt.dot(data_item_biased(train_user, nz_itembiases)) \n",
    "        Xt = np.linalg.solve(A,b)\n",
    "        \n",
    "        user_features[user,:] = Xt[1:num_features+1]\n",
    "        user_biases[user] = Xt[0]\n",
    "\n",
    "    return user_features, user_biases\n",
    "\n",
    "print(\"functions 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9992039618771512.\n",
      "iter: 1, RMSE on training set: 0.9876753791248908.\n",
      "iter: 2, RMSE on training set: 0.9864005907408453.\n",
      "iter: 3, RMSE on training set: 0.9867920781431252.\n",
      "Best iter: 2, with RMSE on test data: 1.0004500614216185. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0004500614216185"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ALS_biased(train, test, num_features = 10, lambda_user = 0.1, lambda_item = 0.1, seed=552):\n",
    "    \n",
    "    stop_criterion = 1e-7\n",
    "\n",
    "    error_list = [0, 0]\n",
    "    max_it = 10 \n",
    "    \n",
    "    error_old = 10\n",
    "    error_new = 5\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features, user_biases, item_biases = init_MF_ALS_biased(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))    \n",
    "    \n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    nz_train, nz_row_colindices, nz_col_rowindices = build_index_groups(train)\n",
    "    _,nz_user_itemindices = map(list,zip(*nz_col_rowindices))\n",
    "    nnz_items_per_user = [len(i) for i in nz_user_itemindices]\n",
    "    _,nz_item_userindices = map(list,zip(*nz_row_colindices))\n",
    "    nnz_users_per_item = [len(i) for i in nz_item_userindices]\n",
    "\n",
    "    print(\"learn the matrix factorization using ALS...\")\n",
    "\n",
    "    for it in np.arange(max_it):\n",
    "        \n",
    "        \n",
    "        item_features, item_biases = update_item_biased_feature(train, user_features, user_biases, lambda_item, nnz_users_per_item, nz_item_userindices)\n",
    "        user_features, user_biases = update_user_biased_feature(train, item_features, item_biases, lambda_user, nnz_items_per_user, nz_user_itemindices)\n",
    "        \n",
    "        prediction = prediction_biased(item_features, item_biases, user_features, user_biases)        \n",
    "        rmse = compute_error_prediction(train, prediction, nz_train)        \n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        error_new = compute_error_prediction(test, prediction, nz_test)\n",
    "        \n",
    "        error_list.append(rmse)\n",
    "        if abs(error_list[-1]-error_list[-2])<stop_criterion:\n",
    "            break\n",
    "        if error_new>error_old:\n",
    "            print(\"Best iter: {}, with RMSE on test data: {}. \".format(it-1,error_old))\n",
    "            break\n",
    "        error_old = error_new\n",
    "\n",
    "    prediction = prediction_biased(item_features, item_biases, user_features, user_biases)\n",
    "    rmse = compute_error_prediction(test, prediction, nz_test)\n",
    "    #print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    #print(\"done\")\n",
    "    return error_old\n",
    "    \n",
    "    # ***************************************************\n",
    "\n",
    "ALS_biased(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def prediction_non_biased(item_features, user_features):    \n",
    "    prediction = user_features.dot(item_features.T).T    \n",
    "    return prediction\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def update_user_feature(\n",
    "        train, item_features, lambda_user,\n",
    "        nnz_items_per_user, nz_user_itemindices):\n",
    "    \"\"\"update user feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    num_items,num_users = train.shape\n",
    "    num_features = item_features.shape[1]\n",
    "    user_feature = np.zeros((num_users,num_features))\n",
    "    for user in np.arange(num_users):\n",
    "        nnz_items = nnz_items_per_user[user]\n",
    "        nz_itemindices = nz_user_itemindices[user]\n",
    "        nz_itemfeatures = item_features[nz_itemindices,:]\n",
    "        A = ((nz_itemfeatures.T).dot(nz_itemfeatures)+lambda_user*nnz_items*np.eye(num_features))\n",
    "        train_user = train[nz_itemindices,user]\n",
    "        b = ((nz_itemfeatures.T).dot(train_user))\n",
    "        user_feature[user,:] = np.linalg.solve(A,b)\n",
    "    # ***************************************************\n",
    "    return user_feature\n",
    "\n",
    "def update_item_feature(\n",
    "        train, user_features, lambda_item,\n",
    "        nnz_users_per_item, nz_item_userindices):\n",
    "    \"\"\"update item feature matrix.\"\"\"\n",
    "    # ***************************************************\n",
    "    num_items,num_users = train.shape\n",
    "    num_features = user_features.shape[1]\n",
    "    item_feature = np.zeros((num_items,num_features))\n",
    "    for item in np.arange(num_items):\n",
    "        nnz_users = nnz_users_per_item[item]\n",
    "        nz_userindices = nz_item_userindices[item]\n",
    "        nz_userfeatures = user_features[nz_userindices,:]\n",
    "        A = ((nz_userfeatures.T).dot(nz_userfeatures)+lambda_item*nnz_users*np.eye(num_features))\n",
    "        train_item = (train[item,nz_userindices])\n",
    "        b = ((nz_userfeatures.T).dot(train_item))\n",
    "        item_feature[item,:] = np.linalg.solve(A,b)\n",
    "    # ***************************************************\n",
    "    return item_feature\n",
    "print(\"functions 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9959192861108764.\n",
      "iter: 1, RMSE on training set: 0.9912939750943721.\n",
      "iter: 2, RMSE on training set: 0.9908659304261874.\n",
      "iter: 3, RMSE on training set: 0.990810472982908.\n",
      "iter: 4, RMSE on training set: 0.9908690709872654.\n",
      "Best iter: 3, with RMSE on test data: 1.002509533585753. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.002509533585753"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ALS(train, test, num_features = 10, lambda_user = 0.1, lambda_item = 0.1, seed=552):\n",
    "    \n",
    "    stop_criterion = 1e-7\n",
    "\n",
    "    error_list = [0, 0]\n",
    "    max_it = 10 \n",
    "    \n",
    "    error_old = 10\n",
    "    error_new = 5\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features = init_MF_ALS(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))    \n",
    "    \n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    \n",
    "    nz_train, nz_row_colindices, nz_col_rowindices = build_index_groups(train)\n",
    "    _,nz_user_itemindices = map(list,zip(*nz_col_rowindices))\n",
    "    nnz_items_per_user = [len(i) for i in nz_user_itemindices]\n",
    "    _,nz_item_userindices = map(list,zip(*nz_row_colindices))\n",
    "    nnz_users_per_item = [len(i) for i in nz_item_userindices]\n",
    "\n",
    "    print(\"learn the matrix factorization using ALS...\")\n",
    "\n",
    "    for it in np.arange(max_it):\n",
    "        \n",
    "        item_features = update_item_feature(train, user_features, lambda_item, nnz_users_per_item, nz_item_userindices)\n",
    "\n",
    "        user_features = update_user_feature(train, item_features, lambda_user, nnz_items_per_user, nz_user_itemindices)\n",
    "        \n",
    "        prediction = prediction_non_biased(item_features, user_features)\n",
    "        \n",
    "        rmse = compute_error_prediction(train, prediction, nz_train)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        error_new = compute_error_prediction(test, prediction, nz_test)\n",
    "        error_list.append(rmse)\n",
    "        if abs(error_list[-1]-error_list[-2])<stop_criterion:\n",
    "            break\n",
    "        if error_new>error_old:\n",
    "            print(\"Best iter: {}, with RMSE on test data: {}. \".format(it-1,error_old))\n",
    "            break\n",
    "        error_old = error_new\n",
    "        \n",
    "    prediction = prediction_non_biased(item_features, user_features)\n",
    "    rmse = compute_error_prediction(test, prediction, nz_test)\n",
    "    #print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    #print(\"done\")\n",
    "    return error_old\n",
    "    \n",
    "    # ***************************************************\n",
    "\n",
    "ALS(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "#need k be multiple of row*col\n",
    "def k_indices_set_generator(ratings, k=5, seed=48):\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # generate random indices\n",
    "    row = ratings.shape[0]\n",
    "    col = ratings.shape[1]\n",
    "    num = row*col\n",
    "    \n",
    "    indices = np.random.permutation(num)\n",
    "    \n",
    "    index_split = int(num/k)\n",
    "    k_indices_set = np.zeros((k,index_split),dtype=np.int)   \n",
    "    \n",
    "    for i in range(k):    \n",
    "        id_start = index_split*i\n",
    "        id_end = index_split*(i+1)\n",
    "        k_indices_set[i] = indices[id_start:id_end]\n",
    "    \n",
    "    return k_indices_set\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def split_data_k(ratings, k_indices_set, k):\n",
    "    \n",
    "    K, index_split = k_indices_set.shape\n",
    "    index_tr = k_indices_set[np.where(np.arange(K) != (k-1))].reshape(((K-1)*index_split,1))[:,0]\n",
    "    index_te = k_indices_set[k-1]\n",
    "    \n",
    "    row = ratings.shape[0]\n",
    "    col = ratings.shape[1]\n",
    "    num = row*col\n",
    "    \n",
    "    rat_arr = ratings.toarray()\n",
    "    \n",
    "    # reshaping\n",
    "    valid_array_te = np.copy(rat_arr).reshape((num,1))\n",
    "    valid_array_tr = np.copy(rat_arr).reshape((num,1))  \n",
    "    \n",
    "    # create split\n",
    "    train = valid_array_tr\n",
    "    train[index_te] = 0\n",
    "    train = train.reshape((row,col))\n",
    "    \n",
    "    test = valid_array_te\n",
    "    test[index_tr] = 0\n",
    "    test = test.reshape((row,col))\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    #print (valid_ratings, train, test)\n",
    "    return train, test\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_minimalist(ratings, K, num_features=5, lambda_user=0.01, lambda_item=0.01):\n",
    "    \n",
    "    k_indices_set = k_indices_set_generator(ratings,10)    \n",
    "\n",
    "    train_rmse_arr=[]\n",
    "    validation_rmse_arr=[]\n",
    "        \n",
    "    for k in range(K):\n",
    "        print('Running {}th fold in {} folds'.format(k+1, K))\n",
    "        train_cross,test_cross = split_data_k(ratings, k_indices_set, k+1)\n",
    "        #train_rmse, validation_rmse = ALS_Biased(training, validation, num_features, lambda_user, lambda_item)\n",
    "        validation_rmse = ALS_biased(train_cross, test_cross, num_features, lambda_user, lambda_item)    \n",
    "        #train_rmse_arr.append(train_rmse)\n",
    "        validation_rmse_arr.append(validation_rmse)\n",
    "        \n",
    "    return validation_rmse_arr\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "def best_lambda_user(lambda_user_arr = [0.001, 0.01, 0.1, 1]):\n",
    "    \n",
    "    K = 4        ## K-fold cross validation\n",
    "    num_features = 8   # K in the lecture notes\n",
    "    \n",
    "    lambda_item = 0.05\n",
    "\n",
    "    train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "    train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "    validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "    validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "    for i, lambda_user in enumerate(lambda_user_arr):\n",
    "        train_rmse_arr = []\n",
    "        validation_rmse_arr = []\n",
    "\n",
    "        print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "        validation_rmse_arr = cross_validation_minimalist(ratings, K, num_features, lambda_user, lambda_item)\n",
    "\n",
    "        #train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "        #train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "        validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "        validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "\n",
    "    ## Plotting results\n",
    "    #plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "    #                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                         validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "    #plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "    plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "    plt.legend(('Train', 'Validation'))\n",
    "    plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "    plt.show()\n",
    "    return list(zip(lambda_user_arr, validation_rmse_mean)) \n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lambda_user=0.001\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9929769056420532.\n",
      "iter: 1, RMSE on training set: 0.9800142814275047.\n",
      "iter: 2, RMSE on training set: 0.9778031585836828.\n",
      "iter: 3, RMSE on training set: 0.9760414437807262.\n",
      "iter: 4, RMSE on training set: 0.9747376355936678.\n",
      "iter: 5, RMSE on training set: 0.9727227409024222.\n",
      "iter: 6, RMSE on training set: 0.914966508465358.\n",
      "Best iter: 5, with RMSE on test data: 0.9948228573310891. \n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9928747129890845.\n",
      "iter: 1, RMSE on training set: 0.9797753201128424.\n",
      "iter: 2, RMSE on training set: 0.977563332740611.\n",
      "iter: 3, RMSE on training set: 0.9758970875471963.\n",
      "iter: 4, RMSE on training set: 0.9746938801399431.\n",
      "iter: 5, RMSE on training set: 0.973649757729912.\n",
      "iter: 6, RMSE on training set: 0.9147112132295677.\n",
      "Best iter: 5, with RMSE on test data: 0.9954410604502942. \n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.992975314616929.\n",
      "iter: 1, RMSE on training set: 0.9799309386929324.\n",
      "iter: 2, RMSE on training set: 0.9776458458770702.\n",
      "iter: 3, RMSE on training set: 0.9758951943999847.\n",
      "iter: 4, RMSE on training set: 0.9746304707820557.\n",
      "iter: 5, RMSE on training set: 0.9735321405854357.\n",
      "iter: 6, RMSE on training set: 0.916440708186796.\n",
      "Best iter: 5, with RMSE on test data: 0.9953350939487903. \n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9931165173100802.\n",
      "iter: 1, RMSE on training set: 0.9800480087361507.\n",
      "iter: 2, RMSE on training set: 0.9778076147756585.\n",
      "iter: 3, RMSE on training set: 0.9760793361077812.\n",
      "iter: 4, RMSE on training set: 0.9748087172934248.\n",
      "iter: 5, RMSE on training set: 0.9731934433852072.\n",
      "iter: 6, RMSE on training set: 0.9129928009940433.\n",
      "Best iter: 5, with RMSE on test data: 0.9934641352933978. \n",
      "Running lambda_user=0.01\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9943210166711585.\n",
      "iter: 1, RMSE on training set: 0.9797990965536134.\n",
      "iter: 2, RMSE on training set: 0.9761176643541272.\n",
      "iter: 3, RMSE on training set: 0.9748253812995347.\n",
      "iter: 4, RMSE on training set: 0.974393396374742.\n",
      "iter: 5, RMSE on training set: 0.9742561509004279.\n",
      "iter: 6, RMSE on training set: 0.9742264489089772.\n",
      "iter: 7, RMSE on training set: 0.974241732184965.\n",
      "iter: 8, RMSE on training set: 0.974172882244886.\n",
      "iter: 9, RMSE on training set: 0.9585003434789755.\n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.994200489486206.\n",
      "iter: 1, RMSE on training set: 0.979547362149942.\n",
      "iter: 2, RMSE on training set: 0.9759653543982625.\n",
      "iter: 3, RMSE on training set: 0.9747539454018767.\n",
      "iter: 4, RMSE on training set: 0.9743555057321669.\n",
      "iter: 5, RMSE on training set: 0.9742350175049083.\n",
      "iter: 6, RMSE on training set: 0.9742153701060043.\n",
      "iter: 7, RMSE on training set: 0.9742373722069073.\n",
      "iter: 8, RMSE on training set: 0.9742508626745296.\n",
      "iter: 9, RMSE on training set: 0.9586548440314241.\n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9942982683131368.\n",
      "iter: 1, RMSE on training set: 0.9796855079451856.\n",
      "iter: 2, RMSE on training set: 0.9759685105206689.\n",
      "iter: 3, RMSE on training set: 0.9747336196015226.\n",
      "iter: 4, RMSE on training set: 0.9743334725393248.\n",
      "iter: 5, RMSE on training set: 0.9742090154013942.\n",
      "iter: 6, RMSE on training set: 0.9741846780459739.\n",
      "iter: 7, RMSE on training set: 0.9742026860959139.\n",
      "iter: 8, RMSE on training set: 0.9741688659196696.\n",
      "iter: 9, RMSE on training set: 0.9582279710676633.\n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9944438126597123.\n",
      "iter: 1, RMSE on training set: 0.979772178325982.\n",
      "iter: 2, RMSE on training set: 0.976147956465776.\n",
      "iter: 3, RMSE on training set: 0.9749321001772043.\n",
      "iter: 4, RMSE on training set: 0.9745262627844763.\n",
      "iter: 5, RMSE on training set: 0.9743938766993181.\n",
      "iter: 6, RMSE on training set: 0.9743640269733691.\n",
      "iter: 7, RMSE on training set: 0.9743784685650245.\n",
      "iter: 8, RMSE on training set: 0.9742856667110423.\n",
      "iter: 9, RMSE on training set: 0.957872355559635.\n",
      "Running lambda_user=0.1\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9989467018137866.\n",
      "iter: 1, RMSE on training set: 0.9835851361046138.\n",
      "iter: 2, RMSE on training set: 0.9820175488196572.\n",
      "iter: 3, RMSE on training set: 0.9820380227730152.\n",
      "iter: 4, RMSE on training set: 0.9822170197148622.\n",
      "iter: 5, RMSE on training set: 0.9824082654251184.\n",
      "Best iter: 4, with RMSE on test data: 0.9970262239147554. \n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9988144287563311.\n",
      "iter: 1, RMSE on training set: 0.983516960601652.\n",
      "iter: 2, RMSE on training set: 0.9819415175291673.\n",
      "iter: 3, RMSE on training set: 0.9819575839677647.\n",
      "iter: 4, RMSE on training set: 0.9821357984155756.\n",
      "iter: 5, RMSE on training set: 0.9823272986618417.\n",
      "Best iter: 4, with RMSE on test data: 0.9977691492061559. \n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9989130022447287.\n",
      "iter: 1, RMSE on training set: 0.9835739656401014.\n",
      "iter: 2, RMSE on training set: 0.9820040412331787.\n",
      "iter: 3, RMSE on training set: 0.9820217828387008.\n",
      "iter: 4, RMSE on training set: 0.9822008729603607.\n",
      "iter: 5, RMSE on training set: 0.9823928364096466.\n",
      "Best iter: 4, with RMSE on test data: 0.9971864547502399. \n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9990600184495712.\n",
      "iter: 1, RMSE on training set: 0.9837418676893939.\n",
      "iter: 2, RMSE on training set: 0.9821606697763626.\n",
      "iter: 3, RMSE on training set: 0.9821774329675211.\n",
      "iter: 4, RMSE on training set: 0.982357338755093.\n",
      "iter: 5, RMSE on training set: 0.9825501295423122.\n",
      "Best iter: 4, with RMSE on test data: 0.9961807609735388. \n",
      "Running lambda_user=1\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.139963915667709.\n",
      "iter: 1, RMSE on training set: 1.0391046649867712.\n",
      "iter: 2, RMSE on training set: 1.0203232408315512.\n",
      "iter: 3, RMSE on training set: 1.0144236437622187.\n",
      "iter: 4, RMSE on training set: 1.011479753476739.\n",
      "iter: 5, RMSE on training set: 1.0096985420107942.\n",
      "iter: 6, RMSE on training set: 1.0085462204272888.\n",
      "iter: 7, RMSE on training set: 1.0077878192632699.\n",
      "iter: 8, RMSE on training set: 1.007288867242128.\n",
      "iter: 9, RMSE on training set: 1.006962227504795.\n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1398210706703822.\n",
      "iter: 1, RMSE on training set: 1.039031980361866.\n",
      "iter: 2, RMSE on training set: 1.020246757222622.\n",
      "iter: 3, RMSE on training set: 1.0143442350310983.\n",
      "iter: 4, RMSE on training set: 1.0113989713344758.\n",
      "iter: 5, RMSE on training set: 1.0096169408900433.\n",
      "iter: 6, RMSE on training set: 1.0084641187927834.\n",
      "iter: 7, RMSE on training set: 1.0077054470491191.\n",
      "iter: 8, RMSE on training set: 1.0072063843001244.\n",
      "iter: 9, RMSE on training set: 1.0068797338508622.\n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.139919782913605.\n",
      "iter: 1, RMSE on training set: 1.039085362480641.\n",
      "iter: 2, RMSE on training set: 1.020305481973404.\n",
      "iter: 3, RMSE on training set: 1.0144038466031142.\n",
      "iter: 4, RMSE on training set: 1.0114587274615827.\n",
      "iter: 5, RMSE on training set: 1.0096766846992196.\n",
      "iter: 6, RMSE on training set: 1.0085237195804353.\n",
      "iter: 7, RMSE on training set: 1.0077648062186293.\n",
      "iter: 8, RMSE on training set: 1.0072654507450456.\n",
      "iter: 9, RMSE on training set: 1.0069384981151148.\n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1401003814193655.\n",
      "iter: 1, RMSE on training set: 1.039255302506086.\n",
      "iter: 2, RMSE on training set: 1.0204720371466207.\n",
      "iter: 3, RMSE on training set: 1.0145701350203122.\n",
      "iter: 4, RMSE on training set: 1.0116259073843241.\n",
      "iter: 5, RMSE on training set: 1.0098448065996783.\n",
      "iter: 6, RMSE on training set: 1.0086925955821735.\n",
      "iter: 7, RMSE on training set: 1.0079342228252628.\n",
      "iter: 8, RMSE on training set: 1.0074352249191394.\n",
      "iter: 9, RMSE on training set: 1.0071084908340298.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEPCAYAAABlZDIgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntfS3yhMFIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1de88ff1c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_lambda_u = best_lambda_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, 0.99476578675589289),\n",
       " (0.01, 0.98815360195381285),\n",
       " (0.1, 0.99704064721117247),\n",
       " (1, 1.0200033806812665)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambda_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "def best_lambda_item(lambda_item_arr = [0.001, 0.01, 0.1, 1]):\n",
    "    \n",
    "    K = 4        ## K-fold cross validation\n",
    "    num_features = 8   # K in the lecture notes\n",
    "    \n",
    "    lambda_user = 0.05\n",
    "\n",
    "    train_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "    train_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "    validation_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "    validation_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "\n",
    "    for i, lambda_item in enumerate(lambda_item_arr):\n",
    "        train_rmse_arr = []\n",
    "        validation_rmse_arr = []\n",
    "\n",
    "        print('Running lambda_item={n}'.format(n=lambda_item))\n",
    "        validation_rmse_arr = cross_validation_minimalist(ratings, K, num_features, lambda_user, lambda_item)\n",
    "\n",
    "        #train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "        #train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "        validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "        validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "\n",
    "    ## Plotting results\n",
    "    #plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "    #                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                         validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "    #plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "    plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "    plt.legend(('Train', 'Validation'))\n",
    "    plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "    plt.show()\n",
    "    return list(zip(lambda_item_arr, validation_rmse_mean)) \n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lambda_item=0.001\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9960873357185817.\n",
      "iter: 1, RMSE on training set: 0.976612366897327.\n",
      "iter: 2, RMSE on training set: 0.9755615526330195.\n",
      "iter: 3, RMSE on training set: 0.9752286440620399.\n",
      "iter: 4, RMSE on training set: 0.9750605669518985.\n",
      "iter: 5, RMSE on training set: 0.9748829115354556.\n",
      "iter: 6, RMSE on training set: 0.9200355362361152.\n",
      "Best iter: 5, with RMSE on test data: 0.9973768914304485. \n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9959573686539831.\n",
      "iter: 1, RMSE on training set: 0.9765698268606118.\n",
      "iter: 2, RMSE on training set: 0.975497949543537.\n",
      "iter: 3, RMSE on training set: 0.9751557762863479.\n",
      "iter: 4, RMSE on training set: 0.9749829979994818.\n",
      "iter: 5, RMSE on training set: 0.9746601479117892.\n",
      "iter: 6, RMSE on training set: 0.9201063005385897.\n",
      "Best iter: 5, with RMSE on test data: 0.9977742418776714. \n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9960584523448576.\n",
      "iter: 1, RMSE on training set: 0.9766011307531289.\n",
      "iter: 2, RMSE on training set: 0.9755384534010002.\n",
      "iter: 3, RMSE on training set: 0.975200781396551.\n",
      "iter: 4, RMSE on training set: 0.9750281693242869.\n",
      "iter: 5, RMSE on training set: 0.9740730935544926.\n",
      "iter: 6, RMSE on training set: 0.9214248978900439.\n",
      "Best iter: 5, with RMSE on test data: 0.996918936446616. \n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9961986855778507.\n",
      "iter: 1, RMSE on training set: 0.9767516344698101.\n",
      "iter: 2, RMSE on training set: 0.9756681743779949.\n",
      "iter: 3, RMSE on training set: 0.9753287552246782.\n",
      "iter: 4, RMSE on training set: 0.975162840762494.\n",
      "iter: 5, RMSE on training set: 0.9749148629501521.\n",
      "iter: 6, RMSE on training set: 0.9193802929541348.\n",
      "Best iter: 5, with RMSE on test data: 0.9967373411913261. \n",
      "Running lambda_item=0.01\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9960730007963843.\n",
      "iter: 1, RMSE on training set: 0.976979124425962.\n",
      "iter: 2, RMSE on training set: 0.97594816345237.\n",
      "iter: 3, RMSE on training set: 0.9757143525039755.\n",
      "iter: 4, RMSE on training set: 0.9756209733032651.\n",
      "iter: 5, RMSE on training set: 0.9755719079959561.\n",
      "iter: 6, RMSE on training set: 0.97554501355207.\n",
      "iter: 7, RMSE on training set: 0.9755315788262213.\n",
      "iter: 8, RMSE on training set: 0.975360362924979.\n",
      "iter: 9, RMSE on training set: 0.9590164578593355.\n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9959429652292933.\n",
      "iter: 1, RMSE on training set: 0.9769299733292667.\n",
      "iter: 2, RMSE on training set: 0.9758816680840917.\n",
      "iter: 3, RMSE on training set: 0.9756406982335099.\n",
      "iter: 4, RMSE on training set: 0.9755441443257101.\n",
      "iter: 5, RMSE on training set: 0.9754937009052109.\n",
      "iter: 6, RMSE on training set: 0.975466355644449.\n",
      "iter: 7, RMSE on training set: 0.9754529963949405.\n",
      "iter: 8, RMSE on training set: 0.9753631439314058.\n",
      "iter: 9, RMSE on training set: 0.9596686737657821.\n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9960435101408676.\n",
      "iter: 1, RMSE on training set: 0.9769717472540761.\n",
      "iter: 2, RMSE on training set: 0.9759217886649005.\n",
      "iter: 3, RMSE on training set: 0.9756827576104875.\n",
      "iter: 4, RMSE on training set: 0.9755860527606596.\n",
      "iter: 5, RMSE on training set: 0.9755341965100806.\n",
      "iter: 6, RMSE on training set: 0.9755050385434151.\n",
      "iter: 7, RMSE on training set: 0.975489886759309.\n",
      "iter: 8, RMSE on training set: 0.9754425013437233.\n",
      "iter: 9, RMSE on training set: 0.9601174611416609.\n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9961848497050056.\n",
      "iter: 1, RMSE on training set: 0.9771160792772804.\n",
      "iter: 2, RMSE on training set: 0.976057658384175.\n",
      "iter: 3, RMSE on training set: 0.9758194678929908.\n",
      "iter: 4, RMSE on training set: 0.9757272493326669.\n",
      "iter: 5, RMSE on training set: 0.975680126013335.\n",
      "iter: 6, RMSE on training set: 0.9756551443679414.\n",
      "iter: 7, RMSE on training set: 0.9756434154626147.\n",
      "iter: 8, RMSE on training set: 0.9754524908914091.\n",
      "iter: 9, RMSE on training set: 0.9589651145763648.\n",
      "Running lambda_item=0.1\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9961177218312554.\n",
      "iter: 1, RMSE on training set: 0.9849389839282124.\n",
      "iter: 2, RMSE on training set: 0.9828036560704623.\n",
      "iter: 3, RMSE on training set: 0.9831438399391411.\n",
      "Best iter: 2, with RMSE on test data: 0.9966690729523255. \n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9959872573017315.\n",
      "iter: 1, RMSE on training set: 0.9848421741101737.\n",
      "iter: 2, RMSE on training set: 0.9827079010572657.\n",
      "iter: 3, RMSE on training set: 0.9830412581754522.\n",
      "Best iter: 2, with RMSE on test data: 0.9975653928249989. \n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9960829226346348.\n",
      "iter: 1, RMSE on training set: 0.9849079992056999.\n",
      "iter: 2, RMSE on training set: 0.9827815450937145.\n",
      "iter: 3, RMSE on training set: 0.9831180624283714.\n",
      "Best iter: 2, with RMSE on test data: 0.9968981429754232. \n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9962347064848559.\n",
      "iter: 1, RMSE on training set: 0.9850674988934104.\n",
      "iter: 2, RMSE on training set: 0.9829382047398993.\n",
      "iter: 3, RMSE on training set: 0.9832756748967797.\n",
      "iter: 4, RMSE on training set: 0.9837757749735807.\n",
      "Best iter: 3, with RMSE on test data: 0.9957795300717036. \n",
      "Running lambda_item=1\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.0051246341465705.\n",
      "iter: 1, RMSE on training set: 0.9978181748608232.\n",
      "iter: 2, RMSE on training set: 0.9990993068107717.\n",
      "Best iter: 1, with RMSE on test data: 1.006489771279814. \n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.0050044521910244.\n",
      "iter: 1, RMSE on training set: 0.9977214974387671.\n",
      "iter: 2, RMSE on training set: 0.9990064204268528.\n",
      "Best iter: 1, with RMSE on test data: 1.0071178812512975. \n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.0050853589111453.\n",
      "iter: 1, RMSE on training set: 0.997793138869334.\n",
      "iter: 2, RMSE on training set: 0.9990783362967482.\n",
      "Best iter: 1, with RMSE on test data: 1.0067258018196554. \n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.005290433362351.\n",
      "iter: 1, RMSE on training set: 0.9979534811429073.\n",
      "iter: 2, RMSE on training set: 0.9992353504364562.\n",
      "Best iter: 1, with RMSE on test data: 1.005678761658273. \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEPCAYAAABlZDIgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noIiISFT8f2usIjJYr9KUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1de88bf3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_lambda_i = best_lambda_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, 0.99720185273651563),\n",
       " (0.01, 0.98931868962408664),\n",
       " (0.1, 0.99672803470611282),\n",
       " (1, 1.0065030540022599)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambda_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "def best_num_features(num_features_arr = [1, 4, 7, 10,13]):\n",
    "    \n",
    "    K = 4        ## K-fold cross validation\n",
    "    \n",
    "    lambda_item = 0.01\n",
    "    lambda_item = 0.01\n",
    "\n",
    "    #train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "    #train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "    validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "    validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "    for i, num_features in enumerate(num_features_arr):\n",
    "        #train_rmse_arr = []\n",
    "        validation_rmse_arr = []\n",
    "\n",
    "        print('Running num_features={n}'.format(n=num_features))\n",
    "        validation_rmse_arr = cross_validation_minimalist(ratings, K, num_features, lambda_user, lambda_item)\n",
    "\n",
    "        #train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "        #train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "        validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "        validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "\n",
    "    ## Plotting results\n",
    "    #plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "    #                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                         validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "    #plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "    plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "    plt.legend(('Train', 'Validation'))\n",
    "    plt.xlabel('num_features'); plt.ylabel('RMSE');\n",
    "    plt.show()\n",
    "    return list(zip(num_features_arr, validation_rmse_mean)) \n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running num_features=1\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1361536689012746.\n",
      "iter: 1, RMSE on training set: 1.0077795772000133.\n",
      "iter: 2, RMSE on training set: 0.9901997506708349.\n",
      "iter: 3, RMSE on training set: 0.9868550758734739.\n",
      "iter: 4, RMSE on training set: 0.9853504841674874.\n",
      "iter: 5, RMSE on training set: 0.9844845258313454.\n",
      "iter: 6, RMSE on training set: 0.9839283053799376.\n",
      "iter: 7, RMSE on training set: 0.9835499297048161.\n",
      "iter: 8, RMSE on training set: 0.9832846598463315.\n",
      "iter: 9, RMSE on training set: 0.9830956763324021.\n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1360132050597316.\n",
      "iter: 1, RMSE on training set: 1.0077411118316377.\n",
      "iter: 2, RMSE on training set: 0.990139534944165.\n",
      "iter: 3, RMSE on training set: 0.9867932167007988.\n",
      "iter: 4, RMSE on training set: 0.9852887654598792.\n",
      "iter: 5, RMSE on training set: 0.9844230351658583.\n",
      "iter: 6, RMSE on training set: 0.9838670469287284.\n",
      "iter: 7, RMSE on training set: 0.9834889262220262.\n",
      "iter: 8, RMSE on training set: 0.9832239324672659.\n",
      "iter: 9, RMSE on training set: 0.9830352309004702.\n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1361113575434632.\n",
      "iter: 1, RMSE on training set: 1.0077869217967808.\n",
      "iter: 2, RMSE on training set: 0.9901927068669049.\n",
      "iter: 3, RMSE on training set: 0.9868473956415085.\n",
      "iter: 4, RMSE on training set: 0.9853425826185781.\n",
      "iter: 5, RMSE on training set: 0.984476381622064.\n",
      "iter: 6, RMSE on training set: 0.9839199692677925.\n",
      "iter: 7, RMSE on training set: 0.9835414708603951.\n",
      "iter: 8, RMSE on training set: 0.9832761382495283.\n",
      "iter: 9, RMSE on training set: 0.9830871347377353.\n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1362964761566272.\n",
      "iter: 1, RMSE on training set: 1.007944619809114.\n",
      "iter: 2, RMSE on training set: 0.9903478454345613.\n",
      "iter: 3, RMSE on training set: 0.9870036795420289.\n",
      "iter: 4, RMSE on training set: 0.9854999489408733.\n",
      "iter: 5, RMSE on training set: 0.9846344599151429.\n",
      "iter: 6, RMSE on training set: 0.9840784896715304.\n",
      "iter: 7, RMSE on training set: 0.9837002498077727.\n",
      "iter: 8, RMSE on training set: 0.9834350513633373.\n",
      "iter: 9, RMSE on training set: 0.9832461006889683.\n",
      "Running num_features=4\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1269852472811825.\n",
      "iter: 1, RMSE on training set: 1.0054860114295496.\n",
      "iter: 2, RMSE on training set: 0.9925945023354023.\n",
      "iter: 3, RMSE on training set: 0.9891030969008039.\n",
      "iter: 4, RMSE on training set: 0.9871467875117835.\n",
      "iter: 5, RMSE on training set: 0.9858674648763498.\n",
      "iter: 6, RMSE on training set: 0.9849733602686096.\n",
      "iter: 7, RMSE on training set: 0.9843297200575625.\n",
      "iter: 8, RMSE on training set: 0.9838611410354975.\n",
      "iter: 9, RMSE on training set: 0.98351908028381.\n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1268418616043603.\n",
      "iter: 1, RMSE on training set: 1.0054463059805874.\n",
      "iter: 2, RMSE on training set: 0.9925375766914117.\n",
      "iter: 3, RMSE on training set: 0.9890428330452937.\n",
      "iter: 4, RMSE on training set: 0.9870853612207049.\n",
      "iter: 5, RMSE on training set: 0.9858054785751573.\n",
      "iter: 6, RMSE on training set: 0.9849111770993222.\n",
      "iter: 7, RMSE on training set: 0.9842676039363447.\n",
      "iter: 8, RMSE on training set: 0.9837992687327001.\n",
      "iter: 9, RMSE on training set: 0.9834575513126137.\n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1269427744450615.\n",
      "iter: 1, RMSE on training set: 1.0054963174923395.\n",
      "iter: 2, RMSE on training set: 0.9925910477035979.\n",
      "iter: 3, RMSE on training set: 0.9890971347502558.\n",
      "iter: 4, RMSE on training set: 0.9871396206872831.\n",
      "iter: 5, RMSE on training set: 0.985859546091674.\n",
      "iter: 6, RMSE on training set: 0.9849649756774894.\n",
      "iter: 7, RMSE on training set: 0.9843210704880013.\n",
      "iter: 8, RMSE on training set: 0.9838523657402763.\n",
      "iter: 9, RMSE on training set: 0.9835102703052121.\n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1271264393357396.\n",
      "iter: 1, RMSE on training set: 1.0056599752006967.\n",
      "iter: 2, RMSE on training set: 0.9927444027865773.\n",
      "iter: 3, RMSE on training set: 0.9892519830444588.\n",
      "iter: 4, RMSE on training set: 0.9872960138056839.\n",
      "iter: 5, RMSE on training set: 0.9860170730535811.\n",
      "iter: 6, RMSE on training set: 0.9851232770392077.\n",
      "iter: 7, RMSE on training set: 0.9844798630793944.\n",
      "iter: 8, RMSE on training set: 0.9840114370608977.\n",
      "iter: 9, RMSE on training set: 0.9836694703383043.\n",
      "Running num_features=7\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.121232856554019.\n",
      "iter: 1, RMSE on training set: 1.0074811819274534.\n",
      "iter: 2, RMSE on training set: 0.995015905045128.\n",
      "iter: 3, RMSE on training set: 0.9908721403874234.\n",
      "iter: 4, RMSE on training set: 0.988464326388825.\n",
      "iter: 5, RMSE on training set: 0.9868582783427488.\n",
      "iter: 6, RMSE on training set: 0.9857182164223918.\n",
      "iter: 7, RMSE on training set: 0.9848868176142909.\n",
      "iter: 8, RMSE on training set: 0.9842749892625583.\n",
      "iter: 9, RMSE on training set: 0.983824467454371.\n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1210876364732425.\n",
      "iter: 1, RMSE on training set: 1.0074409015597294.\n",
      "iter: 2, RMSE on training set: 0.9949597080449868.\n",
      "iter: 3, RMSE on training set: 0.9908121495214383.\n",
      "iter: 4, RMSE on training set: 0.9884027110676252.\n",
      "iter: 5, RMSE on training set: 0.9867957836163755.\n",
      "iter: 6, RMSE on training set: 0.9856553362678682.\n",
      "iter: 7, RMSE on training set: 0.9848239191501477.\n",
      "iter: 8, RMSE on training set: 0.984212325455134.\n",
      "iter: 9, RMSE on training set: 0.9837621895253353.\n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1211903012980058.\n",
      "iter: 1, RMSE on training set: 1.0074924805173788.\n",
      "iter: 2, RMSE on training set: 0.9950136264035759.\n",
      "iter: 3, RMSE on training set: 0.9908665055950067.\n",
      "iter: 4, RMSE on training set: 0.9884571182039085.\n",
      "iter: 5, RMSE on training set: 0.9868501560776729.\n",
      "iter: 6, RMSE on training set: 0.9857095560625603.\n",
      "iter: 7, RMSE on training set: 0.9848778670054399.\n",
      "iter: 8, RMSE on training set: 0.9842659159895994.\n",
      "iter: 9, RMSE on training set: 0.9838153802151173.\n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1213721421944867.\n",
      "iter: 1, RMSE on training set: 1.0076562498453894.\n",
      "iter: 2, RMSE on training set: 0.9951666340861719.\n",
      "iter: 3, RMSE on training set: 0.9910208989196788.\n",
      "iter: 4, RMSE on training set: 0.988613151950034.\n",
      "iter: 5, RMSE on training set: 0.9870074753025524.\n",
      "iter: 6, RMSE on training set: 0.9858677943619636.\n",
      "iter: 7, RMSE on training set: 0.985036710223515.\n",
      "iter: 8, RMSE on training set: 0.9844251148073331.\n",
      "iter: 9, RMSE on training set: 0.9839747507662651.\n",
      "Running num_features=10\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1176385325959686.\n",
      "iter: 1, RMSE on training set: 1.0096227922805647.\n",
      "iter: 2, RMSE on training set: 0.9970257697501684.\n",
      "iter: 3, RMSE on training set: 0.9923264047178764.\n",
      "iter: 4, RMSE on training set: 0.9895480962175512.\n",
      "iter: 5, RMSE on training set: 0.9876764454277113.\n",
      "iter: 6, RMSE on training set: 0.9863367181904462.\n",
      "iter: 7, RMSE on training set: 0.9853523136721347.\n",
      "iter: 8, RMSE on training set: 0.9846229163276093.\n",
      "iter: 9, RMSE on training set: 0.9840825958934502.\n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1174920727514641.\n",
      "iter: 1, RMSE on training set: 1.0095828852537794.\n",
      "iter: 2, RMSE on training set: 0.9969700536742052.\n",
      "iter: 3, RMSE on training set: 0.992266561266695.\n",
      "iter: 4, RMSE on training set: 0.9894862992701502.\n",
      "iter: 5, RMSE on training set: 0.987613536178204.\n",
      "iter: 6, RMSE on training set: 0.98627328137406.\n",
      "iter: 7, RMSE on training set: 0.985288790257954.\n",
      "iter: 8, RMSE on training set: 0.9845596160757035.\n",
      "iter: 9, RMSE on training set: 0.9840197105670929.\n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1175958969134296.\n",
      "iter: 1, RMSE on training set: 1.0096354672284629.\n",
      "iter: 2, RMSE on training set: 0.9970241123748682.\n",
      "iter: 3, RMSE on training set: 0.9923208438540102.\n",
      "iter: 4, RMSE on training set: 0.9895407465575893.\n",
      "iter: 5, RMSE on training set: 0.987668094682794.\n",
      "iter: 6, RMSE on training set: 0.9863277948854974.\n",
      "iter: 7, RMSE on training set: 0.985343092510514.\n",
      "iter: 8, RMSE on training set: 0.9846135816291723.\n",
      "iter: 9, RMSE on training set: 0.9840732674944754.\n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1177760437280961.\n",
      "iter: 1, RMSE on training set: 1.0097986733285498.\n",
      "iter: 2, RMSE on training set: 0.9971768334631367.\n",
      "iter: 3, RMSE on training set: 0.9924748719211566.\n",
      "iter: 4, RMSE on training set: 0.9896964868195866.\n",
      "iter: 5, RMSE on training set: 0.9878252333814735.\n",
      "iter: 6, RMSE on training set: 0.9864859627032105.\n",
      "iter: 7, RMSE on training set: 0.9855019550446741.\n",
      "iter: 8, RMSE on training set: 0.9847728638326457.\n",
      "iter: 9, RMSE on training set: 0.9842327604059653.\n",
      "Running num_features=13\n",
      "Running 1th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1154904875874234.\n",
      "iter: 1, RMSE on training set: 1.0116500877671988.\n",
      "iter: 2, RMSE on training set: 0.998772399669736.\n",
      "iter: 3, RMSE on training set: 0.9936003465057721.\n",
      "iter: 4, RMSE on training set: 0.990503107696727.\n",
      "iter: 5, RMSE on training set: 0.9884009914968255.\n",
      "iter: 6, RMSE on training set: 0.9868871673970995.\n",
      "iter: 7, RMSE on training set: 0.9857686960058021.\n",
      "iter: 8, RMSE on training set: 0.9849356743795105.\n",
      "iter: 9, RMSE on training set: 0.9843156735687625.\n",
      "Running 2th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1153431422183224.\n",
      "iter: 1, RMSE on training set: 1.011610830601412.\n",
      "iter: 2, RMSE on training set: 0.9987171226534943.\n",
      "iter: 3, RMSE on training set: 0.9935406425304065.\n",
      "iter: 4, RMSE on training set: 0.9904411683272033.\n",
      "iter: 5, RMSE on training set: 0.9883377397365468.\n",
      "iter: 6, RMSE on training set: 0.9868232635341972.\n",
      "iter: 7, RMSE on training set: 0.9857046423547696.\n",
      "iter: 8, RMSE on training set: 0.9848718285528628.\n",
      "iter: 9, RMSE on training set: 0.9842522625088592.\n",
      "Running 3th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1154477480195384.\n",
      "iter: 1, RMSE on training set: 1.011664066614074.\n",
      "iter: 2, RMSE on training set: 0.9987711647822138.\n",
      "iter: 3, RMSE on training set: 0.9935947994267932.\n",
      "iter: 4, RMSE on training set: 0.9904956159009902.\n",
      "iter: 5, RMSE on training set: 0.9883924369479875.\n",
      "iter: 6, RMSE on training set: 0.9868780161676096.\n",
      "iter: 7, RMSE on training set: 0.9857592418154494.\n",
      "iter: 8, RMSE on training set: 0.9849261138653697.\n",
      "iter: 9, RMSE on training set: 0.9843061353912367.\n",
      "Running 4th fold in 4 folds\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 1.1156264687572237.\n",
      "iter: 1, RMSE on training set: 1.0118266666520928.\n",
      "iter: 2, RMSE on training set: 0.9989235786619306.\n",
      "iter: 3, RMSE on training set: 0.9937484834640365.\n",
      "iter: 4, RMSE on training set: 0.9906510840136373.\n",
      "iter: 5, RMSE on training set: 0.988549402178451.\n",
      "iter: 6, RMSE on training set: 0.9870361050619627.\n",
      "iter: 7, RMSE on training set: 0.9859181036921769.\n",
      "iter: 8, RMSE on training set: 0.9850854531263873.\n",
      "iter: 9, RMSE on training set: 0.9844657224742519.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEWCAYAAAC0Q+rDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEhglGRERCcz/BxitRjZThEMVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1dece995048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_num_f = best_num_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.0005820872169715),\n",
       " (4, 1.0009716084838933),\n",
       " (7, 1.0012537121532528),\n",
       " (10, 1.0014926039174503),\n",
       " (13, 1.0017085792314464)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_num_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function'compiled'\n"
     ]
    }
   ],
   "source": [
    "def ALS_biased_prediction(train, test, num_features = 1, lambda_user = 0.01, lambda_item = 0.01, seed=552):\n",
    "    \n",
    "    stop_criterion = 1e-7\n",
    "\n",
    "    error_list = [0, 0]\n",
    "    max_it = 50\n",
    "    \n",
    "    error_old = 10\n",
    "    error_new = 5\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # init ALS\n",
    "    user_features, item_features, user_biases, item_biases = init_MF_ALS_biased(train, num_features)\n",
    "    \n",
    "    # ***************************************************\n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))    \n",
    "    \n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    nz_train, nz_row_colindices, nz_col_rowindices = build_index_groups(train)\n",
    "    _,nz_user_itemindices = map(list,zip(*nz_col_rowindices))\n",
    "    nnz_items_per_user = [len(i) for i in nz_user_itemindices]\n",
    "    _,nz_item_userindices = map(list,zip(*nz_row_colindices))\n",
    "    nnz_users_per_item = [len(i) for i in nz_item_userindices]\n",
    "\n",
    "    print(\"learn the matrix factorization using ALS...\")\n",
    "\n",
    "    for it in np.arange(max_it):\n",
    "        \n",
    "        \n",
    "        item_features, item_biases = update_item_biased_feature(train, user_features, user_biases, lambda_item, nnz_users_per_item, nz_item_userindices)\n",
    "        user_features, user_biases = update_user_biased_feature(train, item_features, item_biases, lambda_user, nnz_items_per_user, nz_user_itemindices)\n",
    "        \n",
    "        prediction = prediction_biased(item_features, item_biases, user_features, user_biases)        \n",
    "        rmse = compute_error_prediction(train, prediction, nz_train)        \n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        error_new = compute_error_prediction(test, prediction, nz_test)\n",
    "        \n",
    "        error_list.append(rmse)\n",
    "        if abs(error_list[-1]-error_list[-2])<stop_criterion:\n",
    "            break\n",
    "        if error_new>error_old:\n",
    "            print(\"Best iter: {}, with RMSE on test data: {}. \".format(it-1,error_old))\n",
    "            break\n",
    "        error_old = error_new\n",
    "\n",
    "    prediction = prediction_biased(item_features, item_biases, user_features, user_biases)\n",
    "    rmse = compute_error_prediction(test, prediction, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    #print(\"done\")\n",
    "    return prediction\n",
    "    \n",
    "    # ***************************************************\n",
    "\n",
    "print(\"function'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9943491028330717.\n",
      "iter: 1, RMSE on training set: 0.9777961864210896.\n",
      "iter: 2, RMSE on training set: 0.9747267256694103.\n",
      "iter: 3, RMSE on training set: 0.9734723885790351.\n",
      "iter: 4, RMSE on training set: 0.972975284893822.\n",
      "iter: 5, RMSE on training set: 0.9727492139381406.\n",
      "iter: 6, RMSE on training set: 0.9725910626493434.\n",
      "iter: 7, RMSE on training set: 0.9595437575809322.\n",
      "iter: 8, RMSE on training set: 0.957986669902135.\n",
      "iter: 9, RMSE on training set: 0.9577428772370213.\n",
      "iter: 10, RMSE on training set: 0.9576585901375495.\n",
      "iter: 11, RMSE on training set: 0.957629999725439.\n",
      "iter: 12, RMSE on training set: 0.9576226858930941.\n",
      "iter: 13, RMSE on training set: 0.9576237537261081.\n",
      "iter: 14, RMSE on training set: 0.9576280506531828.\n",
      "iter: 15, RMSE on training set: 0.9576334402750372.\n",
      "iter: 16, RMSE on training set: 0.9576390285926912.\n",
      "iter: 17, RMSE on training set: 0.9576444546699232.\n",
      "iter: 18, RMSE on training set: 0.9576495916832299.\n",
      "iter: 19, RMSE on training set: 0.9576544153794788.\n",
      "iter: 20, RMSE on training set: 0.9576589443661384.\n",
      "iter: 21, RMSE on training set: 0.9576632126209959.\n",
      "iter: 22, RMSE on training set: 0.9576672569786427.\n",
      "iter: 23, RMSE on training set: 0.9576711117433335.\n",
      "iter: 24, RMSE on training set: 0.9576748067122904.\n",
      "iter: 25, RMSE on training set: 0.9576783667972623.\n",
      "iter: 26, RMSE on training set: 0.9576818123455545.\n",
      "iter: 27, RMSE on training set: 0.9576851597156777.\n",
      "iter: 28, RMSE on training set: 0.957688421894318.\n",
      "iter: 29, RMSE on training set: 0.9576916090610238.\n",
      "iter: 30, RMSE on training set: 0.9576947290685541.\n",
      "iter: 31, RMSE on training set: 0.9576977878372352.\n",
      "iter: 32, RMSE on training set: 0.9577007896752437.\n",
      "iter: 33, RMSE on training set: 0.9577037375410596.\n",
      "iter: 34, RMSE on training set: 0.9577066332639238.\n",
      "iter: 35, RMSE on training set: 0.9577094777353041.\n",
      "iter: 36, RMSE on training set: 0.9577122710808341.\n",
      "iter: 37, RMSE on training set: 0.9577150128186337.\n",
      "iter: 38, RMSE on training set: 0.9577177020069173.\n",
      "iter: 39, RMSE on training set: 0.9577203373815502.\n",
      "iter: 40, RMSE on training set: 0.9577229174827591.\n",
      "iter: 41, RMSE on training set: 0.9577254407694801.\n",
      "iter: 42, RMSE on training set: 0.9577279057196744.\n",
      "iter: 43, RMSE on training set: 0.9577303109152774.\n",
      "iter: 44, RMSE on training set: 0.9577326551109527.\n",
      "iter: 45, RMSE on training set: 0.9577349372865307.\n",
      "iter: 46, RMSE on training set: 0.9577371566836681.\n",
      "iter: 47, RMSE on training set: 0.9577393128278754.\n",
      "iter: 48, RMSE on training set: 0.9577414055374591.\n",
      "iter: 49, RMSE on training set: 0.957743434921311.\n",
      "RMSE on test data: 0.9906057423109359.\n"
     ]
    }
   ],
   "source": [
    "prediction = ALS_biased_prediction( train, test, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy done\n"
     ]
    }
   ],
   "source": [
    "pred = np.copy(prediction)\n",
    "print(\"copy done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions'compiled'\n"
     ]
    }
   ],
   "source": [
    "#least square\n",
    "def least_squares(y, tx):\n",
    "    \"\"\"\n",
    "    Least squares using normal equations.\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(tx.T.dot(tx), tx.T.dot(y))\n",
    "    return w\n",
    "\n",
    "def error_mse(y, tx, w):\n",
    "    rmse = np.sqrt((1/len(y))*calculate_mse(y,tx.dot(w)))\n",
    "    return rmse\n",
    "def ridge_regression(y, tx,lambda_ = 0.1):\n",
    "    \"\"\"\n",
    "    Least squares using normal equations (with L2 regularization)\n",
    "    \"\"\"\n",
    "    \n",
    "    reg = 2 * y.size * lambda_ * np.identity(tx.shape[1]) # L2 regularization term\n",
    "    w = np.linalg.solve(tx.T.dot(tx) + reg, tx.T.dot(y))\n",
    "    return w\n",
    "print(\"functions'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function'compiled'\n"
     ]
    }
   ],
   "source": [
    "def feature_adding(train, test, pred):\n",
    "    \"\"\"\n",
    "    built y = real_labels  tx = (pred, #user ratings, #movie ratings, mean rate per user, mean rate per movie)\n",
    "    May be also add std deviation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    \n",
    "    \n",
    "    nnz_u = np.copy(train)\n",
    "    nnz_u[np.where(train > 0)] = 1\n",
    "    \n",
    "    nnz_i = np.copy(train)\n",
    "    nnz_i[np.where(train > 1)] = 1\n",
    "    \n",
    "    num_u = nnz_u.sum(axis=0)\n",
    "    num_i = nnz_i.sum(axis=1)  \n",
    "    mean_u = train.sum(axis=0)/num_u\n",
    "    mean_i = train.sum(axis=1)/num_i\n",
    "    \n",
    "    \n",
    "    std_u = np.std(train, axis=0)\n",
    "    std_i = np.std(train, axis=1)\n",
    "    \n",
    "    y = np.array([train[d,n] for (d,n) in nz_train])\n",
    "    y_test = np.array([test[d,n] for (d,n) in nz_test])\n",
    "    \n",
    "    \n",
    "    tX = np.array([[pred[d,n],num_u[n],num_i[d],mean_u[n],mean_i[d],std_u[n],std_i[d]] for (d,n) in nz_train])\n",
    "    tX_test = np.array([[pred[d,n],num_u[n],num_i[d],mean_u[n],mean_i[d],std_u[n],std_i[d]] for (d,n) in nz_test])\n",
    "   \n",
    "    return y, y_test, tX, tX_test\n",
    "print(\"function'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, y_test, tX, tX_test = feature_adding(train, test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959089393068\n",
      "0.988555750743\n"
     ]
    }
   ],
   "source": [
    "w = ridge_regression(y, tX, 0.01)\n",
    "print(error_mse(y, tX, w))\n",
    "print(error_mse(y_test, tX_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function'compiled'\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def feature_adding_all(train, test, pred):\n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    \n",
    "    \n",
    "    nnz_u = np.copy(train)\n",
    "    nnz_u[np.where(train > 0)] = 1\n",
    "    \n",
    "    nnz_i = np.copy(train)\n",
    "    nnz_i[np.where(train > 1)] = 1\n",
    "    \n",
    "    num_u = nnz_u.sum(axis=0)\n",
    "    num_i = nnz_i.sum(axis=1)  \n",
    "    mean_u = train.sum(axis=0)/num_u\n",
    "    mean_i = train.sum(axis=1)/num_i\n",
    "    \n",
    "    \n",
    "    std_u = np.std(train, axis=0)\n",
    "    std_i = np.std(train, axis=1)\n",
    "    \n",
    "    ind =  itertools.product(np.arange(train.shape[0]), np.arange(train.shape[1]))\n",
    "    \n",
    "    tX = np.array([[pred[d,n],num_u[n],num_i[d],mean_u[n],mean_i[d],std_u[n],std_i[d]] for (d,n) in ind])\n",
    "   \n",
    "    return tX\n",
    "print(\"function'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "tX_all = feature_adding_all(train, test, pred)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def pred_all(tX_all, w):\n",
    "    return tX_all.dot(w)\n",
    "\n",
    "w_nth = np.array([1,0,0,0,0,0,0])\n",
    "pred_all = pred_all(tX_all, w)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "pred_ready = pred_all.reshape((pred.shape[0], pred.shape[1]))\n",
    "print(pred_ready.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function 'compiled'\n"
     ]
    }
   ],
   "source": [
    "def submit_predictions(prediction, outputFilename, sampleSubmissionFilename):\n",
    "    import csv\n",
    "    \n",
    "    ## Read the indices \n",
    "    with open('../data/sampleSubmission.csv','r') as csvinput:\n",
    "        reader = csvinput.read().splitlines()\n",
    "        i=-1\n",
    "        ind = []\n",
    "        pred_rating = []\n",
    "        for row in reader:\n",
    "            if i != -1:\n",
    "                pos, default_rating = row.split(',')\n",
    "                row, col = pos.split(\"_\")\n",
    "                row = int(row.replace(\"r\", \"\"))\n",
    "                col = int(col.replace(\"c\", \"\"))       \n",
    "                pred_rating.append(prediction[row-1, col-1])\n",
    "                ind.append(pos)\n",
    "            i+=1\n",
    "    ## Create rows to be written\n",
    "    rows = zip(ind, pred_rating)\n",
    "\n",
    "    ## Write prediction with indices\n",
    "    import csv\n",
    "    with open(outputFilename, 'w', newline='\\n') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Id','Prediction'])\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data\n",
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##===Load test data====##\n",
    "print(\"Loading test data\")\n",
    "path_dataset = \"../data/sampleSubmission.csv\"\n",
    "submission_ratings = load_data(path_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate predictions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "##====Generate predictions for test data====##\n",
    "print(\"Generate predictions\")\n",
    "prediction = sp.lil_matrix(submission_ratings.get_shape())\n",
    "nz_row, nz_col = submission_ratings.nonzero()\n",
    "nz = list(zip(nz_row, nz_col))\n",
    "\n",
    "for i in range(len(nz_row)):\n",
    "    prediction[nz_row[i], nz_col[i]] = pred_ready[nz_row[i],nz_col[i]]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1176952 stored elements in LInked List format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##==== Create submission file=====##\n",
    "print(\"Creating submission file\")\n",
    "sampleSubmissionFilename = '../data/sampleSubmission.csv'\n",
    "outputFilename = 'submit_alsb_rr.csv'\n",
    "submit_predictions(prediction, outputFilename, sampleSubmissionFilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#OLD CODE USELESS (KIND OF BAK FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1th fold in 5 folds\n",
      "(array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64),)\n",
      "4085649.0\n",
      "454185.0\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.994436648379999.\n",
      "iter: 1, RMSE on training set: 0.9780539219597553.\n",
      "iter: 2, RMSE on training set: 0.9749630432082467.\n",
      "iter: 3, RMSE on training set: 0.9736811634570222.\n",
      "iter: 4, RMSE on training set: 0.9731903804996971.\n",
      "iter: 5, RMSE on training set: 0.9729702893672093.\n",
      "iter: 6, RMSE on training set: 0.9592527448533232.\n",
      "iter: 7, RMSE on training set: 0.934516671008093.\n",
      "Best iter: 6, with RMSE on test data: 0.9910630756394261. \n",
      "Running 2th fold in 5 folds\n",
      "(array([0, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64),)\n",
      "4086506.0\n",
      "453328.0\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9943163298750455.\n",
      "iter: 1, RMSE on training set: 0.9778340878540819.\n",
      "iter: 2, RMSE on training set: 0.974837111564989.\n",
      "iter: 3, RMSE on training set: 0.9736391743828038.\n",
      "iter: 4, RMSE on training set: 0.9731824462527062.\n",
      "iter: 5, RMSE on training set: 0.9729787033600806.\n",
      "iter: 6, RMSE on training set: 0.9594101083125279.\n",
      "iter: 7, RMSE on training set: 0.9325373778895896.\n",
      "Best iter: 6, with RMSE on test data: 0.9909003258237896. \n",
      "Running 3th fold in 5 folds\n",
      "(array([0, 1, 3, 4, 5, 6, 7, 8, 9], dtype=int64),)\n",
      "4085493.0\n",
      "454341.0\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9944139159380767.\n",
      "iter: 1, RMSE on training set: 0.9779193911010876.\n",
      "iter: 2, RMSE on training set: 0.9748168927261401.\n",
      "iter: 3, RMSE on training set: 0.973593996451031.\n",
      "iter: 4, RMSE on training set: 0.9731372224004994.\n",
      "iter: 5, RMSE on training set: 0.9729322097925156.\n",
      "iter: 6, RMSE on training set: 0.9594058132963751.\n",
      "iter: 7, RMSE on training set: 0.9335787884230332.\n",
      "Best iter: 6, with RMSE on test data: 0.9911508490441202. \n",
      "Running 4th fold in 5 folds\n",
      "(array([0, 1, 2, 4, 5, 6, 7, 8, 9], dtype=int64),)\n",
      "4085311.0\n",
      "454523.0\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.9945589093967091.\n",
      "iter: 1, RMSE on training set: 0.9780511704069855.\n",
      "iter: 2, RMSE on training set: 0.9750088044042223.\n",
      "iter: 3, RMSE on training set: 0.9738053586823118.\n",
      "iter: 4, RMSE on training set: 0.9733402426622576.\n",
      "iter: 5, RMSE on training set: 0.9731249262459737.\n",
      "iter: 6, RMSE on training set: 0.9595242261073145.\n",
      "iter: 7, RMSE on training set: 0.9325872290054923.\n",
      "Best iter: 6, with RMSE on test data: 0.9906908555334449. \n",
      "Running 5th fold in 5 folds\n",
      "(array([0, 1, 2, 3, 5, 6, 7, 8, 9], dtype=int64),)\n",
      "4085793.0\n",
      "454041.0\n",
      "learn the matrix factorization using ALS...\n",
      "iter: 0, RMSE on training set: 0.994222439690036.\n",
      "iter: 1, RMSE on training set: 0.9777613096559465.\n",
      "iter: 2, RMSE on training set: 0.9748346436603967.\n",
      "iter: 3, RMSE on training set: 0.9736445430904216.\n",
      "iter: 4, RMSE on training set: 0.9731642311068888.\n",
      "iter: 5, RMSE on training set: 0.9729380279474397.\n",
      "iter: 6, RMSE on training set: 0.9594794470396075.\n",
      "iter: 7, RMSE on training set: 0.9315951886046513.\n",
      "Best iter: 6, with RMSE on test data: 0.9923988648366828. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.99106307563942608,\n",
       " 0.99090032582378962,\n",
       " 0.99115084904412021,\n",
       " 0.99069085553344494,\n",
       " 0.99239886483668283]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_minimalist(ratings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_indices_set = k_indices_set_generator(ratings,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000000,)\n",
      "(1000000,)\n"
     ]
    }
   ],
   "source": [
    "train_cross,test_cross = split_data_k(ratings, k_indices_set, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n",
      "(10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_user_biased, train_item_biased = data_user_biased(train, user_biases),data_item_biased(train, item_biases)\n",
    "print(train_user_biased.shape)\n",
    "print(train_item_biased.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "u_mean = user_mean(train)\n",
    "i_mean = item_mean(train)\n",
    "\n",
    "print(u_mean.shape)\n",
    "print(i_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_features, item_features, user_biases, item_biases = init_MF_ALS_biased(train, 15)\n",
    "\n",
    "print(user_features.shape)\n",
    "print(item_features.shape)\n",
    "print(user_biases.shape)\n",
    "print(item_biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using SGD...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-250-f0ad8b18e8ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RMSE on test data: {}.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mmatrix_factorization_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-250-f0ad8b18e8ed>\u001b[0m in \u001b[0;36mmatrix_factorization_SGD\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnz_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# ***************************************************\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_non_biased\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_items\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mprediction_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-236-9ce04f7b878f>\u001b[0m in \u001b[0;36mprediction_non_biased\u001b[1;34m(item_features, user_features)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprediction_non_biased\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"function 'compiled'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def matrix_factorization_SGD(train, test): #rly bad\n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "    # define parameters\n",
    "    gamma = 0.01\n",
    "    num_features = 10   # K in the lecture notes\n",
    "    lambda_user = 0.1\n",
    "    lambda_item = 0.7\n",
    "    num_epochs = 20     # number of full passes through the train set\n",
    "    errors = [0]\n",
    "    \n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    num_items, num_users = train.shape\n",
    "    \n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        #gamma /= 1.2\n",
    "        \n",
    "        \n",
    "        \n",
    "        for d, n in nz_train:\n",
    "        # ***************************************************\n",
    "            prediction = prediction_non_biased(item_features, user_features)\n",
    "            gradient = np.zeros(((num_items + num_users),num_features))\n",
    "            prediction_error = (train[d,n] - prediction[d,n])\n",
    "            #print(prediction_error)\n",
    "            #gradient entries for W\n",
    "            gradient[d,:] = -(prediction_error)*(user_features[n,:].T) + lambda_item*item_features[d,:]\n",
    "            #gradient entries for Z\n",
    "            gradient[num_items+n,:] = -(prediction_error)*(item_features[d,:]) + lambda_user*user_features[n,:]\n",
    "            \n",
    "            #update\n",
    "            item_features = item_features - gamma*(gradient[:num_items,:])\n",
    "            user_features = user_features - gamma*(gradient[num_items:,:])\n",
    "            \n",
    "        rmse = compute_error_biased(train, prediction, nz_train)\n",
    "\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "        errors.append(rmse)\n",
    "\n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "    # ***************************************************\n",
    "    # TODO\n",
    "    # evaluate the test error.\n",
    "    # ***************************************************\n",
    "    rmse = 0#compute_error_biased(test, user_features, item_features, nz_test)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "\n",
    "matrix_factorization_SGD(train, test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_features, item_biases = update_item_biased_feature(train, user_features, user_biases, 0.01)\n",
    "\n",
    "print(item_features.shape)\n",
    "print(item_biases.shape)\n",
    "\n",
    "user_features, user_biases = update_user_biased_feature(train, item_features, item_biases, 0.01)\n",
    "\n",
    "print(user_features.shape)\n",
    "print(user_biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n"
     ]
    }
   ],
   "source": [
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    nz_train, nz_row_colindices, nz_col_rowindices = build_index_groups(train)\n",
    "    _,nz_user_itemindices = map(list,zip(*nz_col_rowindices))\n",
    "    nnz_items_per_user = [len(i) for i in nz_user_itemindices]\n",
    "    _,nz_item_userindices = map(list,zip(*nz_row_colindices))\n",
    "    nnz_users_per_item = [len(i) for i in nz_item_userindices]\n",
    "    max_it = 20\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n",
      "(10000, 8)\n"
     ]
    }
   ],
   "source": [
    "print(i_feat.dot(u_feat.T).shape)\n",
    "\n",
    "def update_item_feature(train, user_features, lambda_item):\n",
    "    num_users, num_features = user_features.shape\n",
    "    \n",
    "    Xt = user_features.T\n",
    "    A = Xt.dot(Xt.T) + lambda_item*np.eye(num_features)  \n",
    "    b = Xt.dot(train.T) \n",
    "\n",
    "    Yt = np.linalg.solve(A,b)\n",
    "    item_features = Yt.T\n",
    "\n",
    "    return item_features\n",
    "\n",
    "print(\"function 'compiled'\")\n",
    "\n",
    "def update_user_feature(train, item_features, lambda_user):\n",
    "    num_items, num_features = item_features.shape\n",
    "    \n",
    "    Yt = item_features.T\n",
    "    A = Yt.dot(Yt.T) + lambda_user*np.eye(num_features)  \n",
    "    b = Yt.dot(train) \n",
    "\n",
    "    Xt = np.linalg.solve(A,b)\n",
    "    user_features = Xt.T\n",
    "\n",
    "    return user_features\n",
    "\n",
    "print(\"function 'compiled'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Matrix factorisation using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run run.py 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run run.py 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods\n",
    "### CCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data\n",
      "number of items: 10000, number of users: 1000\n",
      "Preprocessing data\n",
      "Splitting data into train and test sets\n",
      "Training model\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9960226377334059.\n",
      "iter: 1, RMSE on training set: 0.9960226376398212.\n",
      "RMSE on test data: 1.0065024878485005.\n",
      "RMSE on train data: 0.9960226376398212.\n",
      "RMSE on test data: 1.0065024878485005.\n"
     ]
    }
   ],
   "source": [
    "%run run.py 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_ratings, train_arr, test_arr = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=10, p_test=0.1)\n",
    "#plot_train_test_data(train_validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import build_index_groups\n",
    "np.seterr(all='raise') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running num_features=1\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964587238475606.\n",
      "Running 2th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964584304313565.\n",
      "Running 3th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964523313834746.\n",
      "Running 4th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964418033298583.\n",
      "Running 5th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9964503244960371.\n",
      "iter: 1, RMSE on training set: 0.9964503242477375.\n",
      "RMSE on test data: 0.9964463963413265.\n",
      "Running num_features=3\n",
      "Running 1th fold in 5 folds\n",
      "learn the matrix factorization using CCD...\n",
      "iter: 0, RMSE on training set: 0.9968016355945376.\n",
      "iter: 1, RMSE on training set: 0.9955862627139658.\n",
      "iter: 2, RMSE on training set: 0.9955849744421196.\n",
      "RMSE on test data: 0.9955935180106164.\n",
      "Running 2th fold in 5 folds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-286ac0b41682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running num_features={n}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n\u001b[0;32m---> 22\u001b[0;31m                                                              num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m## Calculate mean and standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(ratings, K, method, num_items_per_user, num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item, gamma)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running {}th fold in {} folds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m### Split data in kth fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk_fold_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffled_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m### Matrix factorization using SGD/ALS/CCD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/Documents/Etudes/EPFL/pattern/PCML_project2/code/cross_validation.py\u001b[0m in \u001b[0;36mk_fold_generator\u001b[0;34m(X, K, kth_fold, batch_size, data_size, shuffled_index)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mvalidation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXdense\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffled_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetxor1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_val_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_val_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXdense\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_ind\u001b[0m\u001b[0;34m]\u001b[0m                  \u001b[0;31m## Training data indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m## Return sparse matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/asmaetounsi/anaconda3/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__array_finalize__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 5         ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features_arr = [1, 3, 5, 7, 10, 13, 15]   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.7\n",
    "min_num_ratings=10\n",
    "\n",
    "train_rmse_mean = np.zeros(len(num_features_arr))\n",
    "train_rmse_std = np.zeros(len(num_features_arr))\n",
    "validation_rmse_mean = np.zeros(len(num_features_arr))\n",
    "validation_rmse_std = np.zeros(len(num_features_arr))\n",
    "\n",
    "for i, num_features in enumerate(num_features_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running num_features={n}'.format(n=num_features))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings, num_features, lambda_user, lambda_item)\n",
    "        \n",
    "    ## Calculate mean and standard deviation    \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(num_features_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(num_features_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(num_features_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(num_features_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Number of features (K)'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99236697]\n",
      "[  1.11022302e-16]\n",
      "[ 0.99235064]\n",
      "[ 0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_rmse_mean)\n",
    "print(train_rmse_std)\n",
    "print(validation_rmse_mean)\n",
    "print(validation_rmse_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lambda_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running lambda_user=0.01\n",
      "Running 1th fold in 10 folds\n"
     ]
    }
   ],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 2     # 0-SGD 1-ALS\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user_arr = [0.01, 0.1, 1, 10]\n",
    "lambda_item = 0.7\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_user_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_user_arr))\n",
    "\n",
    "for i, lambda_user in enumerate(lambda_user_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_user={n}'.format(n=lambda_user))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_user_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_user_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_user_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_user_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda user'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD 1-ALS\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma = 0.01\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item_arr = [0.01, 0.1, 0.5, 1]\n",
    "\n",
    "train_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "train_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_mean = np.zeros(len(lambda_item_arr))\n",
    "validation_rmse_std = np.zeros(len(lambda_item_arr))\n",
    "\n",
    "for i, lambda_item in enumerate(lambda_item_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running lambda_item={n}'.format(n=lambda_item))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(lambda_item_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(lambda_item_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(lambda_item_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(lambda_item_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Lambda item'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## !!! Takes long time to run\n",
    "\n",
    "method = 0     # 0-SGD\n",
    "K = 10        ## K-fold cross validation\n",
    "gamma_arr = [0.01, 0.1, 1]\n",
    "num_features = 20   # K in the lecture notes\n",
    "lambda_user = 0.1\n",
    "lambda_item = 0.5\n",
    "\n",
    "train_rmse_mean = np.zeros(len(gamma_arr))\n",
    "train_rmse_std = np.zeros(len(gamma_arr))\n",
    "validation_rmse_mean = np.zeros(len(gamma_arr))\n",
    "validation_rmse_std = np.zeros(len(gamma_arr))\n",
    "\n",
    "for i, gamma in enumerate(gamma_arr):\n",
    "    train_rmse_arr = []\n",
    "    validation_rmse_arr = []\n",
    "    \n",
    "    print('Running gamma={n}'.format(n=gamma))\n",
    "    [train_rmse_arr, validation_rmse_arr] = cross_validation(ratings, K, method, num_items_per_user, \n",
    "                                                             num_users_per_item, min_num_ratings=10)\n",
    "        \n",
    "    train_rmse_mean[i] = np.mean(train_rmse_arr)\n",
    "    train_rmse_std[i] = np.std(train_rmse_arr)\n",
    "    validation_rmse_mean[i] = np.mean(validation_rmse_arr)\n",
    "    validation_rmse_std[i] = np.std(validation_rmse_std)\n",
    "    \n",
    "## Plotting results\n",
    "plt.fill_between(gamma_arr, train_rmse_mean - train_rmse_std,\n",
    "                     train_rmse_mean + train_rmse_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(gamma_arr, validation_rmse_mean - validation_rmse_std,\n",
    "                     validation_rmse_mean + validation_rmse_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(gamma_arr, train_rmse_mean, 'o-', color=\"r\")\n",
    "plt.plot(gamma_arr, validation_rmse_mean, 'o-', color=\"g\")\n",
    "plt.legend(('Train', 'Validation'))\n",
    "plt.xlabel('Learning Rate'); plt.ylabel('RMSE');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "#### 1. Compare SGD, ALS with the best set of parameters (based on above results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
